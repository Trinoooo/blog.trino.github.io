[{"title":"新的开始","path":"/新的开始/","content":"做点自己喜欢的事情吧 2022 年 7 月 27 日","tags":["开端"]},{"title":"关于","path":"/about/index.html","content":"悟已往之不谏，知来者之可追。实迷途其未远，觉今是而昨非"},{"path":"/wiki/computer-network/index.html","content":"请注意：本资源来源于网络，如有侵权请联系删除"},{"title":"写在前面","path":"/wiki/computer-systems:a-programmer's-perspective/chapter_01.html","content":"计算机系统是由硬件和系统软件组成的，它们共同工作来运行应用程序。虽然系统的具体实现方式随着时间不断变化，但是系统内在的概念却没有改变。所有计算机系统都有相似的硬件和软件组件，它们又执行着相似的功能。一些程序员希望深入了解这些组件是如何工作的以及这些组件是如何影响程序的正确性和性能的，以此来提高自身的技能。本书便是为这些读者而写的。现在就要开始一次有趣的漫游历程了。如果你全力投身学习本书中的概念，完全理解底层计算机系统以及它对应用程序的影响，那么你会步上成为为数不多的“大牛”的道路。你将会学习一些实践技巧，比如如何避免由计算机表示数字的方式引起的奇怪的数字错误。你将学会怎样通过一些小窍门来优化自己的 C 代码，以充分利用现代处理器和存储器系统的设计。你将了解编译器是如何实现过程调用的，以及如何利用这些知识来避免缓冲区溢出错误带来的安全漏洞，这些弱点给网络和因特网软件带来了巨大的麻烦。你将学会如何识别和避免链接时那些令人讨厌的错误，它们困扰着普通的程序员。你将学会如何编写自己的 Unix shell、自己的动态存储分配包，甚至于自己的 Web 服务器。你会认识并发带来的希望和陷阱，这个主题随着单个芯片上集成了多个处理器核变得越来越重要。在 Kernighan 和 Ritchie 的关于 C 编程语言的经典教材中，他们通过图 1-1 中所示的 hello 程序来向读者介绍 C。尽管 hello 程序非常简单，但是为了让它实现运行，系统的每个主要组成部分都需要协调工作。从某种意义上来说，本书的目的就是要帮助你了解当你在系统上执行 hello 程序时，系统发生了什么以及为什么会这样。 123456#include &lt;stdio.h&gt;int main()&#123; printf(&quot;hello, world &quot;); return 0;&#125; 图1-1指上面代码块，代码文件路径code&#x2F;intro&#x2F;hello.c 我们通过跟踪 hello 程序的生命周期来开始对系统的学习——从它被程序员创建开始，到在系统上运行，输出简单的消息，然后终止。我们将沿着这个程序的生命周期，简要地介绍一些逐步出现的关键概念、专业术语和组成部分。后面的章节将围绕这些内容展开。"},{"title":"前言","path":"/wiki/computer-systems:a-programmer's-perspective/index.html","content":"请注意：本资源来源于网络，如有侵权请联系删除 本书（简称 CS：APP）的主要读者是计算机科学家、计算机工程师，以及那些想通过学习计算机系统的内在运作而能够写出更好程序的人。我们的目的是解释所有计算机系统的本质概念，并向你展示这些概念是如何实实在在地影响应用程序的正确性、性能和实用性的。其他的系统类书籍都是从构建者的角度来写的，讲述如何实现硬件或系统软件，包括操作系统、编译器和网络接口。而本书是从程序员的角度来写的，讲述应用程序员如何能够利用系统知识来编写出更好的程序。当然，学习一个计算机系统应该做些什么，是学习如何构建一个计算机系统的很好的出发点，所以，对于希望继续学习系统软硬件实现的人来说，本书也是一本很有价值的介绍性读物。大多数系统书籍还倾向于重点关注系统的某一个方面，比如：硬件架构、操作系统、编译器或者网络。本书则以程序员的视角统一覆盖了上述所有方面的内容。如果你研究和领会了这本书里的概念，你将开始成为极少数的”牛人”，这些”牛人”知道事情是如何运作的，也知道当事情出现故障时如何修复。你写的程序将能够更好地利用操作系统和系统软件提供的功能，对各种操作条件和运行时参数都能正确操作，运行起来更快，并能避免出现使程序容易受到网络攻击的缺陷。同时，你也要做好更深入探究的准备，研究像编译器、计算机体系结构、操作系统、嵌入式系统、网络互联和网络安全这样的高级题目。 读者应具备的背景知识本书的重点是执行 x86-64 机器代码的系统。对英特尔及其竞争对手而言，x86-64 是他们自 1978 年起，以8086 微处理器为代表，不断进化的最新成果。按照英特尔微处理器产品线的命名规则，这类微处理器俗称为 “x86”。随着半导体技术的演进，单芯片上集成了更多的晶体管，这些处理器的计算能力和内存容量有了很大的增长。在这个过程中，它们从处理 16 位字，发展到引人 IA32 处理器处理 32 位字，再到最近的 x86-64 处理 64 位字。我们考虑的是这些机器如何在 Linux 操作系统上运行 C 语言程序。Linux 是众多继承自最初由贝尔实验室开发的 Unix 的操作系统中的一种。这类操作系统的其他成员包括 Solaris、FreeBSD 和 Mac OSX。近年来，由于 Posix 和标准 Unix 规范的标准化努力，这些操作系统保持了高度兼容性。因此，本书内容几乎直接适用于这些“类 Unix”操作系统。文中包含大量已在 Linux 系统上编译和运行过的程序示例。我们假设你能访问一台这样的机器，并且能够登录，做一些诸如切换目录之类的简单操作。如果你的计算机运行的是 Microsoft Windows 系统，我们建议你选择安装一个虚拟机环境（例如 VirtualBox 或者 VMWare），以便为一种操作系统（客户 OS）编写的程序能在另一种系统（宿主 OS）上运行。我们还假设你对 C 和 C++ 有一定的了解。如果你以前只有 Java 经验，那么你需要付出更多的努力来完成这种转换，不过我们也会帮助你。Java 和 C 有相似的语法和控制语句。不过，有一些 C 语言的特性（特别是指针、显式的动态内存分配和格式化 I&#x2F;O）在 Java 中都是没有的。所幸的是，C 是一个较小的语言，在Brian Kernighan和Dennis Ritchie经典的”K8.R”文献中得到了清晰优美的描述【61】。无论你的编程背景如何，都应该考虑将 K&amp;R 作为个人系统藏书的一部分。如果你只有使用解释性语言的经验，如 Python、Ruby 或 Perl，那么在使用本书之前，需要花费一些时间来学习 C。本书的前几章揭示了C语言程序和它们相对应的机器语言程序之间的交互作用。机器语言示例都是用运行在 x86-64 处理器上的 GNUGCC 编译器生成的。我们不需要你以前有任何硬件、机器语言或是汇编语言编程的经验。 给C语言初学者 - 关于C编程语言的建议为了帮助C语言编程背景薄弱（或全无背景）的读者，我们在书中加入了这样一些专门的注释来突出 C 中一些特别重要的特性。我们假设你熟悉 C++ 或 Java。 如何阅读此书从程序员的角度学习计算机系统是如何工作的会非常有趣，主要是因为你可以主动地做这件事情。无论何时你学到一些新的东西，都可以马上试验并且直接看到运行结果。事实上，我们相信学习系统的唯一方法就是做（do）系统，即在真正的系统上解决具体的问题，或是编写和运行程序。这个主题观念贯穿全书。当引入一个新概念时，将会有一个或多个练习题紧随其后，你应该马上做一做来检验你的理解。这些练习题的解答在每章的末尾。当你阅读时，尝试自己来解答每个问题，然后再查阅答案，看自已的答案是否正确。除第 1 章外，每章后面都有难度不同的家庭作业。对每个家庭作业题，我们标注了难度级别： ① 只需要几分钟。几乎或完全不需要编程。 ② 可能需要将近 20 分钟。通常包括编写和测试一些代码。（许多都源自我们在考试中出的题目。） ③ 需要很大的努力，也许是 1～2 个小时。一般包括编写和测试大量的代码。 ④ 一个实验作业，需要将近 10 个小时。 文中每段代码示例都是由经过 GCC 编译的 C 程序直接生成并在 Linux 系统上进行了测试，没有任何人为的改动。当然，你的系统上 GCC 的版本可能不同，或者根本就是另外一种编译器，那么可能生成不一样的机器代码，但是整体行为表现应该是一样的。所有的源程序代码都可以从 csapp.cs.cmu.edu 上 的CS：APP 主页上获取。在本书中，源程序的文件名列在两条水平线的右边，水平线之间是格式化的代码。比如，图1中的程序能在 code&#x2F;intro&#x2F; 目录下的 hello.c 文件中找到。当遇到这些示例程序时，我们鼓励你在自己的系统上试着运行它们。为了避免本书体积过大、内容过多，我们添加了许多网络旁注（Web aside），包括一些对本书主要内容的补充资料。本书中用 CHAP：TOP 这样的标记形式来引用这些旁注，这里 CHAP 是该章主题的缩写编码，而 TOP 是涉及的话题的缩写编码。例如，网络旁注 DATA：BOOL 包含对第 2 章中数据表示里面有关布尔代数内容的补充资料；而网络旁注 ARCH：VLOG 包含的是用 Verilog 硬件描述语言进行处理器设计的资料，是对第 4 章中处理器设计部分的补充。所有的网络旁注都可以从 CS：APP 的主页上获取。 旁注-什么是旁注在整本书中，你将会遇到很多以这种形式出现的旁注。旁注是附加说明，能使你对当前讨论的主题多一些了解。旁注可以有很多用处。一些是小的历史故事。例如，C 语言、Linux 和 Internet 是从何而来的？有些旁注则是用来澄清学生们经常感到疑感的问题。例如，高速缓存的行、组和块有什么区别？还有些旁注给出了一些现实世界的例子。例如，一个浮点错误怎么毁掉了法国的一枚火箭，或是给出市面上出售的一个磁盘驱动器的几何和运行参数。最后，还有一些旁注仅仅就是一些有趣的内容，例如，什么是“hoinky”？ 本书概述本书由 12 章组成，旨在阐述计算机系统的核心概念。内容概述如下： 第1章：计算机系统漫游。 这一章通过研究 “hello, world” 这个简单程序的生命周期，介绍计算机系统的主要概念和主题。 第2章：信息的表示和处理。 我们讲述了计算机的算术运算，重点描述了会对程序员有 影响的无符号数和数的补码表示的特性。我们考虑数字是如何表示的，以及由此确定对于一个给定的字长，其可能编码值的范围。我们探讨有符号和无符号数字之间类型转换的效果，还阐述算术运算的数学特性。菜鸟级程序员经常很惊奇地了解到（用补码表示的）两个正数的和或者积可能为负。另一方面，补码的算术运算满足很多整数运算的代数特性，因此，编译器可以很安全地把一个常量乘法转化为一系列的移位和加法。我们用C 语言的位级操作来说明布尔代数的原理和应用。我们从两个方面讲述了 IEEE 标准的浮点格式：一是如何用它来表示数值，一是浮点运算的数学属性。对计算机的算术运算有深刻的理解是写出可靠程序的关键。比如，程序员和编译器不能用表达式（x-y&lt;0）来替代（x&lt;y），因为前者可能会产生溢出。甚至也不能用表达式（-y&lt;-x）来替代，因为在补码表示中负数和正数的范围是不对称的。算术溢出是造成程序错误和安全漏洞的一个常见根源，然而很少有书从程序员的角度来讲述计算机算术运算的特性。 第3章：程序的机器级表示。 我们教读者如何阅读由 C 编译器生成的 x86-64 机器代码。我们说明为不同控制结构（比如条件、循环和开关语句）生成的基本指令模式。我们还讲述过程的实现，包括栈分配、寄存器使用惯例和参数传递。我们讨论不同数据结构（如结构、联合和数组）的分配和访问方式。我们还说明实现整数和浮点数算术运算的指令。我们还以分析程序在机器级的样子作为途径，来理解常见的代码安全漏洞（例如缓冲区溢出），以及理解程序员、编译器和操作系统可以采取的减轻这些威胁的措施。学习本章的概念能够帮助读者成为更好的程序员，因为你们懂得程序在机器上是如何表示的。另外一个好处就在于读者会对指针有非常全面而具体的理解。 第4章：处理器体系结构。 这一章讲述基本的组合和时序逻辑元素，并展示这些元素如何在数据通路中组合到一起，来执行 x86-64 指令集的一个称为 “Y86-64” 的简化子集。我们从设计单时钟周期数据通路开始。这个设计概念上非常简单，但是运行速度不会太快。然后我们引入流水线的思想，将处理一条指令所需要的不同步骤实现为独立的阶段。这个设计中，在任何时刻，每个阶段都可以处理不同的指令。我们的五阶段处理器流水线更加实用。本章中处理器设计的控制逻辑是用一种称为 HCL 的简单硬件描述语言来描述的。用 HCL 写的硬件设计能够编译和链接到本书提供的模拟器中，还可以根据这些设计生成 Verilog 描述，它适合合成到实际可以运行的硬件上去。 第5章：优化程序性能。 在这一章里，我们介绍了许多提高代码性能的技术，主要思想就是让程序员通过使编译器能够生成更有效的机器代码来学习编写 C 代码。我们一开始介绍的是减少程序需要做的工作的变换，这些是在任何机器上写任何程序时都应该遵循的。然后讲的是增加生成的机器代码中指令级并行度的变换，因而提高了程序在现代“超标量”处理器上的性能。为了解释这些变换行之有效的原理，我们介绍了一个简单的操作模型，它描述了现代乱序处理器是如何工作的，然后给出了如何根据一个程序的图形化表示中的关键路径来测量一个程序可能的性能。你会惊讶于对 C 代码做一些简单的变换能给程序带来多大的速度提升。 第6章：存储器层次结构。 对应用程序员来说，存储器系统是计算机系统中最直接可见的部分之一。到目前为止，读者一直认同这样一个存储器系统概念模型，认为它是一个有一致访问时间的线性数组。实际上，存储器系统是一个由不同容量、造价和访问时间的存储设备组成的层次结构。我们讲述不同类型的随机存取存储器（RAM）和只读存储器（ROM），以及磁盘和✦固态硬盘✦的几何形状和组织构造。我们描述这些存储设备是如何放置在层次结构中的，讲述访问局部性是如何使这种层次结构成为可能的。我们通过一个独特的观点使这些理论具体化，那就是将存储器系统视为一个“存储器山”，山脊是时间局部性，而斜坡是空间局部性。最后，我们向读者阐述如何通过改善程序的时间局部性和空间局部性来提高应用程序的性能。固态硬盘：直译应为固态驱动器，但固态硬盘一词已经被大家接受，所以沿用。——译者注 第7章：链接。 本章讲述静态和动态链接，包括的概念有可重定位的和可执行的目标文件、符号解析、重定位、静态库、共享目标库、位置无关代码，以及库打桩。大多数讲述系统的书中都不讲链接，我们要讲述它是出于以下原因。第一，程序员遇到的最令人迷惑的问题中，有一些和链接时的小故障有关，尤其是对那些大型软件包来说。第二，链接器生成的目标文件是与一些像加载、虚拟内存和内存映射这样的概念相关的。 第8章：异常控制流。 在本书的这个部分，我们通过介绍异常控制流（即除正常分支和过程调用以外的控制流的变化）的一般概念，打破单一程序的模型。我们给出存在于系统所有层次的异常控制流的例子，从底层的硬件异常和中断，到并发进程的上下文切换，到由于接收 Linux 信号引起的控制流突变，到 C 语言中破坏栈原则的非本地跳转。 在这一章，我们介绍进程的基本概念，进程是对一个正在执行的程序的一种抽象。读者会学习进程是如何工作的，以及如何在应用程序中创建和操纵进程。我们会展示应用程序员如何通过 Linux 系统调用来使用多个进程。学完本章之后，读者就能够编写带作业控制的 Linux shell 了。同时，这里也会向读者初步展示程序的并发执行会引起不确定的行为。 第9章：虚拟内存。我们讲述虚拟内存系统是希望读者对它是如何工作的以及它的特性有所了解。我们想让读者了解为什么不同的并发进程各自都有一个完全相同的地址范围，能共享某些页，而又独占另外一些页。我们还讲了一些管理和操纵虚拟内存的问题。特别地，我们讨论了存储分配操作，就像标准库的 malloc 和 free 操作。阐述这些内容是出于下面几个目的。它加强了这样一个概念，那就是虚拟内存空间只是一个字节数组，程序可以把它划分成不同的存储单元。它可以帮助读者理解当程序包含存储泄漏和非法指针引用等内存引用错误时的后果。最后，许多应用程序员编写自己的优化了的存储分配操作来满足应用程序的需要和特性。这一章比其他任何一章都更能展现将计算机系统中的硬件和软件结合起来阐述的优点。而传统的计算机体系结构和操作系统书籍都只讲述虚拟内存的某一方面。 第10章：系统级 I&#x2F;O。 我们讲述 Unix I&#x2F;O 的基本概念，例如文件和描述符。我们描述如何共享文件，I&#x2F;O 重定向是如何工作的，还有如何访问文件的元数据。我们还开发了一个健壮的带缓冲区的 I&#x2F;O 包，可以正确处理一种称为 short counts 的奇特行为，也就是库函数只读取一部分的输入数据。我们阐述 C 的标准 I&#x2F;O 库，以及它与 Linux I&#x2F;O 的关系，重点谈到标准 I&#x2F;O 的局限性，这些局限性使之不适合网络编程。总的来说，本章的主题是后面两章——网络和并发编程的基础。 第11章：网络编程。 对编程而言，网络是非常有趣的 I&#x2F;O 设备，它将许多我们前面文中学习的概念（比如进程、信号、字节顺序、内存映射和动态内存分配）联系在一起。网络程序还为下一章的主题——并发，提供了一个很令人信服的上下文。本章只是网络编程的一个很小的部分，使读者能够编写一个简单的 Web 服务器。我们还讲述位于所有网络程序底层的客户端-服务器模型。我们展现了一个程序员对 Internet 的观点，并且教读者如何用套接字接口来编写 Internet 客户端和服务器。最后，我们介绍超文本传输协议（HTTP），并开发了一个简单的迭代式 Web 服务器。 第12章：并发编程。 这一章以 Internet 服务器设计为例介绍了并发编程。我们比较对照了三种编写并发程序的基本机制（进程、I&#x2F;O多路复用和线程），并且展示如何用它们来建造并发Internet服务器。我们探讨了用 P、V 信号量操作来实现同步、线程安全和可重入、竞争条件以及死锁等的基本原则。对大多数服务器应用来说，写并发代码都是很关键的。我们还讲述了线程级编程的使用方法，用这种方法来表达应用程序中的并行性，使得程序在多核处理器上能执行得更快。使用所有的核解决同一个计算问题需要很小心谨慎地协调并发线程，既要保证正确性，又要争取获得高性能。 本版新增内容本书的第 1 版于 2003 年出版，第 2 版在 2011 年出版。考虑到计算机技术发展如此迅速，这本书的内容还算是保持得很好。事实证明 Intel x86 的机器上运行 Linux （以及相关操作系统），加上采用 C 语言编程，是一种能够涵盖当今许多系统的组合。然而，硬件技术、编译器和程序库接口的变化，以及很多教师教授这些内容的经验，都促使我们做了大量的修改。第 2 版以来的最大整体变化是，我们的介绍从以 IA32 和 x86-64 为基础，转变为完全以 x86-64 为基础。这种重心的转移影响了很多章节的内容。下面列出一些明显的变化∶ 第1章。我们将第 5 章对 Amdahl 定理的讨论移到了本章。 第2章。读者和评论家的反馈是一致的，本章的一些内容有点令人不知所措。因此，我们澄清了一些知识点，用更加数学的方式来描述，使得这些内容更容易理解。这使得读者能先略过数学细节，获得高层次的总体概念，然后回过头来进行更细致深入的阅读。 第3章。我们将之前基于 IA32 和 x86-64 的表现形式转换为完全基于 x86-64 ，还更新了近期版本 GCC 产生的代码。其结果是大量的重写工作，包括修改了一些概念提出的顺序。同时，我们还首次介绍了对处理浮点数据的程序的机器级支持。由于历史原因，我们给出了一个网络旁注描述 IA32 机器码。 第4章。我们将之前基于 32 位架构的处理器设计修改为支持 64 位字和操作的设计。 第5章。我们更新了内容以反映最近几代 x86-64 处理器的性能。通过引入更多的功能单元和更复杂的控制逻辑，我们开发的基于程序数据流表示的程序性能模型，其性能预测变得 比之前更加可靠。 第6章。我们对内容进行了更新，以反映更多的近期技术。 第7章。针对 x86-64，我们重写了本章，扩充了关于用 GOT 和 PLT 创建位置无关代码的讨论，新增了一节描述更加强大的链接技术，比如库打桩。 第8章。我们增加了对信号处理程序更细致的描述，包括异步信号安全的函数，编写信号处理程序的具体指导原则，以及用 sigsuspend 等待处理程序。 第9章。本章变化不大。 第10章。我们新增了一节说明文件和文件的层次结构，除此之外，本章的变化 不大。 第11章。我们介绍了采用最新 getaddrinfo 和 getnameinfo 函数的、与协议无关和线程安全的网络编程，取代过时的、不可重入的 gethostbyname 和 gethost-byaddr 函数。 第12章。我们扩充了利用线程级并行性使得程序在多核机器上更快运行的内容。 此外，我们还增加和修改了很多练习题和家庭作业。 本书的起源本书起源于 1998 年秋季，我们在卡内基-梅隆（CMU）大学开设的一门编号为 15-213 的介绍性课程∶计算机系统导论（Introduction to Computer System，ICS）【14】。从那以后，每学期都开设了 ICS 这门课程，每学期有超过 400 名学生上课，这些学生从本科二年级到硕士研究生都有，所学专业也很广泛。这门课程是卡内基-梅隆大学计算机科学系（CS）以及电子和计算机工程系（ECE）所有本科生的必修课，也是 CS 和 ECE 大多数高级系统课程的先行必修课。ICS 这门课程的宗旨是用一种不同的方式向学生介绍计算机。因为，我们的学生中几乎没有人有机会亲自去构造一个计算机系统。另一方面，大多数学生，甚至包括所有的计算机科学家和计算机工程师，也需要日常使用计算机和编写计算机程序。所以我们决定从程序员的角度来讲解系统，并采用这样的原则过滤要讲述的内容∶我们只讨论那些影响用户级 C 语言程序的性能、正确性或实用性的主题。比如，我们排除了诸如硬件加法器和总线设计这样的主题。虽然我们谈及了机器语言，但是重点并不在于如何手工编写汇编语言，而是关注 C 语言编译器是如何将 C 语言的结构翻译成机器代码的，包括编译器是如何翻译指针、循环、过程调用以及开关（switch）语句的。更进一步地，我们将更广泛和全盘地看待系统，包括硬件和系统软件，涵盖了包括链接、加载、进程、信号、性能优化、虚拟内存、I&#x2F;O 以及网络与并发编程等在内的主题。这种做法使得我们讲授 ICS 课程的方式对学生来讲既实用、具体，还能动手操作，同时也非常能调动学生的积极性。很快地，我们收到来自学生和教职工非常热烈而积极的反响，我们意识到卡内基-梅隆大学以外的其他人也可以从我们的方法中获益。因此，这本书从 ICS 课程的笔记中应运而生了，而现在我们对它做了修改，使之能够反映科学技术以及计算机系统实现中的变化和进步。通过本书的多个版本和多种语言译本， ICS 和许多相似课程已经成为世界范围内数百所高校的计算机科学和计算机工程课程的一部分。 写给指导教师们∶可以基于本书的课程指导教师可以使用本书来讲授五种不同类型的系统课程。具体每门课程则有赖于课程大纲的要求、个人喜好、学生的背景和能力。图中的课程从左往右越来越强调以程序员的角度来看待系统。以下是简单的描述。 ORG：一门以非传统风格讲述传统主题的计算机组成原理课程。传统的主题包括逻辑设计、处理器体系结构、汇编语言和存储器系统，然而这里更多地强调了对程序员的影响。例如，要反过来考虑数据表示对 C 语言程序的数据类型和操作的影响。又例如，对汇编代码的讲解是基于C语言编译器产生的机器代码，而不是手工编写的汇编代码。 ORG+：一门特别强调硬件对应用程序性能影响的 ORG 课程。和 ORG 课程相比，学生要更多地学习代码优化和改进 C 语言程序的内存性能。 ICS：基本的ICS课程，旨在培养一类程序员，他们能够理解硬件、操作系统和编译系统对应用程序的性能和正确性的影响。和 ORG+ 课程的一个显著不同是，本课程不涉及低层次的处理器体系结构。相反，程序员只同现代乱序处理器的高级模型打交道。ICS 课程非常适合安排到一个 10 周的小学期，如果期望步调更从容一些，也可以延长到一个 15 周的学期。 ICS+：在基本的 ICS 课程基础上，额外论述一些系统编程的问题，比如系统级 I&#x2F;O、网络编程和并发编程。这是卡内基-梅隆大学的一门一学期时长的课程，会讲述本书中除了低级处理器体系结构以外的所有章。 SP：一门系统编程课程。和 ICS+ 课程相似，但是剔除了浮点和性能优化的内容，更加强调系统编程，包括进程控制、动态链接、系统级I&#x2F;O、网络编程和并发编程。指导教师可能会想从其他渠道对某些高级主题做些补充，比如守护进程（daemon）、终端控制和 Unix IPC（进程间通信）。 如果你希望学生更多地了解低层次的处理器体系结构，那么通过 ORG 和 ORG+ 课程可以达到目的。另一方面，如果你想将当前的计算机组成原理课程转换成 ICS 或者 ICS+ 课程，但是又对突然做这样剧烈的变化感到担心，那么你可以逐步递增转向 ICS 课程。你可以从 OGR 课程开始，它以一种非传统的方式教授传统的问题。一旦你对这些内容感到驾轻就熟了，就可以转到 ORG+，最终转到 ICS。如果学生没有 C 语言的经验（比如他们只用 Java 编写过程序），你可以花几周的时间在 C 语言上，然后再讲述 ORG 或者 ICS 课程的内容。最后，我们认为 ORG+ 和 SP 课程适合安排为两期（两个小学期或者两个学期）。或者你可以考虑按照一期 ICS 和一期 SP 的方式来教授 ICS+ 课程。 写给指导教师们∶经过课堂验证的实验练习ICS+ 课程在卡内基-梅隆大学得到了学生很高的评价。学生对这门课程的评价，中值分数一般为 5.0&#x2F;5.0，平均分数一般为 4.6&#x2F;5.0。学生们说这门课非常有趣，令人兴奋，主要就是因为相关的实验练习。这些实验练习可以从 CS∶APP 的主页上获得。下面是本书提供的一些实验的示例。 数据实验。这个实验要求学生实现简单的逻辑和算术运算函数，但是只能使用一个非常有限的C语言子集。比如，只能用位级操作来计算一个数字的绝对值。这个实验可帮助学生了解C语言数据类型的位级表示，以及数据操作的位级行为。 二进制炸弹实验。二进制炸弹是一个作为目标代码文件提供给学生的程序。运行时，它提示用户输入 6 个不同的字符串。如果其中的任何一个不正确，炸弹就会“爆炸”，打印出一条错误消息，并且在一个打分服务器上记录事件日志。学生必须通过对程序反汇编和逆向工程来测定应该是哪 6 个串，从而解除各自炸弹的雷管。该实验能教会学生理解汇编语言，并且强制他们学习怎样使用调试器。 缓冲区溢出实验。它要求学生通过利用一个缓冲区溢出漏洞，来修改一个二进制可执行文件的运行时行为。这个实验可教会学生栈的原理，并让他们了解写那种易于遭受缓冲区溢出攻击的代码的危险性。 体系结构实验。第 4 章的几个家庭作业能够组合成一个实验作业，在实验中，学生修改处理器的 HCL 描述，增加新的指令，修改分支预测策略，或者增加、删除旁路路径和寄存器端口。修改后的处理器能够被模拟，并通过运行自动化测试检测出大多数可能的错误。这个实验使学生能够体验处理器设计中令人激动的部分，而不需要掌握逻辑设计和硬件描述语言的完整知识。 性能实验。学生必须优化应用程序的核心函数（比如卷积积分或矩阵转置）的性能。这个实验可非常清晰地表明高速缓存的特性，并带给学生低级程序优化的经验。 cache 实验。这个实验类似于性能实验，学生编写一个通用高速缓存模拟器，并优化小型矩阵转置核心函数，以最小化对模拟的高速缓存的不命中次数。我们使用 Valgrind 为矩阵转置核心函数生成真实的地址访问记录。 shell实验。学生实现他们自己的带有作业控制的 Unix shell 程序，包括 Ctrl+C和 Ctrl+Z 按键，fg、bg和 jobs 命令。这是学生第一次接触并发，并且让他们对 Unix 的进程控制、信号和信号处理有清晰的了解。 malloc 实验。学生实现他们自己的 malloc、free 和 realloc（可选）版本。这个实验可让学生们清晰地理解数据的布局和组织，并且要求他们评估时间和空间效率的各种权衡及折中。 代理实验。实现一个位于浏览器和万维网其他部分之间的并行 Web 代理。这个实验向学生们揭示了 Web 客户端和服务器这样的主题，并且把课程中的许多概念联系起来，比如字节排序、文件 I&#x2F;O、进程控制、信号、信号处理、内存映射、套接字和并发。学生很高兴能够看到他们的程序在真实的 Web 浏览器和 Web 服务器之间起到的作用。 CS∶APP的教师手册中有对实验的详细讨论，还有关于下载支持软件的说明。"},{"path":"/wiki/storage/index.html","content":"请注意：本资源来源于网络，如有侵权请联系删除 都梁 《亮剑》 《血色浪漫》 《大崩溃》 《狼烟北平》 《荣宝斋》 大仲马 《基督山伯爵》 斯蒂芬·茨威格 《人类的群星闪耀时》 钱锺书 《围城》 乔治·奥威尔 《一九八四》 凯利·麦格尼格尔 《自控力经典套装三册》 乔斯坦·贾德 《苏菲的世界》 兰道尔·门罗 《那些古怪又让人忧心的问题》 路遥 《平凡的世界》 《人生》 米克罗斯·尼兹利 《逃离奥斯维辛》 史铁生 《我与地坛》 卡勒德·胡赛尼 《追风筝的人》 周梅森 《人民的名义》 计算机相关 《代码整洁之道》 《Go Web编程》 《Go程序设计语言》 《Go语言实战》 《Redis设计与实现》 《Redis实战》"},{"title":"前言","path":"/wiki/understanding-nginx:modules-development-and-architecture-resolving(second-edition)/index.html","content":"请注意：本资源来源于网络，如有侵权请联系删除 为什么要写这本书自第1版发行以来，笔者很欣慰得到了广大读者的认可。本书一直致力于说明开发Nginx模块的必备知识，然而由于Nginx功能繁多且性能强大，以致必须要了解的基本技能也很庞杂，而第1版成书匆忙，缺失了几个进阶的技巧描述（例如如何使用变量、slab共享内存等），因此决定在第1版的基础上进一步完善。事实上，我们总能在nginx.conf配置文件中看到各种带着$符号的变量，只要修改带着变量的这一行行配置，就可以不用编译、部署而使得Nginx具备新功能，这些支持变量的Nginx模块提供了极为灵活的功能，第2版通过新增的第15章详细介绍了如何在模块中支持HTTP变量，包括如何在代码中使用其他Nginx模块提供的变量，以及如何定义新的变量供nginx.conf和其他第三方模块使用等。第16章介绍了slab共享内存，这是一套适用于小块内存快速分配释放的内存管理方式，它非常高效，分配与释放速度都是以纳秒计算的，常用于多个worker进程之间的通信，这比第14章介绍的原始的共享内存通信方式要先进很多。第16章不仅详细介绍了它的实现方式，也探讨了它的优缺点，比如，如果模块间要共享的单个对象常常要消耗数KB的空间，这时就需要修改它的实现（例如增大定义的slab页大小），以避免内存的浪费等。Nginx内存池在第1版中只是简单带过，第2版中新增了8.7节介绍了内存池的实现细节，以帮助读者用好最基础的内存池功能。此外，很多读者反馈需要结合TCP来谈谈Nginx，因此在9.10节中笔者试图在不陷入Linux内核细节的情况下，简要介绍了TCP以清晰了解Nginx的事件框架，了解Nginx的高并发能力。这一版新增的第15章的样例代码可以从http://nginx.taohui.org.cn 站点上下载。因笔者工作繁忙，以致第2版拖稿严重，读者的邮件也无法及时回复，非常抱歉。从这版开始会把曾经的回复整理后放在网站上，想必这比回复邮件要更有效率些。 读者对象本书适合以下读者阅读。 对Nginx及如何将它搭建成一个高性能的Web服务器感兴趣的读者。 希望通过开发特定的HTTP模块实现高性能Web服务器的读者。 希望了解Nginx的架构设计，学习其怎样充分使用服务器上的硬件资源的读者。 了解如何快速定位、修复Nginx中深层次Bug的读者。 希望利用Nginx提供的框架，设计出任何基于TCP的、无阻塞的、易于扩展的服务器的读者。 背景知识如果仅希望了解怎样使用已有的Nginx功能搭建服务器，那么阅读本书不需要什么先决条件。但如果希望通过阅读本书的第二、第三两部分，来学习Nginx的模块开发和架构设计技巧时，则必须了解C语言的基本语法。在阅读本书第三部分时，需要读者对TCP有一个基本的了解，同时对Linux操作系统也应该有简单的了解。 如何阅读本书我很希望将本书写成一本“step by step”式（循序渐进式）的书籍，因为这样最能节省读者的时间，然而，由于3个主要写作目的想解决的问题都不是那么简单，所以这本书只能做一个折中的处理。在第一部分的前两章中，将只探讨如何使用Nginx这一个问题。阅读这一部分的读者不需要了解C语言，就可以学习如何部署Nginx，学习如何向其中添加各种官方、第三方的功能模块，如何通过修改配置文件来更改Nginx及各模块的功能，如何修改Linux操作系统上的参数来优化服务器性能，最终向用户提供企业级的Web服务器。这一部分介绍配置项的方式，更偏重于领着对Nginx还比较陌生的读者熟悉它，通过了解几个基本Nginx模块的配置修改方式，进而使读者可以通过查询官网、第三方网站来了解如何使用所有Nginx模块的用法。在第二部分的第3章~第7章中，都是以例子来介绍HTTP模块的开发方式的，这里有些接近于“step by step”的学习方式，我在写作这一部分时，会通过循序渐进的方式使读者能够快速上手，同时会穿插着介绍其常见用法的基本原理。在第三部分，将开始介绍Nginx的完整框架，阅读到这里将会了解第二部分中HTTP模块为何以此种方式开发，同时将可以轻易地开发Nginx模块。这一部分并不仅仅满足于阐述Nginx架构，而是会探讨其为何如此设计，只有这样才能抛开HTTP框架、邮件代理框架，实现一种新的业务框架、一种新的模块类型。对于Nginx的使用还不熟悉的读者应当从第1章开始学习，前两章将帮助你快速了解Nginx。使用过Nginx，但对如何开发Nginx的HTTP模块不太了解的读者可以直接从第3章开始学习，在这一章阅读完后，即可编写一个功能大致完整的HTTP模块。然而，编写企业级的模块必须阅读完第4章才能做到，这一章将会介绍编写产品线上服务器程序时必备的3个手段。第5章举例说明了两种编写复杂HTTP模块的方式，在第三部分会对这两个方式有进一步的说明。第6章介绍一种特殊的HTTP模块——HTTP过滤模块的编写方法。第7章探讨基础容器的用法，这同样是复杂模块的必备工具。如果读者对于普通HTTP模块的编写已经很熟悉，想深入地实现更为复杂的HTTP模块，或者想了解邮件代理服务器的设计与实现，或者希望编写一种新的处理其他协议的模块，或者仅仅想了解Nginx的架构设计，都可以直接从第8章开始学习，这一章会从整体上系统介绍Nginx的模块式设计。第9章的事件框架是Nginx处理TCP的基础，这一章无法跳过。阅读第8章、第9章时可能会遇到许多第7章介绍过的容器，这时可以回到第7章查询其用法和意义。第10章~第12章在介绍HTTP框架，通过这3章的学习会对HTTP模块的开发有深入的了解，同时可以学习HTTP框架的优秀设计。第13章简单介绍了邮件代理服务器的设计，它近似于简化版的HTTP框架。第14章介绍了进程间同步的工具。第15章介绍了HTTP变量，包括如何使用已有变量、支持用户在nginx.conf中修改变量的值、支持其他模块开发者使用自己定义的变量等。第16章介绍了slab共享内存，该内存极为高效，可用于多个worker进程间的通信。为了不让读者陷入代码的“汪洋大海”中，在本书中大量使用了图表，这样可以使读者快速、大体地了解流程和原理，在这基础上，如果读者还希望了解代码是如何实现的，可以针对性地阅读源代码中的相应方法。在代码的关键地方会通过添加注释的方式加以说明。希望这种方式能够帮助读者减少阅读花费的时间，更快、更好地把握住Nginx，同时深入到细节中。写作本书第1版时，Nginx的最新稳定版本是1.0.14，所以当时是基于此版本来写作的。截止到第2版完成时，Nginx的稳定版本已经上升到了1.8.0。但这不会对本书的阅读造成困惑，笔者验证过示例代码，均可以运行在最新版本的Nginx中，这是因为本书主要是在介绍Nginx的基本框架代码，以及怎样使用这些框架代码开发新的Nginx模块。在这些基本框架代码中，Nginx一般不会做任何改变，否则已有的大量Nginx模块将无法工作，这种损失是不可承受的。而且Nginx框架为具体的功能模块提供了足够的灵活性，修改功能时很少需要修改框架代码。Nginx是跨平台的服务器，然而这本书将只针对于最常见的Linux操作系统进行分析，这样做一方面是篇幅所限，另一方面则是本书的写作目的主要在于告诉大家如何基于Nginx编写代码，而不是怎样在一个具体的操作系统上修改配置使用Nginx。因此，即使本书以Linux系统为代表讲述Nginx，也不会影响使用其他操作系统的读者阅读，操作系统的差别相对于本书内容的影响实在是非常小。 勘误与支持由于作者的水平有限，加之编写的时间也很仓促，书中难免会出现一些错误或者不准确的地方，恳请读者批评指正。为此，我特意创建了一个在线支持与应急方案的二级站点：http://nginx.weebly.com 。读者可以将书中的错误发布在Bug勘误表页面中，同时如果读者遇到任何问题，也可以访问Q&amp;A页面，我将尽量在线上为读者提供最满意的解答。书中的全部源文件都将发布在这个网站上，我也会将相应的功能更新及时发布出来。如果你有更多的宝贵意见，也欢迎你发送邮件至我的邮箱&#114;&#117;&#x73;&#x73;&#101;&#x6c;&#108;&#116;&#x61;&#x6f;&#x40;&#102;&#111;&#120;&#109;&#x61;&#x69;&#x6c;&#x2e;&#x63;&#x6f;&#109;，期待能够听到读者的真挚反馈。 致谢我首先要感谢Igor Sysoev，他在Nginx设计上展现的功力令人折服，正是他的工作成果才有了本书诞生的意义。lisa是机械工业出版社华章公司的优秀编辑，非常值得信任。在这半年的写作过程中，她花费了很多时间、精力来阅读我的书稿，指出了许多文字上和格式上的错误，她提出的建议都大大提高了本书的可读性。在这半年时间里，一边工作一边写作给我带来了很大的压力，所以我要感谢我的父母在生活上对我无微不至的照顾，使我可以全力投入到写作中。繁忙的工作之余，写作又占用了休息时间的绝大部分，感谢我的太太毛业勤对我的体谅和鼓励，让我始终以高昂的斗志投入到本书的写作中。感谢我工作中的同事们，正是在与他们一起战斗在一线的日子里，我才不断地对技术有新地感悟；正是那些“充满激情的岁月，才使得我越来越热爱服务器技术的开发。谨以此书，献给我最亲爱的家人，以及众多热爱Nginx的朋友。—— 2015年10月 陶辉"},{"title":"推荐序","path":"/wiki/kubernetes-in-action/index.html","content":"2013年，Docker问世，由于其简练易用的使用范式，极大降低了容器技术的使用门槛，引爆了容器技术，一场轰轰烈烈的由容器带来的新计算革命开始了。 2014年，预见到了容器带来的革命性变化，七牛内部成立了一个新项目——QCOS，全称为QiniuCloudOperatingSystem（也可以理解为Qiniu Cluster Operating System），目标是实现一个数据中心操作系统，让开发人员使用数据中心如同使用一台机器一样容易。在当时，Kubernetes项目也刚刚开始，还在规格设计阶段。我们通读了Kubernetes的设计，决定自己干。这是一个非常疯狂的想法。促使我们决定自己干的原因有两点：一是我们存储也自己干，而且干成了，所以计算自己干，也不无成功的可能；二是Kubernetes刚开始，一切如果跟随Kubernetes，那么我们做起事情来肯定束手束脚，没办法按照商业公司的推进速度来推进。但是做着做着我们就发现，计算不像存储，计算问题是一个非常开放性的问题。而以容器为核心的计算系统，其复杂性也不同于以虚拟机（VM）为代表的计算系统，因为虚拟机（VM）是以虚拟一台机器为边界的，其问题域同样比较闭合。但是容器计算就是要打破机器的边界，让计算力在数据中心内自由调度。这个问题域涉及面非常广，除了常规的计算力调度、负载均衡、部署升级、服务发现、日志与监控，这些东西都要以全新的方式来解决。实际上给数据中心做一个操作系统并不是什么新想法。20世纪80年代中期，贝尔实验室就开启了一个名为Plan9的操作系统项目，目的就是做一个数据中心操作系统。参与这个操作系统的名单都大名鼎鼎：Rob Pike、Ken Thompson，等等。你没看错，就是今天创建了Go语言的那帮人。他们在Plan9项目解散后被Google抢了过去，换了一个思路继续做数据中心操作系统 —— 从面向数据中心的语言开始：Go语言就这样诞生了。而随着Go语言的流行，Docker、Kubernetes接连诞生，继续续写着数据中心操作系统的梦想。Kubernetes诞生之初，虽然嘴里衔着Google Borg系统的金钥匙，但是同期竞争的项目还是比较多的，除了七牛自己内部发起的QCOS外，比较知名的还有Docker Swam和Mesos。但是到了2016年，这场竞争越来越趋于Kubernetes一统天下。七牛内部QCOS项目也放弃了自研，将方向转向了Kubernetes阵营。QCOS项目对七牛有着特殊的意义，它是七牛业务多元化的开始。在此之前，七牛秉承专注做好一件事情，着眼于对象存储一个点，从单点切入，把单点做到极致的思路，获得了极佳的口碑，大量的移动互联网应用都选择了七牛作为它们的图片和视频托管的云服务提供商。选择做QCOS，实际上是我们在打第二个根据地时，选择了一条极其艰难的道路。今天我们QCOS团队（内部已更名为KIRK团队）发起了Kubernetes in Action一书的翻译，他们邀请我给译本作序，我脑子里不由自主想起了这段历史。选择基于Kubernetes，是我们从商业上来说的务实选择，但是它并不代表放弃自主研发，只是把梦想暂时封存在心里。中国古话说，师夷长技以制夷，别人的好东西我们是要学习的，学好了我们才能完成从模仿到超越的过程。Kubernetes的背后，是一场新计算革命，是真正的云计算2.0，我们期待更多有想法的开发者能够学习Kubernetes，能够加入这场计算革命。也欢迎大家加入七牛。是为序。 —— 许式伟 七牛云CEO"},{"title":"致谢","path":"/wiki/kubernetes-in-action/preview/preview_03.html","content":"在我开始写这本书之前，从来就没有想到将来会有这么多人参与进来，将一堆粗略的手稿整理成一本可以发行的书籍，我要感谢他们的帮助。首先，我要感谢Erin Twohey，是他指导了我写这本书。还有Michael Stephens，从我开始写书的第一天直到后来一年半的时间里，他始终给予我信心，并激励着我继续做下去。要感谢我的两位编辑。第一位编辑Andrew Warren，是他协助我完成了此书的首章，迈出了艰难的第一步。第二位编辑Elesha Hyde，庆幸有他接管Andrew的工作，和我并肩作战，直到完成本书的全部章节。我知道自己是一个难以沟通合作的人，但是他们表现出了极大的宽容与耐心。我要感谢Jeanne Boyarsky，他是我的第一个审稿人，在我写作期间，不断地审阅我的稿子，并给出很多有用的建议。后来又有了Jeanne和Elesha，他们三人让这本书的品质达到了预期的美好。如今我收到了大量外部读者与评论者的好评，这都是他们的功劳。要感谢我的技术校对Antonio Magnaghi，以及所有的外部审稿人：Al Krinker、Alessandro Campeis、Alexander Myltsev、Csaba Sari、David DiMaria、Elias Rangel、Erisk Zelenka、Fabrizio Cucci、Jared Dunca、 Keith Donaldson、Michael Bright、Paolo Antinori、Peter Perlepes和Tiklu Ganguly。在我心情糟糕到想要放弃写作时，他们的鼓励继续温暖我前行。另一方面，他们建设性的想法帮助我完善了那些因为精力有限而匆匆拼凑的章节。他们还积极指出了那些晦涩难懂的章节，并给出了如何优化的建议。这期间他们也提出了一些很好的问题，让我及时地意识到 自己的错误，并完善了手稿的初版。我还需要感谢一些读者，他们在Manning MEAP（Manning Early Access Program）购买了这本书的早期版本，并通过在线论坛讨论或者直接联系我的方式，给出了很多建议，特别是Vimal Kansal、Panoo Patierno和Roland Huß，他们发现了不少内容前后不一致和描述性错误。我还要感谢那些在Manning工作并参与过此书出版的所有人。我同样要感谢高中和大学期间的好友Ales Justin，他引荐我到Red Hat，结识了Cloud Enablement团队的同事。感谢他们，如果不是进入这家公司，进入这个团队，我不可能写这本书。最后，我要感谢我的妻子与孩子，在过去的18个月，我把自己反锁在办公室里，忽略了对他们的陪伴，即便这样，他们表现出了对我的莫大理解与支持。感谢他们所有人！"},{"title":"关于作者","path":"/wiki/kubernetes-in-action/preview/preview_05.html","content":"马尔科·卢克沙（Marko Lukša）是一位具有20年开发Web应用、ERP系统、框架以及中间件软件经验的专业软件工程师。1985年，六岁的他在父亲给他买的一台二手ZX光谱计算机（ZX Spectrum computer）上开始了编程的第一步。小学时，他是全国Logo语言编程大赛的冠军，他在参加编程夏令营时学习了Pascal编程。从那时起，他开始使用各种编程语言开发软件。 他从高中在万维网的早期就开始搭建动态网站。在斯洛文尼亚卢布尔雅那大学（University of Ljubljana）学习计算机科学期间，他在当地一家公司为医疗和电信行业开发软件。最终，他成为了Red Hat的员工，最初开发了谷歌应用程序引擎（Google App Engine）API的开源实现，该API底层使用了Red Hat的JBoss中间件产品。他还参与了CDI&#x2F;Weld、Infinispan&#x2F;JBoss DataGrid等项目。自2014年底以来，他一直是Red Hat Cloud Enablement团队的一员，在该团队中，他的职责包括了解Kubernetes和相关技术的最新发展趋势，保证公司的软件中间件充分利用Kubernetes和OpenShift的特性。"},{"title":"译者序","path":"/wiki/kubernetes-in-action/preview/preview_01.html","content":"早在2011年创业初期，七牛就决定使用GoLang作为主要开发语言，那时距离GoLang 1.0的正式发布还有将近一年的时间。当时我们就断定，在分布式时代，GoLang这种语言必定会大放异彩。今天，七牛的绝大多数线上服务都是用GoLang实现的，Golang帮助我们以最高的效率实现应用，快速响应客户需求。现在，我们也很幸运地看到当前最热门的开源项目如Docker、Kubernetes等，也都是基于GoLang来实现的。就像早期拥抱GoLang一样，七牛也是Docker和Kubernetes技术的坚定拥抱者和践行者。早在2014年，我们就基于Docker自研了一套容器集群管理系统，用于图片、音视频转码应用实例的资源调度，这套系统在线上运行了好几年，现在正逐渐被Kubernetes替代。Docker和Kubernetes的出现，让我们发现了构建数据中心操作系统（DCOS）的可能性。Docker的轻量级、Kubernetes的灵活性和开放性，能让我们以API调用的方式来交付计算力（包括CPU、内存、网络、磁 盘等），能让业务应用摆脱传统资源交付方式和既往运维手段的束缚，更快捷地得到部署、升级和管理，业务迭代速度大大加快，资源利用率大幅提高。早在几年前，我们在七牛就成立了专门的容器云团队，致力于打造更健壮、更易用的容器集群调度管理系统。现在，我们在七牛内部全面推广和应用Kubernetes，不仅把无状态服务运行在Kubernetes中，也把有状态服务比如数据库运行在Kubernetes中，正如使用GoLang提高了我们的开发效率一样，使用Kubernetes大大提高了我们的部署和运维效率。在七牛，我们坚定地认为，Kubernetes会成为下一个Linux，但是管理的不再是单台机器，而是以DCOS的方式来管理整个数据中心。熟练地掌握和使用Kubernetes，将成为每个前后端工程师的必备技能，Kubernetes将成为发布前后端服务的标准途径。这本书的翻译，我们集中了七牛容器云团队，以及其他七牛内部热心志愿者的力量，针对翻译的每个术语我们认真推敲，尽最大可能达 到“信”“达”“雅”的程度。鉴于水平有限，难免有纰漏，请读者谅解。希望这本书能带领你进入Kubernetes的世界。参与本书翻译的七牛容器团队成员有：卢兴铭（致谢词、第1章）、李雪瓅（第2章）、路涛（第3章）、孙讷敏（第4章）、刘岩（第5章）、孙毅飞（第6章）、林培裕（第7章）、周玉壁（第8章）、 陈凯俊（第9章）、杨冠军（第10章）、张钦尧（第11章）、况永巧 （第12章）、易弢（第13章）、王浩宇（第14章）、王雪瑞（第15章）、屈啸（第16章）、金鑫鑫（第17章）、陈忠杰（第18章）。由袁晓沛任翻译组组长，由李雪瓅、马力、冯义勇负责审校。袁晓沛 容器计算部技术总监"},{"title":"关于本书","path":"/wiki/kubernetes-in-action/preview/preview_04.html","content":"本书旨在让你能够熟练使用Kubernetes。它介绍了在Kubernetes中有效地开发和运行应用所需的几乎所有概念。在深入研究Kubernetes之前，本书概述了Docker等容器技术，包括如何构建容器，以便即使以前没有使用过这些技术的读者也可以使用它们。然后，它会慢慢带你从基本概念到实现原理了解大部分的Kubernetes知识。 本书适合谁本书主要关注应用开发人员，但也从操作的角度概述了应用的管理。它适合任何对在多服务器上运行和管理容器化应用感兴趣的人。对于希望学习容器技术以及大规模的容器编排的人，无论是初学者还是高级软件工程师都将得到在Kubernetes环境中开发、容器化和运行应用所需的专业知识。阅读本书不需要预先了解容器或Kubernetes技术。本书以渐进的方式展开主题，不会使用让非专家开发者难以理解的应用源代码。读者至少应该具备编程、计算机网络和运行Linux基本命令的基础知识，并了解常用的计算机协议，如HTTP协议。本书分为三个部分，涵盖18个章节。第一部分简要地介绍Docker和Kubernetes、如何设置Kubernetes集群，以及如何在集群中运行一个简单的应用。它包括两章： 第1章解释了什么是Kubernetes、Kubernetes的起源，以及它如何帮助解决当今大规模应用管理的问题。 第2章是关于如何构建容器镜像并在Kubernetes集群中运行的实践教程。还解释了如何运行本地单节点Kubernetes集群，以及在云上运行适当的多节点集群。 第二部分介绍了在Kubernetes中运行应用必须理解的关键概念。本章内容如下： 第3章介绍了Kubernetes的基本构建模块——pod，并解释了如何通过标签组织pod和其他Kubernetes对象。 第4章将向你介绍Kubernetes如何通过自动重启容器来保持应用程序的健康。还展示了如何正确地运行托管的pod，水平伸缩它们，使它们能够抵抗集群节点的故障，并在未来或定期运行它们。 第5章介绍了pod如何向运行在集群内外的客户端暴露它们提供的服务，还展示了运行在集群中的pod是如何发现和访问集群内外的服务的。 第6章解释了在同一个pod中运行的多个容器如何共享文件，以及如何管理持久化存储并使得pod可以访问。 第7章介绍了如何将配置数据和敏感信息（如凭据）传递给运行在pod中的应用。 第8章描述了应用如何获得正在运行的Kubernetes环境的信息，以及如何通过与Kubernetes通信来更改集群的状态。 第9章介绍了Deployment的概念，并解释了在Kubernetes环境中运行和更新应用的正确方法。 第三部分深入研究了Kubernetes集群的内部，介绍了一些额外的概念，并从更高的角度回顾了在前两部分中所学到的所有内容。这是最后一组章节： 第11章深入Kubernetes的底层，解释了组成Kubernetes集群的所有组件，以及每个组件的作用。它还解释了pod如何通过网络进行通信，以及服务如何跨多个pod形成负载平衡。 第12章解释了如何保护Kubernetes API服务器，以及通过扩展集群使用身份验证和授权。 第13章介绍了pod如何访问节点的资源，以及集群管理员如何防止pod访问节点的资源。 第14章深入了解限制每个应用程序允许使用的计算资源，配置应用的QoS（Quality of Service）保证，以及监控各个应用的资源使用情况。 还会介绍如何防止用户消耗太多资源。 第15章讨论了如何通过配置Kubernetes来自动伸缩应用运行的副本数，以及在当前集群节点数量不能接受任何新增应用时，如何对集群进行扩容。 第16章介绍了如何确保pod只被调度到特定的节点，或者如何防止它们被调度到其他节点。还介绍了如何确保pod被调度在一起，或者如何防止它们调度在一起。 第17章介绍了如何开发应用程序并部署在集群中。还介绍了如何配置开发和测试工作流来提高开发效率。 第18章介绍了如何使用自己的自定义对象扩展Kubernetes，以及其他人是如何开发并创建企业级应用平台的。 随着章节的深入，不仅可以了解单个构建Kubernetes的模块，还可以逐步增加对使用kubectl命令行工具的理解。 关于代码虽然这本书没有包含很多实际的源代码，但是包含了许多YAML格式的Kubernetes资源清单，以及shell命令及其输出。所有这些都是用等宽字体显示的，以便与普通文本相互区分。shell命令大部分以粗体文字展示，以便与命令的输出清晰地区分，但为了表示强调有时只有命令的最重要的部分或是命令输出的一部分是粗体的。在大多数情况下，为了适应图书有限的版面空间，命令输出会被重新格式化。此外，由于Kubernetes的命令行工具kubectl不断发展更新，新版本输出的信息可能会比书中显示的更多。如果输出结果不完全一致，请不要感到困惑。代码清单有时会包括续行标记（➥）表示一行文字延续到下一行。代码清单还可能包括注释，这些注释用于解释最重要的部分。在文本段落包含一些常见的元素，如Pod、ReplicationController、ReplicaSet、DaemonSet等，为了避免代码字体的过度复杂，便于阅读，都以常规字体显示。在某些地方，大写字母开头的“Pod”表示Pod资源，小写则表示实际运行的容器组。本书中的所有示例都使用谷歌Kubernetes引擎（Google Kubernetes Engine），在Kubernetes 1.8版本和Minikube的本地集群进行了运行测试。完整的源代码和YAML清单可以在https://github.com/luksa/kubernetes-in-action找到，或者从出版商的网站www.manning.com/books/kubernetes-in-action中下载。其他线上资源可以在下面的网址中找到许多额外的Kubernetes信息： Kubernetes官方网站：https://kubernetes.io 经常发布有用信息的Kubernetes博客：http://blog.kubernetes.io Kubernetes社区的Slack频道：http://slack.k8s.io Kubernetes以及Cloud Native Computing Foundation的YouTube频道： https://www.youtube.com/channel/UCZ2bu0qutTOM0tHYa_jkIwg https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA 为了获得对于单个话题更深入的理解或者贡献Kubernetes项目，可以查看Kubernetes特别兴趣小组（SIGs）:https://github.com/kubernetes/kubernetes/wiki/Special-InterestGroups-(SIGs)。 最后，由于Kubernetes是一个开源项目，Kubernetes源码本身包含大量的信息。可以访问Kubernetes和相关的仓库：https://github.com/kubernetes/。 读者服务轻松注册成为博文视点社区用户( www.broadview.com.cn )，扫码直达本书页面。提交勘误：您对书中内容的修改意见可在 提交勘误 处提交，若被采纳，将获赠博文视点社区积分（在您购买电子书时，积分可用来抵扣相应金额）。交流互动：在页面下方读者评论 处留下您的疑问或观点，与我们和其他读者一同学习交流。页面入口：http://www.broadview.com.cn/34995"},{"title":"前言","path":"/wiki/kubernetes-in-action/preview/preview_02.html","content":"在Red Hat工作了几年之后，2014年底，我被分配到一个叫Cloud Enablement的新团队。我们的任务是将公司的中间件系列产品引入基于Kubernetes开发的OpenShift容器平台。当时，Kubernetes还在初始的1.0 版本中，甚至还没有正式发布。我们团队必须尽快了解Kubernetes的细节，以便能够充分利用Kubernetes的一切，为软件开发设定正确的方向。当遇到问题时，很难判断是我们出错了，还是仅仅是碰到了一个Kubernetes的bug。从那以后，Kubernetes有了长足的发展，我对它的理解也有了很大的提升。当我第一次使用它的时候，大多数人甚至从未听说过Kubernetes。现在，几乎每个软件工程师都知道它，Kuberntes已经成为在云上和内部数据中心运行应用程序的增长最快和使用最广泛的方式之一。在使用Kubernetes的第一个月，我写了一篇包含两部分的博客文章，介绍如何在OpenShift&#x2F;Kubernetes中运行JBoss WildFly应用服务集群。当时，我从未想过一篇简单的博客文章会让曼宁出版社的人联系我，询问我是否愿意写一本关于Kubernetes的书。当然，我不能拒绝这样的提议，尽管我确信他们也会联系其他人，甚至最终会选择其他人。经过一年半的写作和研究，完成了本书，这是一次很棒的经历。写一本关于一项技术的书是比使用更好的了解它的方法。随着我对 Kubernetes了解的深入，以及Kubernetes本身的不断发展，我不断地回到之前写完的章节，添加更多的信息。我是一个完美主义者，所以永远不会对这本书感到绝对满意，但我很高兴听到MEAP（Manning Early Access Program）的许多读者觉得它是一本很好的学习Kubernetes的指南。我的目的是让读者了解技术本身，并教会他们如何使用工具有效地在Kubernetes集群中开发和部署应用程序。本书的重点不在如何建立和维护一个高可用的Kubernetes集群，但本书的最后一部分告诉读者这样一个集群应该包含什么，这能让大家很容易地理解处理这个问题的额外资源。希望你能享受阅读此书，并且让你学到如何能够充分利用Kubernetes系统的强大之处。"},{"title":"关于封面插画","path":"/wiki/kubernetes-in-action/preview/preview_06.html","content":"本书封面上的人物肖像名为“Divan的成员”（Member of the Divan）,Divan是指土耳其国务委员会或理事机构。图片来自1802年1月1日出版的由伦敦老邦德街的威廉·米勒（William Miller）创作的一套奥斯曼帝国服装设计作品集。作品集的标题页丢失了，至今没有找到。作品集的目录用英文和法文标出了这些人物，每幅插图上都有创作的两位艺术家的名字，艺术家们应该会为自己的作品出现在200年之后的一本电脑编程书的封面感到惊讶吧。作品集由Manning的一位编辑从位于曼哈顿西26街“车库”的一个古董跳蚤市场买的。卖家是一位居住在土耳其安卡拉的美国人，交易发生在他收拾摊子的时候。Manning的编辑当时身上没有足够的现金，信用卡和支票都被卖家礼貌地拒绝了。随着那天晚上卖主飞回安卡拉，情况变得令人绝望。后来，通过一种类似握手的旧式口头协议，交易出现了转机。卖主提议用电汇的方式把钱汇给他，然后编辑拿着一张包含银行 信息的纸和一叠照片走了出来。第二天我们把钱转了过去，我们仍然对这个陌生人的信任心存感激，印象深刻，让人想起很久以前发生的事情。Manning 以呈现两个世纪前丰富多彩的地区生活的书籍封面来颂扬 计算机产业的独创性和首创性，并让像这本画册这样的古籍和藏品中的绘画作品重现生机。"},{"title":"第二章 Nginx的配置","path":"/wiki/understanding-nginx:modules-development-and-architecture-resolving(second-edition)/section_01/chapter_02.html","content":"Nginx拥有大量官方发布的模块和第三方模块，这些已有的模块可以帮助我们实现Web服务器上很多的功能。使用这些模块时，仅仅需要增加、修改一些配置项即可。因此，本章的目的是熟悉Nginx的配置文件，包括配置文件的语法格式、运行所有Nginx服务必须具备的基础配置以及使用HTTP核心模块配置静态Web服务器的方法，最后还会介绍反向代理服务器。通过本章的学习，读者可以：熟练地配置一个静态Web服务器；对影响Web服务器性能的各个配置项有深入的理解；对配置语法有全面的了解。通过互联网或其他途径得到任意模块的配置说明，然后可通过修改nginx.conf文件来使用这些模块的功能。 2.1 运行中的Nginx进程间的关系在正式提供服务的产品环境下，部署Nginx时都是使用一个master进程来管理多个worker进程，一般情况下，worker进程的数量与服务器上的CPU核心数相等。每一个worker进程都是繁忙的，它们在真正地提供互联网服务，master进程则很“清闲”，只负责监控管理worker进程。worker进程之间通过共享内存、原子操作等一些进程间通信机制来实现负载均衡等功能（第9章将会介绍负载均衡机制，第14章将会介绍负载均衡锁的实现）。部署后Nginx进程间的关系如图2-1所示。Nginx是支持单进程（master进程）提供服务的，那么为什么产品环境下要按照master-worker方式配置同时启动多个进程呢？这样做的好处主要有以下两点： 由于master进程不会对用户请求提供服务，只用于管理真正提供服务的worker进程，所以master进程可以是唯一的，它仅专注于自己的纯管理工作，为管理员提供命令行服务，包括诸如启动服务、停止服务、重载配置文件、平滑升级程序等。master进程需要拥有较大的权限，例如，通常会利用root用户启动master进程。worker进程的权限要小于或等于master进程，这样master进程才可以完全地管理worker进程。当任意一个worker进程出现错误从而导致coredump时，master进程会立刻启动新的worker进程继续服务。 多个worker进程处理互联网请求不但可以提高服务的健壮性（一个worker进程出错后，其他worker进程仍然可以正常提供服务），最重要的是，这样可以充分利用现在常见的SMP多核架构，从而实现微观上真正的多核并发处理。因此，用一个进程（master进程）来处理互联网请求肯定是不合适的。另外，为什么要把worker进程数量设置得与CPU核心数量一致呢？这正是Nginx与Apache服务器的不同之处。在Apache上每个进程在一个时刻只处理一个请求，因此，如果希望Web服务器拥有并发处理的请求数更多，就要把Apache的进程或线程数设置得更多，通常会达到一台服务器拥有几百个工作进程，这样大量的进程间切换将带来无谓的系统资源消耗。而Nginx则不然，一个worker进程可以同时处理的请求数只受限于内存大小，而且在架构设计上，不同的worker进程之间处理并发请求时几乎没有同步锁的限制，worker进通常不会进入睡眠状态，因此，当Nginx上的进程数与CPU核心数相等时（最好每一个worker进程都绑定特定的CPU核心），进程间切换的代价是最小的。图2-1 部署后Nginx进程间的关系 举例来说，如果产品中的服务器CPU核心数为8，那么就需要配置8个worker进程（见图2-2）。 图2-2 worker进程的数量尽量与CPU核心数相等 如果对路径部分都使用默认配置，那么Nginx运行目录为&#x2F;usr&#x2F;local&#x2F;nginx，其目录结构如下。 123456789101112131415161718192021222324252627282930|---sbin| |---nginx|---conf| |---koi-win| |---koi-utf| |---win-utf| |---mime.types| |---mime.types.default| |---fastcgi_params| |---fastcgi_params.default| |---fastcgi.conf| |---fastcgi.conf.default| |---uwsgi_params| |---uwsgi_params.default| |---scgi_params| |---scgi_params.default| |---nginx.conf| |---nginx.conf.default|---logs| |---error.log| |---access.log| |---nginx.pid|---html| |---50x.html| |---index.html|---client_body_temp|---proxy_temp|---fastcgi_temp|---uwsgi_temp|---scgi_temp 2.2 Nginx配置的通用语法Nginx的配置文件其实是一个普通的文本文件。下面来看一个简单的例子。 1234567891011121314151617user nobody;worker_processes 8;error_log /var/log/nginx/error.log error;#pid logs/nginx.pid;events &#123; use epoll; worker_connections 50000;&#125;http &#123; include mime.types; default_type application/octet-stream; log_format main &#x27;$remote_addr [$time_local] &quot;$request&quot; &#x27; &#x27;$status $bytes_sent &quot;$http_referer&quot; &#x27; &#x27;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&#x27;; access_log logs/access.log main buffer=32k; ...｝ 在这段简短的配置代码中，每一行配置项的语法格式都将在2.2.2节介绍，出现的events和http块配置项将在2.2.1节介绍，以#符号开头的注释将在2.2.3节介绍，类似“buffer&#x3D;32k”这样的配置项的单位将在2.2.4节介绍。 2.2.1 块配置项块配置项由一个块配置项名和一对大括号组成。具体示例如下： 12345678910111213events &#123;... &#125;http &#123; upstream backend &#123; server 127.0.0.1:8080; &#125; gzip on; server &#123; ... location /webstatic &#123; gzip off; &#125; &#125;&#125; 上面代码段中的events、http、server、location、upstream等都是块配置项，块配置项之后是否如“location&#x2F;webstatic{…}”那样在后面加上参数，取决于解析这个块配置项的模块，不能一概而论，但块配置项一定会用大括号把一系列所属的配置项全包含进来，表示大括号内的配置项同时生效。所有的事件类配置都要在events块中，http、server等配置也遵循这个规定。块配置项可以嵌套。内层块直接继承外层块，例如，上例中，server块里的任意配置都是基于http块里的已有配置的。当内外层块中的配置发生冲突时，究竟是以内层块还是外层块的配置为准，取决于解析这个配置项的模块，第4章将会介绍http块内配置项冲突的处理方法。例如，上例在http模块中已经打开了“gzip on;”，但其下的location&#x2F;webstatic又把gzip关闭了：gzip off;，最终，在&#x2F;webstatic的处理模块中，gzip模块是按照gzip off来处理请求的。 2.2.2 配置项的语法格式从上文的示例可以看出，最基本的配置项语法格式如下： 123配置项名 配置项值1 配置项值2 ... ; 下面解释一下配置项的构成部分。首先，在行首的是配置项名，这些配置项名必须是Nginx的某一个模块想要处理的，否则Nginx会认为配置文件出现了非法的配置项名。配置项名输入结束后，将以空格作为分隔符。其次是配置项值，它可以是数字或字符串（当然也包括正则表达式）。针对一个配置项，既可以只有一个值，也可以包含多个值，配置项值之间仍然由空格符来分隔。当然，一个配置项对应的值究竟有多少个，取决于解析这个配置项的模块。我们必须根据某个Nginx模块对一个配置项的约定来更改配置项，第4章将会介绍模块是如何约定一个配置项的格式。最后，每行配置的结尾需要加上分号。 注意如果配置项值中包括语法符号，比如空格符，那么需要使用单引号或双引号括住配置项值，否则Nginx会报语法错误。例如：log_format main $remote_addr - $remote_user [$time_local] “$request” ; 2.2.3 配置项的注释如果有一个配置项暂时需要注释掉，那么可以加“#”注释掉这一行配置。例如： 1#pid logs/nginx.pid; 2.2.4 配置项的单位大部分模块遵循一些通用的规定，如指定空间大小时不用每次都定义到字节、指定时间时不用精确到毫秒。当指定空间大小时，可以使用的单位包括： K或者k千字节（KiloByte，KB）。 M或者m兆字节（MegaByte，MB）。例如：12gzip_buffers 4 8k;client_max_body_size 64M; 当指定时间时，可以使用的单位包括： ms（毫秒），s（秒），m（分钟），h（小时），d（天），w（周，包含7天），M（月，包含30天），y（年，包含365天）。例如：123expires 10y;proxy_read_timeout 600;client_body_timeout 2m; 注意配置项后的值究竟是否可以使用这些单位，取决于解析该配置项的模块。如果这个模块使用了Nginx框架提供的相应解析配置项方法，那么配置项值才可以携带单位。第4章中详细描述了Nginx框架提供的14种预设解析方法，其中一些方法将可以解析以上列出的单位。 2.2.5 在配置中使用变量有些模块允许在配置项中使用变量，如在日志记录部分，具体示例如下。 123log_format main &#x27;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &#x27; &#x27;$status $bytes_sent &quot;$http_referer&quot; &#x27; &#x27;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&#x27;; 其中，remote_addr是一个变量，使用它的时候前面要加上$符号。需要注意的是，这种变量只有少数模块支持，并不是通用的。许多模块在解析请求时都会提供多个变量（如本章后面提到的http core module、http proxy module、http upstream module等），以使其他模块的配置可以即时使用。我们在学习某个模块提供的配置说明时可以关注它是否提供变量。 提示在执行configure命令时，我们已经把许多模块编译进Nginx中，但是否启用这些模块，一般取决于配置文件中相应的配置项。换句话说，每个Nginx模块都有自己感兴趣的配置项，大部分模块都必须在nginx.conf中读取某个配置项后才会在运行时启用。例如，只有当配置http{…}这个配置项时，ngx_http_module模块才会在Nginx中启用，其他依赖ngx_http_module的模块也才能正常使用。 2.3 Nginx服务的基本配置Nginx在运行时，至少必须加载几个核心模块和一个事件类模块。这些模块运行时所支持的配置项称为基本配置——所有其他模块执行时都依赖的配置项。下面详述基本配置项的用法。由于配置项较多，所以把它们按照用户使用时的预期功能分成了以下4类： 用于调试、定位问题的配置项。 正常运行的必备配置项。 优化性能的配置项。 事件类配置项（有些事件类配置项归纳到优化性能类，这是因为它们虽然也属于events{}块，但作用是优化性能）。 有这么一些配置项，即使没有显式地进行配置，它们也会有默认的值，如daemon，即使在nginx.conf中没有对它进行配置，也相当于打开了这个功能，这点需要注意。对于这样的配置项，作者会在下面相应的配置项描述上加入一行“默认：”来进行说明。 2.3.1 用于调试进程和定位问题的配置项先来看一下用于调试进程、定位问题的配置项，如下所示。 是否以守护进程方式运行Nginx语法： daemon on|off;默认： daemon on;守护进程（daemon）是脱离终端并且在后台运行的进程。它脱离终端是为了避免进程执行过程中的信息在任何终端上显示，这样一来，进程也不会被任何终端所产生的信息所打断。Nginx毫无疑问是一个需要以守护进程方式运行的服务，因此，默认都是以这种方式运行的。不过Nginx还是提供了关闭守护进程的模式，之所以提供这种模式，是为了方便跟踪调试Nginx，毕竟用gdb调试进程时最烦琐的就是如何继续跟进fork出的子进程了。这在第三部分研究Nginx架构时很有用。 是否以master&#x2F;worker方式工作语法： master_process on|off;默认： master_process on;可以看到，在如图2-1所示的产品环境中，是以一个master进程管理多个worker进程的方式运行的，几乎所有的产品环境下，Nginx都以这种方式工作。与daemon配置相同，提供master_process配置也是为了方便跟踪调试Nginx。如果用off关闭了master_process方式，就不会fork出worker子进程来处理请求，而是用master进程自身来处理请求。 error日志的设置语法： error_log/path/file level;默认： error_log logs/error.log error;error日志是定位Nginx问题的最佳工具，我们可以根据自己的需求妥善设置error日志的路径和级别。&#x2F;path&#x2F;file参数可以是一个具体的文件，例如，默认情况下是logs&#x2F;error.log文件，最好将它放到一个磁盘空间足够大的位置；&#x2F;path&#x2F;file也可以是&#x2F;dev&#x2F;null，这样就不会输出任何日志了，这也是关闭error日志的唯一手段；&#x2F;path&#x2F;file也可以是stderr，这样日志会输出到标准错误文件中。level是日志的输出级别，取值范围是debug、info、notice、warn、error、crit、alert、emerg，从左至右级别依次增大。当设定为一个级别时，大于或等于该级别的日志都会被输出到&#x2F;path&#x2F;file文件中，小于该级别的日志则不会输出。例如，当设定为error级别时，error、crit、alert、emerg级别的日志都会输出。如果设定的日志级别是debug，则会输出所有的日志，这样数据量会很大，需要预先确保&#x2F;path&#x2F;file所在磁盘有足够的磁盘空间。注意如果日志级别设定到debug，必须在configure时加入–with-debug配置项。 是否处理几个特殊的调试点语法： debug_points[stop|abort]这个配置项也是用来帮助用户跟踪调试Nginx的。它接受两个参数：stop和abort。Nginx在一些关键的错误逻辑中（Nginx 1.0.14版本中有8处）设置了调试点。如果设置了debug_points为stop，那么Nginx的代码执行到这些调试点时就会发出SIGSTOP信号以用于调试。如果debug_points设置为abort，则会产生一个coredump文件，可以使用gdb来查看Nginx当时的各种信息。通常不会使用这个配置项。 仅对指定的客户端输出debug级别的日志语法： debug_connection[IP|CIDR]这个配置项实际上属于事件类配置，因此，它必须放在events{…} 中才有效。它的值可以是IP地址或CIDR地址，例如：1234events &#123; debug_connection 10.224.66.14; debug_connection 10.224.57.0/24;&#125; 这样，仅仅来自以上IP地址的请求才会输出debug级别的日志，其他请求仍然沿用error_log中配置的日志级别。上面这个配置对修复Bug很有用，特别是定位高并发请求下才会发生的问题。注意使用debug_connection前，需确保在执行configure时已经加入了–with-debug参数，否则不会生效。 限制coredump核心转储文件的大小语法： worker_rlimit_core size;在Linux系统中，当进程发生错误或收到信号而终止时，系统会将进程执行时的内存内容（核心映像）写入一个文件（core文件），以作为调试之用，这就是所谓的核心转储（core dumps）。当Nginx进程出现一些非法操作（如内存越界）导致进程直接被操作系统强制结束时，会生成核心转储core文件，可以从core文件获取当时的堆栈、寄存器等信息，从而帮助我们定位问题。但这种core文件中的许多信息不一定是用户需要的，如果不加以限制，那么可能一个core文件会达到几GB，这样随便coredumps几次就会把磁盘占满，引发严重问题。通过worker_rlimit_core配置可以限制core文件的大小，从而有效帮助用户定位问题。 指定coredump文件生成目录语法： working_directory path;worker进程的工作目录。这个配置项的唯一用途就是设置coredump文件所放置的目录，协助定位问题。因此，需确保worker进程有权限向working_directory指定的目录中写入文件。 2.3.2 正常运行的配置项下面是正常运行的配置项的相关介绍。 定义环境变量语法： env VAR|VAR=VALUE这个配置项可以让用户直接设置操作系统上的环境变量。例如：1env TESTPATH=/tmp/; 嵌入其他配置文件语法： include/path/file;include配置项可以将其他配置文件嵌入到当前的nginx.conf文件中，它的参数既可以是绝对路径，也可以是相对路径（相对于Nginx的配置目录，即nginx.conf所在的目录），例如：12include mime.types;include vhost/*.conf; 可以看到，参数的值可以是一个明确的文件名，也可以是含有通配符*的文件名，同时可以一次嵌入多个配置文件。 pid文件的路径语法： pid path/file;默认： pid logs/nginx.pid;保存master进程ID的pid文件存放路径。默认与configure执行时的参数“–pid-path”所指定的路径是相同的，也可以随时修改，但应确保Nginx有权在相应的目标中创建pid文件，该文件直接影响Nginx是否可以运行。 Nginx worker进程运行的用户及用户组语法： user username[groupname];默认： user nobody nobody;user用于设置master进程启动后，fork出的worker进程运行在哪个用户和用户组下。当按照“user username;”设置时，用户组名与用户名相同。若用户在configure命令执行时使用了参数–user&#x3D;username和–group&#x3D;groupname，此时nginx.conf将使用参数中指定的用户和用户组。 指定Nginx worker进程可以打开的最大句柄描述符个数语法： worker_rlimit_nofile limit;设置一个worker进程可以打开的最大文件句柄数。 限制信号队列语法： worker_rlimit_sigpending limit;设置每个用户发往Nginx的信号队列的大小。也就是说，当某个用户的信号队列满了，这个用户再发送的信号量会被丢掉。 2.3.3 优化性能的配置项下面是优化性能的配置项的相关介绍。 Nginx worker进程个数语法： worker_processes number;默认： worker_processes 1;在master&#x2F;worker运行方式下，定义worker进程的个数。worker进程的数量会直接影响性能。那么，用户配置多少个worker进程才好呢？这实际上与业务需求有关。每个worker进程都是单线程的进程，它们会调用各个模块以实现多种多样的功能。如果这些模块确认不会出现阻塞式的调用，那么，有多少CPU内核就应该配置多少个进程；反之，如果有可能出现阻塞式调用，那么需要配置稍多一些的worker进程。例如，如果业务方面会致使用户请求大量读取本地磁盘上的静态资源文件，而且服务器上的内存较小，以至于大部分的请求访问静态资源文件时都必须读取磁盘（磁头的寻址是缓慢的），而不是内存中的磁盘缓存，那么磁盘I&#x2F;O调用可能会阻塞住worker进程少量时间，进而导致服务整体性能下降。多worker进程可以充分利用多核系统架构，但若worker进程的数量多于CPU内核数，那么会增大进程间切换带来的消耗（Linux是抢占式内核）。一般情况下，用户要配置与CPU内核数相等的worker进程，并且使用下面的worker_cpu_affinity配置来绑定CPU内核。 绑定Nginx worker进程到指定的CPU内核语法： worker_cpu_affinity cpumask[cpumask...]为什么要绑定worker进程到指定的CPU内核呢？假定每一个worker进程都是非常繁忙的，如果多个worker进程都在抢同一个CPU，那么这就会出现同步问题。反之，如果每一个worker进程都独享一个CPU，就在内核的调度策略上实现了完全的并发。例如，如果有4颗CPU内核，就可以进行如下配置：12worker_processes 4;worker_cpu_affinity 1000 0100 0010 0001; 注意worker_cpu_affinity配置仅对Linux操作系统有效。Linux操作系统使用sched_setaffinity()系统调用实现这个功能。 SSL硬件加速语法： ssl_engine device；如果服务器上有SSL硬件加速设备，那么就可以进行配置以加快SSL协议的处理速度。用户可以使用OpenSSL提供的命令来查看是否有SSL硬件加速设备：1openssl engine -t 系统调用gettimeofday的执行频率语法： timer_resolution t;默认情况下，每次内核的事件调用（如epoll、select、poll、kqueue等）返回时，都会执行一次gettimeofday，实现用内核的时钟来更新Nginx中的缓存时钟。在早期的Linux内核中，gettimeofday的执行代价不小，因为中间有一次内核态到用户态的内存复制。当需要降低gettimeofday的调用频率时，可以使用timer_resolution配置。例如，“timer_resolution 100ms；”表示至少每100ms才调用一次gettimeofday。但在目前的大多数内核中，如x86-64体系架构，gettimeofday只是一次vsyscall，仅仅对共享内存页中的数据做访问，并不是通常的系统调用，代价并不大，一般不必使用这个配置。而且，如果希望日志文件中每行打印的时间更准确，也可以使用它。 Nginx worker进程优先级设置语法： worker_priority nice;默认： worker_priority 0;该配置项用于设置Nginx worker进程的nice优先级。在Linux或其他类UNIX操作系统中，当许多进程都处于可执行状态时，将按照所有进程的优先级来决定本次内核选择哪一个进程执行。进程所分配的CPU时间片大小也与进程优先级相关，优先级越高，进程分配到的时间片也就越大（例如，在默认配置下，最小的时间片只有5ms，最大的时间片则有800ms）。这样，优先级高的进程会占有更多的系统资源。优先级由静态优先级和内核根据进程执行情况所做的动态调整（目前只有±5的调整）共同决定。nice值是进程的静态优先级，它的取值范围是–20~+19，–20是最高优先级，+19是最低优先级。因此，如果用户希望Nginx占有更多的系统资源，那么可以把nice值配置得更小一些，但不建议比内核进程的nice值（通常为–5）还要小。 2.3.4 事件类配置项下面是事件类配置项的相关介绍。 是否打开accept锁语法： accept_mutex[on|off]默认： accept_mutext on;accept_mutex是Nginx的负载均衡锁，本书会在第9章事件处理框架中详述Nginx是如何实现负载均衡的。这里，读者仅需要知道accept_mutex这把锁可以让多个worker进程轮流地、序列化地与新的客户端建立TCP连接。当某一个worker进程建立的连接数量达到worker_connections配置的最大连接数的7&#x2F;8时，会大大地减小该worker进程试图建立新TCP连接的机会，以此实现所有worker进程之上处理的客户端请求数尽量接近。accept锁默认是打开的，如果关闭它，那么建立TCP连接的耗时会更短，但worker进程之间的负载会非常不均衡，因此不建议关闭它。 lock文件的路径语法： lock_file path/file;默认： lock_file logs/nginx.lock;accept锁可能需要这个lock文件，如果accept锁关闭，lock_file配置完全不生效。如果打开了accept锁，并且由于编译程序、操作系统架构等因素导致Nginx不支持原子锁，这时才会用文件锁实现accept锁 （14.8.1节将会介绍文件锁的用法），这样lock_file指定的lock文件才会生效。注意在基于i386、AMD64、Sparc64、PPC64体系架构的操作系统上，若使用GCC、Intel C++、SunPro C++编译器来编译Nginx，则可以肯定这时的Nginx是支持原子锁的，因为Nginx会利用CPU的特性并用汇编语言来实现它（可以参考14.3节x86架构下原子操作的实现）。这时的lock_file配置是没有意义的。 使用accept锁后到真正建立连接之间的延迟时间语法： accept_mutex_delay Nms;默认： accept_mutex_delay 500ms;在使用accept锁后，同一时间只有一个worker进程能够取到accept锁。这个accept锁不是阻塞锁，如果取不到会立刻返回。如果有一个worker进程试图取accept锁而没有取到，它至少要等accept_mutex_delay定义的时间间隔后才能再次试图取锁。 批量建立新连接语法： multi_accept[on|off];默认： multi_accept off;当事件模型通知有新连接时，尽可能地对本次调度中客户端发起的所有TCP请求都建立连接。 选择事件模型语法： use[kqueue|rtsig|epoll|/dev/poll|select|poll|eventport];默认： Nginx会自动使用最适合的事件模型。对于Linux操作系统来说，可供选择的事件驱动模型有poll、select、epoll三种。epoll当然是性能最高的一种，在9.6节会解释epoll为什么可以处理大并发连接。 每个worker的最大连接数语法： worker_connections number;定义每个worker进程可以同时处理的最大连接数。 2.4 用HTTP核心模块配置一个静态Web服务器静态Web服务器的主要功能由ngx_http_core_module模块（HTTP框架的主要成员）实现，当然，一个完整的静态Web服务器还有许多功能是由其他的HTTP模块实现的。本节主要讨论如何配置一个包含基本功能的静态Web服务器，文中会完整地说明ngx_http_core_module模块提供的配置项及变量的用法，但不会过多说明其他HTTP模块的配置项。在阅读完本节内容后，读者应当可以通过简单的查询相关模块（如ngx_http_gzip_filter_module、ngx_http_image_filter_module等）的配置项说明，方便地在nginx.conf配置文件中加入新的配置项，从而实现更多的Web服务器功能。除了2.3节提到的基本配置项外，一个典型的静态Web服务器还会包含多个server块和location块，例如： 123456789101112131415161718192021222324http &#123; gzip on; upstream &#123; ... &#125; ... server &#123; listen localhost:80; ... location /webstatic &#123; if ... &#123; ... &#125; root /opt/webresource; ... &#125; location ~* .(jpg|jpeg|png|jpe|gif)$ &#123; ... &#125; &#125; server &#123; ... &#125;&#125; 所有的HTTP配置项都必须直属于http块、server块、location块、upstream块或if块等（HTTP配置项自然必须全部在http{}块之内，这里的“直属于”是指配置项直接所属的大括号对应的配置块），同时，在描述每个配置项的功能时，会说明它可以在上述的哪个块中存在，因为有些配置项可以任意地出现在某一个块中，而有些配置项只能出现在特定的块中，在第4章介绍自定义配置项的读取时，相信读者就会体会到这种设计思路。Nginx为配置一个完整的静态Web服务器提供了非常多的功能，下面会把这些配置项分为以下8类进行详述：虚拟主机与请求的分发、文件路径的定义、内存及磁盘资源的分配、网络连接的设置、MIME类型的设置、对客户端请求的限制、文件操作的优化、对客户端请求的特殊处理。这种划分只是为了帮助大家从功能上理解这些配置项。在这之后会列出ngx_http_core_module模块提供的变量，以及简单说明它们的意义。 2.4.1 虚拟主机与请求的分发由于IP地址的数量有限，因此经常存在多个主机域名对应着同一个IP地址的情况，这时在nginx.conf中就可以按照server_name（对应用户请求中的主机域名）并通过server块来定义虚拟主机，每个server块就是一个虚拟主机，它只处理与之相对应的主机域名请求。这样，一台服务器上的Nginx就能以不同的方式处理访问不同主机域名的HTTP请求了。 监听端口语法： listen address:port[default(deprecated in 0.8.21)|default_server|[backlog=num|rcvbuf=size|sndbuf=size|accept_filter=filter|deferred|bind|ipv6only=[on|off]|ssl]];默认： listen 80;配置块： serverlisten参数决定Nginx服务如何监听端口。在listen后可以只加IP地址、端口或主机名，非常灵活，例如：12345listen 127.0.0.1:8000;listen 127.0.0.1; #注意：不加端口时，默认监听80端口listen 8000;listen *:8000;listen localhost:8000; 如果服务器使用IPv6地址，那么可以这样使用：123listen [::]:8000;listen [fe80::1];listen [:::a8c9:1234]:80; 在地址和端口后，还可以加上其他参数，例如：12listen 443 default_server ssl;listen 127.0.0.1 default_server accept_filter=dataready backlog=1024; 下面说明listen可用参数的意义。 default：将所在的server块作为整个Web服务的默认server块。如果没有设置这个参数，那么将会以在nginx.conf中找到的第一个server块作为默认server块。为什么需要默认虚拟主机呢？当一个请求无法匹配配置文件中的所有主机域名时，就会选用默认的虚拟主机（在11.3节介绍默认主机的使用）。 default_server：同上。 backlog&#x3D;num：表示TCP中backlog队列的大小。默认为–1，表示不予设置。在TCP建立三次握手过程中，进程还没有开始处理监听句柄，这时backlog队列将会放置这些新连接。可如果backlog队列已满，还有新的客户端试图通过三次握手建立TCP连接，这时客户端将会建立连接失败。 rcvbuf&#x3D;size：设置监听句柄的SO_RCVBUF参数。 sndbuf&#x3D;size：设置监听句柄的SO_SNDBUF参数。 accept_filter：设置accept过滤器，只对FreeBSD操作系统有用。 deferred：在设置该参数后，若用户发起建立连接请求，并且完成了TCP的三次握手，内核也不会为了这次的连接调度worker进程来处理，只有用户真的发送请求数据时（内核已经在网卡中收到请求数据包），内核才会唤醒worker进程处理这个连接。这个参数适用于大并发的情况下，它减轻了worker进程的负担。当请求数据来临时，worker进程才会开始处理这个连接。只有确认上面所说的应用场景符合自己的业务需求时，才可以使用deferred配置。 bind：绑定当前端口&#x2F;地址对，如127.0.0.1:8000。只有同时对一个端口监听多个地址时才会生效。 ssl：在当前监听的端口上建立的连接必须基于SSL协议。 主机名称语法： server_name name[...];默认： server_name&quot;&quot;;配置块： serverserver_name后可以跟多个主机名称，如server_name www.testweb.com 、download.testweb.com;。在开始处理一个HTTP请求时，Nginx会取出header头中的Host，与每个server中的server_name进行匹配，以此决定到底由哪一个server块来处理这个请求。有可能一个Host与多个server块中的server_name都匹配，这时就会根据匹配优先级来选择实际处理的server块。server_name与Host的匹配优先级如下： 首先选择所有字符串完全匹配的server_name，如www.testweb.com 。 其次选择通配符在前面的server_name，如*.testweb.com。 再次选择通配符在后面的server_name，如www.testweb.* 。 最后选择使用正则表达式才匹配的server_name，如~^.testweb.com$。实际上，这个规则正是7.7节中介绍的带通配符散列表的实现依据，同时，在10.4节也介绍了虚拟主机配置的管理。如果Host与所有的server_name都不匹配，这时将会按下列顺序选择处理的server块。 优先选择在listen配置项后加入[default|default_server]的server块。 找到匹配listen端口的第一个server块。如果server_name后跟着空字符串（如server_name””;），那么表示匹配没有Host这个HTTP头部的请求。 注意Nginx正是使用server_name配置项针对特定Host域名的请求提供不同的服务，以此实现虚拟主机功能。 server_names_hash_bucket_size语法： server_names_hash_bucket_size size;默认： server_names_hash_bucket_size 32|64|128;配置块： http、server、location为了提高快速寻找到相应server name的能力，Nginx使用散列表来存储server name。server_names_hash_bucket_size设置了每个散列桶占用的内存大小。 server_names_hash_max_size语法： server_names_hash_max_size size;默认： server_names_hash_max_size 512;配置块： http、server、locationserver_names_hash_max_size会影响散列表的冲突率。 server_names_hash_max_size越大，消耗的内存就越多，但散列key的冲突率则会降低，检索速度也更快。server_names_hash_max_size越小，消耗的内存就越小，但散列key的冲突率可能增高。 重定向主机名称的处理语法： server_name_in_redirect on|off;默认： server_name_in_redirect on;配置块： http、server或者location该配置需要配合server_name使用。在使用on打开时，表示在重定向请求时会使用server_name里配置的第一个主机名代替原先请求中的Host头部，而使用off关闭时，表示在重定向请求时使用请求本身的Host头部。 location语法： location[=|~|~*|^~|@]/uri/&#123;...&#125;配置块： serverlocation会尝试根据用户请求中的URI来匹配上面的&#x2F;uri表达式，如果可以匹配，就选择location{}块中的配置来处理用户请求。当然，匹配方式是多样的，下面介绍location的匹配规则。 &#x3D;表示把URI作为字符串，以便与参数中的uri做完全匹配。例如：1234location = / &#123; # 只有当用户请求是 /时，才会使用该location下的配置 ...&#125; ~表示匹配URI时是字母大小写敏感的。 ~*表示匹配URI时忽略字母大小写问题。 ^~表示匹配URI时只需要其前半部分与uri参数匹配即可。例如：1234location ^~ /images/ &#123; # 以/images/开始的请求都会匹配上 ...&#125; @表示仅用于Nginx服务内部请求之间的重定向，带有@的location不直接处理用户请求。当然，在uri参数里是可以用正则表达式的，例如：1234location ~* \\.(gif|jpg|jpeg)$ &#123; # 匹配以.gif、.jpg、.jpeg结尾的请求 ...&#125; 注意，location是有顺序的，当一个请求有可能匹配多个location时，实际上这个请求会被第一个location处理。在以上各种匹配方式中，都只能表达为“如果匹配…则…”。如果需要表达“如果不匹配…则…”，就很难直接做到。有一种解决方法是在最后一个location中使用&#x2F;作为参数，它会匹配所有的HTTP请求，这样就可以表示如果不能匹配前面的所有location，则由“&#x2F;”这个location处理。例如：1234location / &#123; # /可以匹配所有请求 ...&#125; 2.4.2 文件路径的定义下面介绍一下文件路径的定义配置项。 以root方式设置资源路径语法： root path;默认： root html;配置块： http、server、location、if例如，定义资源文件相对于HTTP请求的根目录。123location /download/ &#123; root /opt/web/html/;&#125; 在上面的配置中，如果有一个请求的URI是&#x2F;download&#x2F;index&#x2F;test.html，那么Web服务器将会返回服务器上&#x2F;opt&#x2F;web&#x2F;html&#x2F;download&#x2F;index&#x2F;test.html文件的内容。 以alias方式设置资源路径语法： alias path;配置块： locationalias也是用来设置文件资源路径的，它与root的不同点主要在于如何解读紧跟location后面的uri参数，这将会致使alias与root以不同的方式将用户请求映射到真正的磁盘文件上。例如，如果有一个请求的URI是&#x2F;conf&#x2F;nginx.conf，而用户实际想访问的文件在&#x2F;usr&#x2F;local&#x2F;nginx&#x2F;conf&#x2F;nginx.conf，那么想要使用alias来进行设置的话，可以采用如下方式：123location /conf &#123; alias /usr/local/nginx/conf/;&#125; 如果用root设置，那么语句如下所示：123location /conf &#123; root /usr/local/nginx/;&#125; 使用alias时，在URI向实际文件路径的映射过程中，已经把location后配置的&#x2F;conf这部分字符串丢弃掉，因此，&#x2F;conf&#x2F;nginx.conf请求将根据alias path映射为path&#x2F;nginx.conf。root则不然，它会根据完整的URI请求来映射，因此，&#x2F;conf&#x2F;nginx.conf请求会根据root path映射为path&#x2F;conf&#x2F;nginx.conf。这也是root可以放置到http、server、location或if块中，而alias只能放置到location块中的原因。alias后面还可以添加正则表达式，例如：123location ~ ^/test/(\\w+)\\.(\\w+)$ &#123; alias /usr/local/nginx/$2/$1.$2;&#125; 这样，请求在访问&#x2F;test&#x2F;nginx.conf时，Nginx会返回&#x2F;usr&#x2F;local&#x2F;nginx&#x2F;conf&#x2F;nginx.conf文件中的内容。 访问首页语法： index file...;默认： index index.html;配置块： http、server、location有时，访问站点时的URI是&#x2F;，这时一般是返回网站的首页，而这与root和alias都不同。这里用ngx_http_index_module模块提供的index配置实现。index后可以跟多个文件参数，Nginx将会按照顺序来访问这些文件，例如：1234location / &#123; root path; index /index.html /html/index.php /index.php;&#125; 接收到请求后，Nginx首先会尝试访问path&#x2F;index.php文件，如果可以访问，就直接返回文件内容结束请求，否则再试图返回path&#x2F;html&#x2F;index.php文件的内容，依此类推。 根据HTTP返回码重定向页面语法： error_page code[code...][=|=answer-code]uri|@named_location配置块： http、server、location、if当对于某个请求返回错误码时，如果匹配上了error_page中设置的code，则重定向到新的URI中。例如：1234error_page 404 /404.html;error_page 502 503 504 /50x.html;error_page 403 http://example.com/forbidden.html;error_page 404 = @fetch; 注意，虽然重定向了URI，但返回的HTTP错误码还是与原来的相同。用户可以通过“&#x3D;”来更改返回的错误码，例如：12error_page 404 =200 /empty.gif;error_page 404 =403 /forbidden.gif; 也可以不指定确切的返回错误码，而是由重定向后实际处理的真实结果来决定，这时，只要把“&#x3D;”后面的错误码去掉即可，例如：1error_page 404 = /empty.gif; 如果不想修改URI，只是想让这样的请求重定向到另一个location中进行处理，那么可以这样设置：123456location / ( error_page 404 @fallback;)location @fallback ( proxy_pass http://backend;) 这样，返回404的请求会被反向代理到http://backend 上游服务器中处理。 是否允许递归使用error_page语法： recursive_error_pages[on|off];默认： recursive_error_pages off;配置块： http、server、location确定是否允许递归地定义error_page。 try_files语法： try_files path1[path2]uri;配置块： server、locationtry_files后要跟若干路径，如path1 path2…，而且最后必须要有uri参数，意义如下：尝试按照顺序访问每一个path，如果可以有效地读取，就直接向用户返回这个path对应的文件结束请求，否则继续向下访问。如果所有的path都找不到有效的文件，就重定向到最后的参数uri上。因此，最后这个参数uri必须存在，而且它应该是可以有效重定向的。例如：1234try_files /system/maintenance.html $uri $uri/index.html $uri.html @other;location @other &#123; proxy_pass http://backend;&#125; 上面这段代码表示如果前面的路径，如&#x2F;system&#x2F;maintenance.html等，都找不到，就会反向代理到http://backend 服务上。还可以用指定错误码的方式与error_page配合使用，例如：123location / &#123;try_files $uri $uri/ /error.phpc=404 =404;&#125; 2.4.3 内存及磁盘资源的分配下面介绍处理请求时内存、磁盘资源分配的配置项。 HTTP包体只存储到磁盘文件中语法： client_body_in_file_only on|clean|off;默认： client_body_in_file_only off;配置块： http、server、location当值为非off时，用户请求中的HTTP包体一律存储到磁盘文件中，即使只有0字节也会存储为文件。当请求结束时，如果配置为on，则这个文件不会被删除（该配置一般用于调试、定位问题），但如果配置为clean，则会删除该文件。 HTTP包体尽量写入到一个内存buffer中语法： client_body_in_single_buffer on|off;默认： client_body_in_single_buffer off;配置块： http、server、location用户请求中的HTTP包体一律存储到内存buffer中。当然，如果HTTP包体的大小超过了下面client_body_buffer_size设置的值，包体还是会写入到磁盘文件中。 存储HTTP头部的内存buffer大小语法： client_header_buffer_size size;默认： client_header_buffer_size 1k;配置块： http、server上面配置项定义了正常情况下Nginx接收用户请求中HTTP header部分（包括HTTP行和HTTP头部）时分配的内存buffer大小。有时，请求中的HTTP header部分可能会超过这个大小，这时large_client_header_buffers定义的buffer将会生效。 存储超大HTTP头部的内存buffer大小语法： large_client_header_buffers number size;默认： large_client_header_buffers 48k;配置块： http、serverlarge_client_header_buffers定义了Nginx接收一个超大HTTP头部请求的buffer个数和每个buffer的大小。如果HTTP请求行（如GET&#x2F;index HTTP&#x2F;1.1）的大小超过上面的单个buffer，则返回”Request URI too large”(414)。请求中一般会有许多header，每一个header的大小也不能超过单个buffer的大小，否则会返回”Bad request”(400)。当然，请求行和请求头部的总和也不可以超过buffer个数*buffer大小。 存储HTTP包体的内存buffer大小语法： client_body_buffer_size size;默认： client_body_buffer_size 8k/16k;配置块： http、server、location上面配置项定义了Nginx接收HTTP包体的内存缓冲区大小。也就是说，HTTP包体会先接收到指定的这块缓存中，之后才决定是否写入磁盘。{ % note color:light 注意 如果用户请求中含有HTTP头部Content-Length，并且其标识的长度小于定义的buffer大小，那么Nginx会自动降低本次请求所使用的内存buffer，以降低内存消耗。 %} HTTP包体的临时存放目录语法： client_body_temp_path dir-path[level1[level2[level3]]]默认： client_body_temp_path client_body_temp;配置块： http、server、location上面配置项定义HTTP包体存放的临时目录。在接收HTTP包体时，如果包体的大小大于client_body_buffer_size，则会以一个递增的整数命名并存放到client_body_temp_path指定的目录中。后面跟着的level1、level2、level3，是为了防止一个目录下的文件数量太多，从而导致性能下降，因此使用了level参数，这样可以按照临时文件名最多再加三层目录。例如：1client_body_temp_path /opt/nginx/client_temp 1 2; 如果新上传的HTTP包体使用00000123456作为临时文件名，就会被存放在这个目录中。&#x2F;opt&#x2F;nginx&#x2F;client_temp&#x2F;6&#x2F;45&#x2F;00000123456 connection_pool_size语法： connection_pool_size size;默认： connection_pool_size 256;配置块： http、serverNginx对于每个建立成功的TCP连接会预先分配一个内存池，上面的size配置项将指定这个内存池的初始大小（即ngx_connection_t结构体中的pool内存池初始大小，9.8.1节将介绍这个内存池是何时分配的），用于减少内核对于小块内存的分配次数。需慎重设置，因为更大的size会使服务器消耗的内存增多，而更小的size则会引发更多的内存分配次数。 request_pool_size语法： request_pool_size size;默认： request_pool_size 4k;配置块： http、serverNginx开始处理HTTP请求时，将会为每个请求都分配一个内存池，size配置项将指定这个内存池的初始大小（即ngx_http_request_t结构体中的pool内存池初始大小，11.3节将介绍这个内存池是何时分配的），用于减少内核对于小块内存的分配次数。TCP连接关闭时会销毁connection_pool_size指定的连接内存池，HTTP请求结束时会销毁request_pool_size指定的HTTP请求内存池，但它们的创建、销毁时间并不一致，因为一个TCP连接可能被复用于多个HTTP请求。8.7节会详述内存池原理。 2.4.4 网络连接的设置下面介绍网络连接的设置配置项。 读取HTTP头部的超时时间语法： client_header_timeout time（默认单位：秒）;默认： client_header_timeout 60;配置块： http、server、location客户端与服务器建立连接后将开始接收HTTP头部，在这个过程中，如果在一个时间间隔（超时时间）内没有读取到客户端发来的字节，则认为超时，并向客户端返回408(“Request timed out”)响应。 读取HTTP包体的超时时间语法： client_body_timeout time（默认单位：秒）；默认： client_body_timeout 60;配置块： http、server、location此配置项与client_header_timeout相似，只是这个超时时间只在读取HTTP包体时才有效。 发送响应的超时时间语法： send_timeout time;默认： send_timeout 60;配置块： http、server、location这个超时时间是发送响应的超时时间，即Nginx服务器向客户端发送了数据包，但客户端一直没有去接收这个数据包。如果某个连接超过send_timeout定义的超时时间，那么Nginx将会关闭这个连接。 reset_timeout_connection语法： reset_timeout_connection on|off;默认： reset_timeout_connection off;配置块： http、server、location连接超时后将通过向客户端发送RST包来直接重置连接。这个选项打开后，Nginx会在某个连接超时后，不是使用正常情形下的四次握手关闭TCP连接，而是直接向用户发送RST重置包，不再等待用户的应答，直接释放Nginx服务器上关于这个套接字使用的所有缓存（如TCP滑动窗口）。相比正常的关闭方式，它使得服务器避免产生许多处于FIN_WAIT_1、FIN_WAIT_2、TIME_WAIT状态的TCP连接。注意，使用RST重置包关闭连接会带来一些问题，默认情况下不会开启。 lingering_close语法： lingering_close off|on|always;默认： lingering_close on;配置块： http、server、location该配置控制Nginx关闭用户连接的方式。always表示关闭用户连接前必须无条件地处理连接上所有用户发送的数据。off表示关闭连接时完全不管连接上是否已经有准备就绪的来自用户的数据。on是中间值，一般情况下在关闭连接前都会处理连接上的用户发送的数据，除了有些情况下在业务上认定这之后的数据是不必要的。 lingering_time语法： lingering_time time;默认： lingering_time 30s;配置块： http、server、locationlingering_close启用后，这个配置项对于上传大文件很有用。上文讲过，当用户请求的Content-Length大于max_client_body_size配置时，Nginx服务会立刻向用户发送413（Request entity too large）响应。但是，很多客户端可能不管413返回值，仍然持续不断地上传HTTP body，这时，经过了lingering_time设置的时间后，Nginx将不管用户是否仍在上传，都会把连接关闭掉。 lingering_timeout语法： lingering_timeout time;默认： lingering_timeout 5s;配置块： http、server、locationlingering_close生效后，在关闭连接前，会检测是否有用户发送的数据到达服务器，如果超过lingering_timeout时间后还没有数据可读，就直接关闭连接；否则，必须在读取完连接缓冲区上的数据并丢弃掉后才会关闭连接。 对某些浏览器禁用keepalive功能语法： keepalive_disable[msie6|safari|none]...默认： keepalive_disablemsie6 safari配置块： http、server、locationHTTP请求中的keepalive功能是为了让多个请求复用一个HTTP长连接，这个功能对服务器的性能提高是很有帮助的。但有些浏览器，如IE 6和Safari，它们对于使用keepalive功能的POST请求处理有功能性问题。因此，针对IE 6及其早期版本、Safari浏览器默认是禁用keepalive功能的。 keepalive超时时间语法： keepalive_timeout time（默认单位：秒）;默认： keepalive_timeout 75;配置块： http、server、location一个keepalive连接在闲置超过一定时间后（默认的是75秒），服务器和浏览器都会去关闭这个连接。当然，keepalive_timeout配置项是用来约束Nginx服务器的，Nginx也会按照规范把这个时间传给浏览器，但每个浏览器对待keepalive的策略有可能是不同的。 一个keepalive长连接上允许承载的请求最大数语法： keepalive_requests n;默认： keepalive_requests 100;配置块： http、server、location一个keepalive连接上默认最多只能发送100个请求。 tcp_nodelay语法： tcp_nodelay on|off;默认： tcp_nodelay on;配置块： http、server、location确定对keepalive连接是否使用TCP_NODELAY选项。 tcp_nopush语法： tcp_nopush on|off;默认： tcp_nopush off;配置块： http、server、location在打开sendfile选项时，确定是否开启FreeBSD系统上的TCP_NOPUSH或Linux系统上的TCP_CORK功能。打开tcp_nopush后，将会在发送响应时把整个响应包头放到一个TCP包中发送。 2.4.5 MIME类型的设置下面是MIME类型的设置配置项。 MIME type与文件扩展的映射语法： type&#123;...&#125;;配置块： http、server、location定义MIME type到文件扩展名的映射。多个扩展名可以映射到同一个MIME type。例如： 123456types &#123; text/html html; text/html conf; image/gif gif; image/jpeg jpg;&#125; 默认MIME type语法： default_type MIME-type;默认： default_type text/plain;配置块： http、server、location当找不到相应的MIME type与文件扩展名之间的映射时，使用默认的MIME type作为HTTP header中的Content-Type。 types_hash_bucket_size语法： types_hash_bucket_size size;默认： types_hash_bucket_size 32|64|128;配置块： http、server、location为了快速寻找到相应MIME type，Nginx使用散列表来存储MIMEtype与文件扩展名。types_hash_bucket_size设置了每个散列桶占用的内存大小。 types_hash_max_size语法： types_hash_max_size size;默认： types_hash_max_size 1024;配置块： http、server、locationtypes_hash_max_size影响散列表的冲突率。types_hash_max_size越大，就会消耗更多的内存，但散列key的冲突率会降低，检索速度就更快。types_hash_max_size越小，消耗的内存就越小，但散列key的冲突率可能上升。 2.4.6 对客户端请求的限制下面介绍对客户端请求的限制的配置项。 按HTTP方法名限制用户请求语法： limit_except method...&#123;...&#125;配置块： locationNginx通过limit_except后面指定的方法名来限制用户请求。方法名可取值包括：GET、HEAD、POST、PUT、DELETE、MKCOL、COPY、MOVE、OPTIONS、PROPFIND、PROPPATCH、LOCK、UNLOCK或者PATCH。例如：1234limit_except GET &#123; allow 192.168.1.0/32; deny all;&#125; 注意，允许GET方法就意味着也允许HEAD方法。因此，上面这段代码表示的是禁止GET方法和HEAD方法，但其他HTTP方法是允许的。 HTTP请求包体的最大值语法： client_max_body_size size;默认： client_max_body_size 1m;配置块： http、server、location浏览器在发送含有较大HTTP包体的请求时，其头部会有一个Content-Length字段，client_max_body_size是用来限制Content-Length所示值的大小的。因此，这个限制包体的配置非常有用处，因为不用等Nginx接收完所有的HTTP包体——这有可能消耗很长时间——就可以告诉用户请求过大不被接受。例如，用户试图上传一个10GB的文件，Nginx在收完包头后，发现Content-Length超过client_max_body_size定义的值，就直接发送413(“Request Entity Too Large”)响应给客户端。 对请求的限速语法： limit_rate speed;默认： limit_rate 0;配置块： http、server、location、if此配置是对客户端请求限制每秒传输的字节数。speed可以使用2.2.4节中提到的多种单位，默认参数为0，表示不限速。针对不同的客户端，可以用$limit_rate参数执行不同的限速策略。例如：12345server &#123; if ($slow) &#123; set $limit_rate 4k; &#125;&#125; limit_rate_after语法： limit_rate_after time;默认： limit_rate_after 1m;配置块： http、server、location、if此配置表示Nginx向客户端发送的响应长度超过limit_rate_after后才开始限速。例如：12limit_rate_after 1m;limit_rate 100k; 11.9.2节将从源码上介绍limit_rate_after与limit_rate的区别，以及HTTP框架是如何使用它们来限制发送响应速度的。 2.4.7 文件操作的优化下面介绍文件操作的优化配置项。 sendfile系统调用语法： sendfile on|off;默认： sendfile off;配置块： http、server、location可以启用Linux上的sendfile系统调用来发送文件，它减少了内核态与用户态之间的两次内存复制，这样就会从磁盘中读取文件后直接在内核态发送到网卡设备，提高了发送文件的效率。 AIO系统调用语法： aio on|off;默认： aio off;配置块： http、server、location此配置项表示是否在FreeBSD或Linux系统上启用内核级别的异步文件I&#x2F;O功能。注意，它与sendfile功能是互斥的。 directio语法： directio size|off;默认： directio off;配置块： http、server、location此配置项在FreeBSD和Linux系统上使用O_DIRECT选项去读取文件，缓冲区大小为size，通常对大文件的读取速度有优化作用。注意，它与sendfile功能是互斥的。 directio_alignment语法： directio_alignment size;默认： directio_alignment 512;配置块： http、server、location它与directio配合使用，指定以directio方式读取文件时的对齐方式。一般情况下，512B已经足够了，但针对一些高性能文件系统，如Linux下的XFS文件系统，可能需要设置到4KB作为对齐方式。 打开文件缓存语法： open_file_cache max=N[inactive=time]|off;默认： open_file_cache off;配置块： http、server、location文件缓存会在内存中存储以下3种信息： 文件句柄、文件大小和上次修改时间。 已经打开过的目录结构。 没有找到的或者没有权限操作的文件信息。这样，通过读取缓存就减少了对磁盘的操作。该配置项后面跟3种参数。 max：表示在内存中存储元素的最大个数。当达到最大限制数量后，将采用LRU（Least Recently Used）算法从缓存中淘汰最近最少使用的元素。 inactive：表示在inactive指定的时间段内没有被访问过的元素将会被淘汰。默认时间为60秒。 off：关闭缓存功能。例如：1open_file_cache max=1000 inactive=20s; 是否缓存打开文件错误的信息语法： open_file_cache_errors on|off;默认： open_file_cache_errors off;配置块： http、server、location此配置项表示是否在文件缓存中缓存打开文件时出现的找不到路径、没有权限等错误信息。 不被淘汰的最小访问次数语法： open_file_cache_min_uses number;默认： open_file_cache_min_uses 1;配置块： http、server、location它与open_file_cache中的inactive参数配合使用。如果在inactive指定的时间段内，访问次数超过了open_file_cache_min_uses指定的最小次数，那么将不会被淘汰出缓存。 检验缓存中元素有效性的频率语法： open_file_cache_valid time;默认： open_file_cache_valid 60s;配置块： http、server、location默认为每60秒检查一次缓存中的元素是否仍有效。 2.4.8 对客户端请求的特殊处理下面介绍对客户端请求的特殊处理的配置项。 忽略不合法的HTTP头部语法： ignore_invalid_headers on|off;默认： ignore_invalid_headers on;配置块： http、server如果将其设置为off，那么当出现不合法的HTTP头部时，Nginx会拒绝服务，并直接向用户发送400（Bad Request）错误。如果将其设置为on，则会忽略此HTTP头部。 HTTP头部是否允许下划线语法： underscores_in_headers on|off;默认： underscores_in_headers off;配置块： http、server默认为off，表示HTTP头部的名称中不允许带“_”（下划线）。 对If-Modified-Since头部的处理策略语法： if_modified_since[off|exact|before];默认： if_modified_since exact;配置块： http、server、location出于性能考虑，Web浏览器一般会在客户端本地缓存一些文件，并存储当时获取的时间。这样，下次向Web服务器获取缓存过的资源时，就可以用If-Modified-Since头部把上次获取的时间捎带上，而if_modified_since将根据后面的参数决定如何处理If-Modified-Since头部。相关参数说明如下。 off：表示忽略用户请求中的If-Modified-Since头部。这时，如果获取一个文件，那么会正常地返回文件内容。HTTP响应码通常是200。 exact：将If-Modified-Since头部包含的时间与将要返回的文件上次修改的时间做精确比较，如果没有匹配上，则返回200和文件的实际内容，如果匹配上，则表示浏览器缓存的文件内容已经是最新的了，没有必要再返回文件从而浪费时间与带宽了，这时会返回304 Not Modified，浏览器收到后会直接读取自己的本地缓存。 before：是比exact更宽松的比较。只要文件的上次修改时间等于或者早于用户请求中的If-Modified-Since头部的时间，就会向客户端返回304 Not Modified。 文件未找到时是否记录到error日志语法： log_not_found on|off;默认： log_not_found on;配置块： http、server、location此配置项表示当处理用户请求且需要访问文件时，如果没有找到文件，是否将错误日志记录到error.log文件中。这仅用于定位问题。 merge_slashes语法： merge_slashes on|off;默认： merge_slashes on;配置块： http、server、location此配置项表示是否合并相邻的“&#x2F;”，例如，&#x2F;&#x2F;test&#x2F;&#x2F;&#x2F;a.txt，在配置为on时，会将其匹配为location&#x2F;test&#x2F;a.txt；如果配置为off，则不会匹配，URI将仍然是&#x2F;&#x2F;test&#x2F;&#x2F;&#x2F;a.txt。 DNS解析地址语法： resolver address...;配置块： http、server、location设置DNS名字解析服务器的地址，例如：1resolver 127.0.0.1 192.0.2.1; DNS解析的超时时间语法： resolver_timeout time;默认： resolver_timeout 30s;配置块： http、server、location此配置项表示DNS解析的超时时间。 返回错误页面时是否在Server中注明Nginx版本语法： server_tokens on|off;默认： server_tokens on;配置块： http、server、location表示处理请求出错时是否在响应的Server头部中标明Nginx版本，这是为了方便定位问题。 2.4.9 ngx_http_core_module模块提供的变量在记录access_log访问日志文件时，可以使用ngx_http_core_module模块处理请求时所产生的丰富的变量，当然，这些变量还可以用于其他HTTP模块。例如，当URI中的某个参数满足设定的条件时，有些HTTP模块的配置项可以使用类似$arg_PARAMETER这样的变量。又如，若想把每个请求中的限速信息记录到access日志文件中，则可以使用$limit_rate变量。表2-1列出了ngx_http_core_module模块提供的这些变量。 表2-1 ngx_http_core_module模块提供的变量 2.5 用HTTP proxy module配置一个反向代理服务器反向代理（reverse proxy）方式是指用代理服务器来接受Internet上的连接请求，然后将请求转发给内部网络中的上游服务器，并将从上游服务器上得到的结果返回给Internet上请求连接的客户端，此时代理服务器对外的表现就是一个Web服务器。充当反向代理服务器也是Nginx的一种常见用法（反向代理服务器必须能够处理大量并发请求），本节将介绍Nginx作为HTTP反向代理服务器的基本用法。由于Nginx具有“强悍”的高并发高负载能力，因此一般会作为前端的服务器直接向客户端提供静态文件服务。但也有一些复杂、多变的业务不适合放到Nginx服务器上，这时会用Apache、Tomcat等服务器来处理。于是，Nginx通常会被配置为既是静态Web服务器也是反向代理服务器（如图2-3所示），不适合Nginx处理的请求就会直接转发到上游服务器中处理。 图2-3 作为静态Web服务器与反向代理服务器的Nginx 与Squid等其他反向代理服务器相比，Nginx的反向代理功能有自己的特点，如图2-4所示。当客户端发来HTTP请求时，Nginx并不会立刻转发到上游服务器，而是先把用户的请求（包括HTTP包体）完整地接收到Nginx所在服务器的硬盘或者内存中，然后再向上游服务器发起连接，把缓存的客户端请求转发到上游服务器。而Squid等代理服务器则采用一边接收客户端请求，一边转发到上游服务器的方式。Nginx的这种工作方式有什么优缺点呢？很明显，缺点是延长了一个请求的处理时间，并增加了用于缓存请求内容的内存和磁盘空间。而优点则是降低了上游服务器的负载，尽量把压力放在Nginx服务器上。 图2-4 Nginx作为反向代理服务器时转发请求的流程 Nginx的这种工作方式为什么会降低上游服务器的负载呢？通常，客户端与代理服务器之间的网络环境会比较复杂，多半是“走”公网，网速平均下来可能较慢，因此，一个请求可能要持续很久才能完成。而代理服务器与上游服务器之间一般是“走”内网，或者有专线连接，传输速度较快。Squid等反向代理服务器在与客户端建立连接且还没有开始接收HTTP包体时，就已经向上游服务器建立了连接。例如，某个请求要上传一个1GB的文件，那么每次Squid在收到一个TCP分包（如2KB）时，就会即时地向上游服务器转发。在接收客户端完整HTTP包体的漫长过程中，上游服务器始终要维持这个连接，这直接对上游服务器的并发处理能力提出了挑战。Nginx则不然，它在接收到完整的客户端请求（如1GB的文件）后，才会与上游服务器建立连接转发请求，由于是内网，所以这个转发过程会执行得很快。这样，一个客户端请求占用上游服务器的连接时间就会非常短，也就是说，Nginx的这种反向代理方案主要是为了降低上游服务器的并发压力。Nginx将上游服务器的响应转发到客户端有许多种方法，第12章将介绍其中常见的两种方式。 2.5.1 负载均衡的基本配置作为代理服务器，一般都需要向上游服务器的集群转发请求。这里的负载均衡是指选择一种策略，尽量把请求平均地分布到每一台上游服务器上。下面介绍负载均衡的配置项。 upstream块语法： upstream name&#123;...&#125;配置块： httpupstream块定义了一个上游服务器的集群，便于反向代理中的proxy_pass使用。例如：12345678910upstream backend &#123; server backend1.example.com; server backend2.example.com; server backend3.example.com;&#125;server &#123; location / &#123; proxy_pass http://backend; &#125;&#125; server语法： server name[parameters];配置块： upstreamserver配置项指定了一台上游服务器的名字，这个名字可以是域名、IP地址端口、UNIX句柄等，在其后还可以跟下列参数。 weight&#x3D;number：设置向这台上游服务器转发的权重，默认为1。 max_fails&#x3D;number：该选项与fail_timeout配合使用，指在fail_timeout时间段内，如果向当前的上游服务器转发失败次数超过number，则认为在当前的fail_timeout时间段内这台上游服务器不可用。max_fails默认为1，如果设置为0，则表示不检查失败次数。 fail_timeout&#x3D;time：fail_timeout表示该时间段内转发失败多少次后就认为上游服务器暂时不可用，用于优化反向代理功能。它与向上游服务器建立连接的超时时间、读取上游服务器的响应超时时间等完全无关。fail_timeout默认为10秒。 down：表示所在的上游服务器永久下线，只在使用ip_hash配置项时才有用。 backup：在使用ip_hash配置项时它是无效的。它表示所在的上游服务器只是备份服务器，只有在所有的非备份上游服务器都失效后，才会向所在的上游服务器转发请求。例如：12345upstream backend &#123; server backend1.example.com weight=5; server 127.0.0.1:8080 max_fails=3 fail_timeout=30s; server unix:/tmp/backend3;&#125; ip_hash语法： ip_hash; 配置块： upstream在有些场景下，我们可能会希望来自某一个用户的请求始终落到固定的一台上游服务器中。例如，假设上游服务器会缓存一些信息，如果同一个用户的请求任意地转发到集群中的任一台上游服务器中，那么每一台上游服务器都有可能会缓存同一份信息，这既会造成资源的浪费，也会难以有效地管理缓存信息。ip_hash就是用以解决上述问题的，它首先根据客户端的IP地址计算出一个key，将key按照upstream集群里的上游服务器数量进行取模，然后以取模后的结果把请求转发到相应的上游服务器中。这样就确保了同一个客户端的请求只会转发到指定的上游服务器中。ip_hash与weight（权重）配置不可同时使用。如果upstream集群中有一台上游服务器暂时不可用，不能直接删除该配置，而是要down参数标识，确保转发策略的一贯性。例如：1234567upstream backend &#123; ip_hash; server backend1.example.com; server backend2.example.com; server backend3.example.com down; server backend4.example.com;&#125; 记录日志时支持的变量如果需要将负载均衡时的一些信息记录到access_log日志中，那么在定义日志格式时可以使用负载均衡功能提供的变量，见表2-2。表2-2 访问上游服务器时可以使用的变量 例如，可以在定义access_log访问日志格式时使用表2-2中的变量。12345log_format timing &#x27;$remote_addr - $remote_user [$time_local] $request &#x27; &#x27;upstream_response_time $upstream_response_time &#x27; &#x27;msec $msec request_time $request_time&#x27;;log_format up_head &#x27;$remote_addr - $remote_user [$time_local] $request &#x27; &#x27;upstream_http_content_type $upstream_http_content_type&#x27;; 2.5.2 反向代理的基本配置下面介绍反向代理的基本配置项。 proxy_pass语法： proxy_pass URL;配置块： location、if此配置项将当前请求反向代理到URL参数指定的服务器上，URL可以是主机名或IP地址加端口的形式，例如：1proxy_pass http://localhost:8000/uri/; 也可以是UNIX句柄：1proxy_pass http://unix:/path/to/backend.socket:/uri/ ; 还可以如上节负载均衡中所示，直接使用upstream块，例如：12345678upstream backend &#123; ... &#125;server &#123; location / &#123; proxy_pass http://backend; &#125;&#125; 用户可以把HTTP转换成更安全的HTTPS，例如：1proxy_pass https://192.168.0.1; 默认情况下反向代理是不会转发请求中的Host头部的。如果需要转发，那么必须加上配置：1proxy_set_header Host $host; proxy_method语法： proxy_method method;配置块： http、server、location此配置项表示转发时的协议方法名。例如设置为：1proxy_method POST; 那么客户端发来的GET请求在转发时方法名也会改为POST。 proxy_hide_header语法： proxy_hide_header the_header;配置块： http、server、locationNginx会将上游服务器的响应转发给客户端，但默认不会转发以下HTTP头部字段：Date、Server、X-Pad和X-Accel-*。使用proxy_hide_header后可以任意地指定哪些HTTP头部字段不能被转发。例如：12proxy_hide_header Cache-Control;proxy_hide_header MicrosoftOfficeWebServer; proxy_pass_header语法： proxy_pass_header the_header;配置块： http、server、location与proxy_hide_header功能相反，proxy_pass_header会将原来禁止转发的header设置为允许转发。例如：1proxy_pass_header X-Accel-Redirect; proxy_pass_request_body语法： proxy_pass_request_body on|off;默认： proxy_pass_request_body on;配置块： http、server、location作用为确定是否向上游服务器发送HTTP包体部分。 proxy_pass_request_headers语法： proxy_pass_request_headers on|off;默认： proxy_pass_request_headers on;配置块： http、server、location作用为确定是否转发HTTP头部。 proxy_redirect语法： proxy_redirect[default|off|redirect replacement];默认： proxy_redirect default;配置块： http、server、location当上游服务器返回的响应是重定向或刷新请求（如HTTP响应码是301或者302）时，proxy_redirect可以重设HTTP头部的location或refresh字段。例如，如果上游服务器发出的响应是302重定向请求，location字段的URI是http://localhost:8000/two/some/uri/ ，那么在下面的配置情况下，实际转发给客户端的location是http://frontend/one/some/uri/ 。12proxy_redirect http://localhost:8000/two/http://frontend/one/; 这里还可以使用ngx-http-core-module提供的变量来设置新的location字段。例如：12proxy_redirect http://localhost:8000/http://$host:$server_port/; 也可以省略replacement参数中的主机名部分，这时会用虚拟主机名称来填充。例如：1proxy_redirect http://localhost:8000/two//one/; 使用off参数时，将使location或者refresh字段维持不变。例如：1proxy_redirect off; 使用默认的default参数时，会按照proxy_pass配置项和所属的location配置项重组发往客户端的location头部。例如，下面两种配置效果是一样的：12345678location /one/ &#123; proxy_pass http://upstream:port/two/; proxy_redirect default;&#125;location /one/ &#123; proxy_pass http://upstream:port/two/; proxy_redirect http://upstream:port/two//one/;&#125; proxy_next_upstream语法： proxy_next_upstream[error|timeout|invalid_header|http_500|http_502|http_503|http_504|http_404|off];默认： proxy_next_upstream error timeout;配置块： http、server、location此配置项表示当向一台上游服务器转发请求出现错误时，继续换一台上游服务器处理这个请求。前面已经说过，上游服务器一旦开始发送应答，Nginx反向代理服务器会立刻把应答包转发给客户端。因此，一旦Nginx开始向客户端发送响应包，之后的过程中若出现错误也是不允许换下一台上游服务器继续处理的。这很好理解，这样才可以更好地保证客户端只收到来自一个上游服务器的应答。 proxy_next_upstream的参数用来说明在哪些情况下会继续选择下一台上游服务器转发请求。 error：当向上游服务器发起连接、发送请求、读取响应时出错。 timeout：发送请求或读取响应时发生超时。 invalid_header：上游服务器发送的响应是不合法的。 http_500：上游服务器返回的HTTP响应码是500。 http_502：上游服务器返回的HTTP响应码是502。 http_503：上游服务器返回的HTTP响应码是503。 http_504：上游服务器返回的HTTP响应码是504。 http_404：上游服务器返回的HTTP响应码是404。 off：关闭proxy_next_upstream功能—出错就选择另一台上游服务器再次转发。 Nginx的反向代理模块还提供了很多种配置，如设置连接的超时时间、临时文件如何存储，以及最重要的如何缓存上游服务器响应等功能。这些配置可以通过阅读ngx_http_proxy_module模块的说明了解，只有深入地理解，才能实现一个高性能的反向代理服务器。本节只是介绍反向代理服务器的基本功能，在第12章中我们将会深入地探索upstream机制，到那时，读者也许会发现ngx_http_proxy_module模块只是使用upstream机制实现了反向代理功能而已。 2.6 小结Nginx由少量的核心框架代码和许多模块组成，每个模块都有它独特的功能。因此，读者可以通过查看每个模块实现了什么功能，来了解Nginx可以帮我们做些什么。Nginx的Wiki网站（http://wiki.nginx.org/Modules ）上列出了官方提供的所有模块及配置项，仔细观察就会发现，这些配置项的语法与本章的内容都是很相近的，读者只需要弄清楚模块说明中每个配置项的意义即可。另外，网页http://wiki.nginx.org/3rdPartyModules 中列出了Wiki上已知的几十个第三方模块，同时读者还可以从搜索引擎上搜索到更多的第三方模块。了解每个模块的配置项用法，并在Nginx中使用这些模块，可以让Nginx做到更多。随着对本书的学习，读者会对Nginx模块的设计思路有深入的了解，也会渐渐熟悉如何编写一个模块。如果某个模块的实现与你的想法有出入，可以更改这个模块的源码，实现你期望的业务功能。如果所有的模块都没有你想要的功能，不妨自己重写一个定制的模块，也可以申请发布到Nginx网站上供大家分享。"},{"title":"写在前面","path":"/wiki/understanding-nginx:modules-development-and-architecture-resolving(second-edition)/section_01/section_01.html","content":"第一章 研究Nginx前的准备工作 第二章 Nginx的配置"},{"title":"第一章 研究Nginx前的准备工作","path":"/wiki/understanding-nginx:modules-development-and-architecture-resolving(second-edition)/section_01/chapter_01.html","content":"2012年，Nginx荣获年度云计算开发奖（2012 Cloud Award for Developer of the Year），并成长为世界第二大Web服务器。全世界流量最高的前1000名网站中，超过25%都使用Nginx来处理海量的互联网请求。Nginx已经成为业界高性能Web服务器的代名词。那么，什么是Nginx？它有哪些特点？我们选择Nginx的理由是什么？如何编译安装Nginx？这种安装方式背后隐藏的又是什么样的思想呢？本章将会回答上述问题。 1.1 Nginx是什么人们在了解新事物时，往往习惯通过类比来帮助自己理解事物的概貌。那么，我们在学习Nginx时也采用同样的方式，先来看看Nginx的竞争对手——Apache、Lighttpd、Tomcat、Jetty、IIS，它们都是Web服务器，或者叫做WWW（World Wide Web）服务器，相应地也都具备Web服务器的基本功能：基于REST架构风格[1] ，以统一资源描述符（Uniform Resource Identifier，URI）或者统一资源定位符（Uniform Resource Locator，URL）作为沟通依据，通过HTTP为浏览器等客户端程序提供各种网络服务。然而，由于这些Web服务器在设计阶段就受到许多局限，例如当时的互联网用户规模、网络带宽、产品特点等局限，并且各自的定位与发展方向都不尽相同，使得每一款Web服务器的特点与应用场合都很鲜明。Tomcat和Jetty面向Java语言，先天就是重量级的Web服务器，它的性能与Nginx没有可比性，这里略过。IIS只能在Windows操作系统上运行。Windows作为服务器在稳定性与其他一些性能上都不如类UNIX操作系统，因此，在需要高性能Web服务器的场合下，IIS可能会被“冷落”。Apache的发展时期很长，而且是目前毫无争议的世界第一大Web服务器，图1-1中是12年来（2010~2012年）世界Web服务器的使用排名情况。 图1-1 Netcraft对于644275754个站点31.4M个域名Web服务器使用情况的调查结果（2012年3月） 从图1-1中可以看出，Apache目前处于领先地位。Apache有许多优点，如稳定、开源、跨平台等，但它出现的时间太长了，在它兴起的年代，互联网的产业规模远远比不上今天，所以它被设计成了一个重量级的、不支持高并发的Web服务器。在Apache服务器上，如果有数以万计的并发HTTP请求同时访问，就会导致服务器上消耗大量内存，操作系统内核对成百上千的Apache进程做进程间切换也会消耗大量CPU资源，并导致HTTP请求的平均响应速度降低，这些都决定了Apache不可能成为高性能Web服务器，这也促使了Lighttpd和Nginx的出现。观察图1-1中Nginx成长的曲线，体会一下Nginx抢占市场时的“咄咄逼人”吧。Lighttpd和Nginx一样，都是轻量级、高性能的Web服务器，欧美的业界开发者比较钟爱Lighttpd，而国内的公司更青睐Nginx，Lighttpd使用得比较少。在了解了Nginx的竞争对手之后，相信大家对Nginx也有了直观感受，下面让我们来正式地认识一下Nginx吧。 提示Nginx发音：engineX 来自俄罗斯的Igor Sysoev在为Rambler Media（http://www.rambler.ru/ ）工作期间，使用C语言开发了Nginx。Nginx作为Web服务器，一直为俄罗斯著名的门户网站Rambler Media提供着出色、稳定的服务。Igor Sysoev将Nginx的代码开源，并且赋予其最自由的2-clause BSD-like license[2] 许可证。由于Nginx使用基于事件驱动的架构能够并发处理百万级别的TCP连接，高度模块化的设计和自由的许可证使得扩展Nginx功能的第三方模块层出不穷，而且优秀的设计带来了极佳的稳定性，因此其作为Web服务器被广泛应用到大流量的网站上，包括腾讯、新浪、网易、淘宝等访问量巨大的网站。2012年2月和3月Netcraft对Web服务器的调查如表1-1所示，可以看出，Nginx的市场份额越来越大。 表1-1 Netcraft对于Web服务器市场占有率前4位软件的调查（2012年2月和3月） Nginx是一个跨平台的Web服务器，可运行在Linux、FreeBSD、Solaris、AIX、Mac OS、Windows等操作系统上，并且它还可以使用当前操作系统特有的一些高效API来提高自己的性能。例如，对于高效处理大规模并发连接，它支持Linux上的epoll（epoll是Linux上处理大并发网络连接的利器，9.6.1节中将会详细说明epoll的工作原理）、Solaris上的event ports和FreeBSD上的kqueue等。又如，对于Linux，Nginx支持其独有的sendfile系统调用，这个系统调用可以高效地把硬盘中的数据发送到网络上（不需要先把硬盘数据复制到用户态内存上再发送），这极大地减少了内核态与用户态数据间的复制动作。种种迹象都表明，Nginx以性能为王。2011年7月，Nginx正式成立公司，由Igor Sysoev担任CTO，立足于提供商业级的Web服务器。[1] 参见Roy Fielding博士的论文《Architectural Styles and the Design of Network-based Software Architectures》，可在http://www.ics.uci.edu/~fielding/pubs/dissertation/top.htm查看原文。[2] BSD（Berkeley Software Distribution）许可协议是自由软件（开源 软件的一个子集）中使用最广泛的许可协议之一。与其他许可协议相 比，BSD许可协议从GNU通用公共许可协议（GPL）到限制重重的著 作权（copyright）都要宽松一些，事实上，它跟公有领域更为接近。BSD许可协议被认为是copycenter（中间版权），界于标准的copyright与GPL的copyleft之间。2-clause BSD-like license是BSD许可协议中最宽 松的一种，它对开发者再次使用BSD软件只有两个基本的要求：一是 如果再发布的产品中包含源代码，则在源代码中必须带有原来代码中 的BSD协议；二是如果再发布的只是二进制类库&#x2F;软件，则需要在类库&#x2F;软件的文档和版权声明中包含原来代码中的BSD协议。 1.2 为什么选择Nginx为什么选择Nginx？因为它具有以下特点： 更快这表现在两个方面：一方面，在正常情况下，单次请求会得到更快的响应；另一方面，在高峰期（如有数以万计的并发请求），Nginx可以比其他Web服务器更快地响应请求。实际上，本书第三部分中大量的篇幅都是在说明Nginx是如何做到这两点的。 高扩展性Nginx的设计极具扩展性，它完全是由多个不同功能、不同层次、不同类型且耦合度极低的模块组成。因此，当对某一个模块修复Bug或进行升级时，可以专注于模块自身，无须在意其他。而且在HTTP模块中，还设计了HTTP过滤器模块：一个正常的HTTP模块在处理完请求后，会有一串HTTP过滤器模块对请求的结果进行再处理。这样，当我们开发一个新的HTTP模块时，不但可以使用诸如HTTP核心模块、events模块、log模块等不同层次或者不同类型的模块，还可以原封不动地复用大量已有的HTTP过滤器模块。这种低耦合度的优秀设计，造就了Nginx庞大的第三方模块，当然，公开的第三方模块也如官方发布的模块一样容易使用。Nginx的模块都是嵌入到二进制文件中执行的，无论官方发布的模块还是第三方模块都是如此。这使得第三方模块一样具备极其优秀的性能，充分利用Nginx的高并发特性，因此，许多高流量的网站都倾向于开发符合自己业务特性的定制模块。 高可靠性高可靠性是我们选择Nginx的最基本条件，因为Nginx的可靠性是大家有目共睹的，很多家高流量网站都在核心服务器上大规模使用Nginx。Nginx的高可靠性来自于其核心框架代码的优秀设计、模块设计的简单性；另外，官方提供的常用模块都非常稳定，每个worker进程相对独立，master进程在1个worker进程出错时可以快速“拉起”新的worker子进程提供服务。 低内存消耗一般情况下，10000个非活跃的HTTP Keep-Alive连接在Nginx中仅消耗2.5MB的内存，这是Nginx支持高并发连接的基础。从第3章开始，我们会接触到Nginx在内存中为了维护一个HTTP连接所分配的对象，届时将会看到，实际上Nginx一直在为用户考虑（尤其是在高并发时）如何使得内存的消耗更少。 单机支持10万以上的并发连接这是一个非常重要的特性！随着互联网的迅猛发展和互联网用户数量的成倍增长，各大公司、网站都需要应付海量并发请求，一个能够在峰值期顶住10万以上并发请求的Server，无疑会得到大家的青睐。理论上，Nginx支持的并发连接上限取决于内存，10万远未封顶。当然，能够及时地处理更多的并发请求，是与业务特点紧密相关的，本书第8~11章将会详细说明如何实现这个特点。 热部署master管理进程与worker工作进程的分离设计，使得Nginx能够提供热部署功能，即可以在7×24小时不间断服务的前提下，升级Nginx的可执行文件。当然，它也支持不停止服务就更新配置项、更换日志文件等功能。 最自由的BSD许可协议这是Nginx可以快速发展的强大动力。BSD许可协议不只是允许用户免费使用Nginx，它还允许用户在自己的项目中直接使用或修改Nginx源码，然后发布。这吸引了无数开发者继续为Nginx贡献自己的智慧。 以上7个特点当然不是Nginx的全部，拥有无数个官方功能模块、第三方功能模块使得Nginx能够满足绝大部分应用场景，这些功能模块间可以叠加以实现更加强大、复杂的功能，有些模块还支持Nginx与Perl、Lua等脚本语言集成工作，大大提高了开发效率。这些特点促使用户在寻找一个Web服务器时更多考虑Nginx。当然，选择Nginx的核心理由还是它能在支持高并发请求的同时保持高效的服务。如果Web服务器的业务访问量巨大，就需要保证在数以百万计的请求同时访问服务时，用户可以获得良好的体验，不会出现并发访问量达到一个数字后，新的用户无法获取服务，或者虽然成功地建立起了TCP连接，但大部分请求却得不到响应的情况。通常，高峰期服务器的访问量可能是正常情况下的许多倍，若有热点事件的发生，可能会导致正常情况下非常顺畅的服务器直接“挂 死”。然而，如果在部署服务器时，就预先针对这种情况进行扩容，又会使得正常情况下所有服务器的负载过低，这会造成大量的资源浪费。因此，我们会希望在这之间取得平衡，也就是说，在低并发压力下，用户可以获得高速体验，而在高并发压力下，更多的用户都能接入，可能访问速度会下降，但这只应受制于带宽和处理器的速度，而不应该是服务器设计导致的软件瓶颈。事实上，由于中国互联网用户群体的数量巨大，致使对Web服务器的设计往往要比欧美公司更加困难。例如，对于全球性的一些网站而言，欧美用户分布在两个半球，欧洲用户活跃时，美洲用户通常在休息，反之亦然。而国内巨大的用户群体则对业界的程序员提出更高的挑战，早上9点和晚上20点到24点这些时间段的并发请求压力是非常巨大的。尤其节假日、寒暑假到来之时，更会对服务器提出极高的要求。另外，国内业务上的特性，也会引导用户在同一时间大并发地访问服务器。例如，许多SNS网页游戏会在固定的时间点刷新游戏资源或者允许“偷菜”等好友互动操作。这些会导致服务器处理高并发请求的压力增大。上述情形都对我们的互联网服务在大并发压力下是否还能够给予用户良好的体验提出了更高的要求。若要提供更好的服务，那么可以从多方面入手，例如，修改业务特性、引导用户从高峰期分流或者把服务分层分级、对于不同并发压力给用户提供不同级别的服务等。但最根本的是，Web服务器要能支持大并发压力下的正常服务，这才是关键。快速增长的互联网用户群以及业内所有互联网服务提供商越来越好的用户体验，都促使我们在大流量服务中用Nginx取代其他Web服务器。Nginx先天的事件驱动型设计、全异步的网络I&#x2F;O处理机制、极少的进程间切换以及许多优化设计，都使得Nginx天生善于处理高并发压力下的互联网请求，同时Nginx降低了资源消耗，可以把服务器硬件资源“压榨”到极致。 1.3 准备工作由于Linux具有免费、使用广泛、商业支持越来越完善等特点，本书将主要针对Linux上运行的Nginx来进行介绍。需要说明的是，本书不是使用手册，而是介绍Nginx作为Web服务器的设计思想，以及如何更有效地使用Nginx达成目的，而这些内容在各操作系统上基本是相通的（除了第9章关于事件驱动方式以及第14章的进程间同步方式在类UNIX操作系统上略有不同以外）。 1.3.1 Linux操作系统首先我们需要一个内核为Linux 2.6及以上版本的操作系统，因为Linux 2.6及以上内核才支持epoll，而在Linux上使用select或poll来解决事件的多路复用，是无法解决高并发压力问题的。我们可以使用uname-a命令来查询Linux内核版本，例如： 12:wehf2wng001:root &gt; uname -aLinux wehf2wng001 2.6.18-128.el5 #1 SMP Wed Jan 21 10:41:14 EST 2009 x86_64 x86_64 x86_64 GNU/Linux 执行结果表明内核版本是2.6.18，符合我们的要求。 1.3.2 使用Nginx的必备软件如果要使用Nginx的常用功能，那么首先需要确保该操作系统上至少安装了如下软件。 GCC编译器GCC（GNU Compiler Collection）可用来编译C语言程序。Nginx不会直接提供二进制可执行程序（1.2.x版本中已经开始提供某些操作系统上的二进制安装包了，不过，本书探讨如何开发Nginx模块是必须通过直接编译源代码进行的），这有许多原因，本章后面会详述。我们可以使用最简单的yum方式安装GCC，例如：1yum install -y gcc GCC是必需的编译工具。在第3章会提到如何使用C++来编写Nginx HTTP模块，这时就需要用到G++编译器了。G++编译器也可以用yum安装，例如：1yum install -y gcc-c++ Linux上有许多软件安装方式，yum只是其中比较方便的一种，其他方式这里不再赘述。 PCRE库PCRE（Perl Compatible Regular Expressions，Perl兼容正则表达式）是由Philip Hazel开发的函数库，目前为很多软件所使用，该库支持正则表达式。它由RegEx演化而来，实际上，Perl正则表达式也是源自于Henry Spencer写的RegEx。如果我们在配置文件nginx.conf里使用了正则表达式，那么在编译Nginx时就必须把PCRE库编译进Nginx，因为Nginx的HTTP模块要靠它来解析正则表达式。当然，如果你确认不会使用正则表达式，就不必安装它。其yum安装方式如下：1yum install -y pcre pcre-devel pcre-devel是使用PCRE做二次开发时所需要的开发库，包括头文件等，这也是编译Nginx所必须使用的。 zlib库zlib库用于对HTTP包的内容做gzip格式的压缩，如果我们在nginx.conf里配置了gzip on，并指定对于某些类型（content-type）的HTTP响应使用gzip来进行压缩以减少网络传输量，那么，在编译时就必须把zlib编译进Nginx。其yum安装方式如下：1yum install -y zlib zlib-devel 同理，zlib是直接使用的库，zlib-devel是二次开发所需要的库。 OpenSSL开发库如果我们的服务器不只是要支持HTTP，还需要在更安全的SSL协议上传输HTTP，那么就需要拥有OpenSSL了。另外，如果我们想使用MD5、SHA1等散列函数，那么也需要安装它。其yum安装方式如下：1yum install -y openssl openssl-devel 上面所列的4个库只是完成Web服务器最基本功能所必需的。Nginx是高度自由化的Web服务器，它的功能是由许多模块来支持的。而这些模块可根据我们的使用需求来定制，如果某些模块不需要使用则完全不必理会它。同样，如果使用了某个模块，而这个模块使用了一些类似zlib或OpenSSL等的第三方库，那么就必须先安装这些软件。 1.3.3 磁盘目录要使用Nginx，还需要在Linux文件系统上准备以下目录。 Nginx源代码存放目录该目录用于放置从官网上下载的Nginx源码文件，以及第三方或我们自己所写的模块源代码文件。 Nginx编译阶段产生的中间文件存放目录该目录用于放置在configure命令执行后所生成的源文件及目录，以及make命令执行后生成的目标文件和最终连接成功的二进制文件。默认情况下，configure命令会将该目录命名为objs，并放在Nginx源代码目录下。 部署目录该目录存放实际Nginx服务运行期间所需要的二进制文件、配置文件等。默认情况下，该目录为&#x2F;usr&#x2F;local&#x2F;nginx。 日志文件存放目录日志文件通常会比较大，当研究Nginx的底层架构时，需要打开debug级别的日志，这个级别的日志非常详细，会导致日志文件的大小增长得极快，需要预先分配一个拥有更大磁盘空间的目录。 1.3.4 Linux内核参数的优化由于默认的Linux内核参数考虑的是最通用的场景，这明显不符合用于支持高并发访问的Web服务器的定义，所以需要修改Linux内核参数，使得Nginx可以拥有更高的性能。在优化内核时，可以做的事情很多，不过，我们通常会根据业务特点来进行调整，当Nginx作为静态Web内容服务器、反向代理服务器或是提供图片缩略图功能（实时压缩图片）的服务器时，其内核参数的调整都是不同的。这里只针对最通用的、使Nginx支持更多并发请求的TCP网络参数做简单说明。首先，需要修改&#x2F;etc&#x2F;sysctl.conf来更改内核参数。例如，最常用的配置： 123456789101112131415fs.file-max = 999999 net.ipv4.tcp_tw_reuse = 1net.ipv4.tcp_keepalive_time = 600net.ipv4.tcp_fin_timeout = 30net.ipv4.tcp_max_tw_buckets = 5000net.ipv4.ip_local_port_range = 1024 61000net.ipv4.tcp_rmem = 4096 32768 262142net.ipv4.tcp_wmem = 4096 32768 262142net.core.netdev_max_backlog = 8096net.core.rmem_default = 262144net.core.wmem_default = 262144net.core.rmem_max = 2097152net.core.wmem_max = 2097152net.ipv4.tcp_syncookies = 1net.ipv4.tcp_max_syn.backlog=1024 然后执行sysctl-p命令，使上述修改生效。上面的参数意义解释如下： file-max：这个参数表示进程（比如一个worker进程）可以同时打开的最大句柄数，这个参数直接限制最大并发连接数，需根据实际情况配置。 tcp_tw_reuse：这个参数设置为1，表示允许将TIME-WAIT状态的socket重新用于新的TCP连接，这对于服务器来说很有意义，因为服务器上总会有大量TIME-WAIT状态的连接。 tcp_keepalive_time：这个参数表示当keepalive启用时，TCP发送keepalive消息的频度。默认是2小时，若将其设置得小一些，可以更快地清理无效的连接。 tcp_fin_timeout：这个参数表示当服务器主动关闭连接时，socket保持在FIN-WAIT-2状态的最大时间。 tcp_max_tw_buckets：这个参数表示操作系统允许TIME_WAIT套接字数量的最大值，如果超过这个数字，TIME_WAIT套接字将立刻被清除并打印警告信息。该参数默认为180000，过多的TIME_WAIT套接字会使Web服务器变慢。 tcp_max_syn_backlog：这个参数表示TCP三次握手建立阶段接收SYN请求队列的最大长度，默认为1024，将其设置得大一些可以使出现Nginx繁忙来不及accept新连接的情况时，Linux不至于丢失客户端发起的连接请求。 ip_local_port_range：这个参数定义了在UDP和TCP连接中本地（不包括连接的远端）端口的取值范围。 net.ipv4.tcp_rmem：这个参数定义了TCP接收缓存（用于TCP接收滑动窗口）的最小值、默认值、最大值。 net.ipv4.tcp_wmem：这个参数定义了TCP发送缓存（用于TCP发送滑动窗口）的最小值、默认值、最大值。 netdev_max_backlog：当网卡接收数据包的速度大于内核处理的速度时，会有一个队列保存这些数据包。这个参数表示该队列的最大值。 rmem_default：这个参数表示内核套接字接收缓存区默认的大小。 wmem_default：这个参数表示内核套接字发送缓存区默认的大小。 rmem_max：这个参数表示内核套接字接收缓存区的最大大小。 wmem_max：这个参数表示内核套接字发送缓存区的最大大小。 注意滑动窗口的大小与套接字缓存区会在一定程度上影响并发连接的数目。每个TCP连接都会为维护TCP滑动窗口而消耗内存，这个窗口会根据服务器的处理速度收缩或扩张。参数wmem_max的设置，需要平衡物理内存的总大小、Nginx并发处理的最大连接数量（由nginx.conf中的worker_processes和worker_connections参数决定）而确定。当然，如果仅仅为了提高并发量使服务器不出现Out Of Memory问题而去降低滑动窗口大小，那么并不合适，因为滑动窗口过小会影响大数据量的传输速度。rmem_default、wmem_default、rmem_max、wmem_max这4个参数的设置需要根据我们的业务特性以及实际的硬件成本来综合考虑。 tcp_syncookies：该参数与性能无关，用于解决TCP的SYN攻击。 1.3.5 获取Nginx源码可以在Nginx官方网站（http://nginx.org/en/download.html ）获取Nginx源码包。将下载的nginx-1.0.14.tar.gz源码压缩包放置到准备好的Nginx源代码目录中，然后解压。例如： 1tar -zxvf nginx-1.0.14.tar.gz 本书编写时的Nginx最新稳定版本为1.0.14（如图1-2所示），本书后续部分都将以此版本作为基准。当然，本书将要说明的Nginx核心代码一般不会有改动（否则大量第三方模块的功能就无法保证了），即使下载其他版本的Nginx源码包也不会影响阅读本书。 图1-2 Nginx的不同版本 1.4 编译安装Nginx安装Nginx最简单的方式是，进入nginx-1.0.14目录后执行以下3行命令： 123./configuremakemake install configure命令做了大量的“幕后”工作，包括检测操作系统内核和已经安装的软件，参数的解析，中间目录的生成以及根据各种参数生成一些C源码文件、Makefile文件等。make命令根据configure命令生成的Makefile文件编译Nginx工程，并生成目标文件、最终的二进制文件。make install命令根据configure执行时的参数将Nginx部署到指定的安装目录，包括相关目录的建立和二进制文件、配置文件的复制。 1.5 configure详解可以看出，configure命令至关重要，下文将详细介绍如何使用configure命令，并分析configure到底是如何工作的，从中我们也可以看出Nginx的一些设计思想。 1.5.1 configure的命令参数1.路径相关的参数表1-2列出了Nginx在编译期、运行期中与路径相关的各种参数。 表1-2 configure支持的路径相关参数 2.编译相关的参数表1-3列出了编译Nginx时与编译器相关的参数。 表1-3 configure支持的编译相关参数 3.依赖软件的相关参数表1-4~表1-8列出了Nginx依赖的常用软件支持的参数。 表1-4 PCRE的设置参数 表1-5 OpenSSL的设置参数 表1-6 原子库的设置参数 表1-7 散列函数库的设置参数 表1-8 zlib库的设置参数 4.模块相关的参数除了少量核心代码外，Nginx完全是由各种功能模块组成的。这些模块会根据配置参数决定自己的行为，因此，正确地使用各个模块非常关键。在configure的参数中，我们把它们分为五大类。 事件模块。 默认即编译进入Nginx的HTTP模块。 默认不会编译进入Nginx的HTTP模块。 邮件代理服务器相关的mail模块。 其他模块。 事件模块表1-9中列出了Nginx可以选择哪些事件模块编译到产品中。表1-9 configure支持的事件模块参数 默认即编译进入Nginx的HTTP模块表1-10列出了默认就会编译进Nginx的核心HTTP模块，以及如何把这些HTTP模块从产品中去除。表1-10 configure中默认编译到Nginx中的HTTP模块参数 默认不会编译进入Nginx的HTTP模块表1-11列出了默认不会编译至Nginx中的HTTP模块以及把它们加入产品中的方法。表1-11 configure中默认不会编译到Nginx中的HTTP模块参数 邮件代理服务器相关的mail模块表1-12列出了把邮件模块编译到产品中的参数。表1-12 configure提供的邮件模块参数 其他参数configure还接收一些其他参数，表1-13中列出了相关参数的说明。表1-13 configure提供的其他参数 1.5.2 configure执行流程我们看到configure命令支持非常多的参数，读者可能会好奇它在执行时到底做了哪些事情，本节将通过解析configure源码来对它有一个感性的认识。configure由Shell脚本编写，中间会调用&#x2F;auto&#x2F;目录下的脚本。这里将只对configure脚本本身做分析，对于它所调用的auto目录下的其他工具脚本则只做功能性的说明。configure脚本的内容如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119#!/bin/sh# Copyright (C) Igor Sysoev# Copyright (C) Nginx, Inc.#auto/options脚本处理configure命令的参数。例如，如果参数是--help，那么显示支持的所有参数格式。options脚本会定义后续工作将要用到的变量，然后根据本次参数以及默认值设置这些变量. auto/options#auto/init脚本初始化后续将产生的文件路径。例如，Makefile、ngx_modules.c等文件默认情况下将会在&lt;nginx-source&gt;/objs/. auto/init#auto/sources脚本将分析Nginx的源码结构，这样才能构造后续的Makefile文件. auto/sources#编译过程中所有目标文件生成的路径由—builddir=DIR参数指定，默认情况下为&lt;nginx-source&gt;/objs，此时这个目录将会被创建test -d $NGX_OBJS || mkdir $NGX_OBJS#开始准备建立ngx_auto_headers.h、autoconf.err等必要的编译文件echo &gt; $NGX_AUTO_HEADERS_Hecho &gt; $NGX_AUTOCONF_ERR#向objs/ngx_auto_config.h写入命令行带的参数echo &quot;#define NGX_CONFIGURE \\&quot;$NGX_CONFIGURE\\&quot;&quot; &gt; $NGX_AUTO_CONFIG_H#判断DEBUG标志，如果有，那么在objs/ngx_auto_config.h文件中写入DEBUG宏if [ $NGX_DEBUG = YES ]; then have=NGX_DEBUG . auto/havefi#现在开始检查操作系统参数是否支持后续编译if test -z &quot;$NGX_PLATFORM&quot;; then echo &quot;checking for OS&quot; NGX_SYSTEM=`uname -s 2&gt;/dev/null` NGX_RELEASE=`uname -r 2&gt;/dev/null` NGX_MACHINE=`uname -m 2&gt;/dev/null`#屏幕上输出OS名称、内核版本、32位/64位内核 echo &quot; + $NGX_SYSTEM $NGX_RELEASE $NGX_MACHINE&quot; NGX_PLATFORM=&quot;$NGX_SYSTEM:$NGX_RELEASE:$NGX_MACHINE&quot;; case &quot;$NGX_SYSTEM&quot; in MINGW32_*) NGX_PLATFORM=win32 ;; esacelse echo &quot;building for $NGX_PLATFORM&quot; NGX_SYSTEM=$NGX_PLATFORMfi#检查并设置编译器，如GCC是否安装、GCC版本是否支持后续编译nginx. auto/cc/conf#对非Windows操作系统定义一些必要的头文件，并检查其是否存在，以此决定configure后续步骤是否可以成功[1]if [ &quot;$NGX_PLATFORM&quot; != win32 ]; then . auto/headersfi#对于当前操作系统，定义一些特定的操作系统相关的方法并检查当前环境是否支持。例如，对于Linux，在这里使用sched_setaffinity设置进程优先级，使用Linux特有的sendfile系统调用来加速向网络中发送文件块. auto/os/conf#定义类UNIX 操作系统中通用的头文件和系统调用等，并检查当前环境是否支持if [ &quot;$NGX_PLATFORM&quot; != win32 ]; then . auto/unixfi#最核心的构造运行期modules的脚本。它将会生成ngx_modules.c文件，这个文件会被编译进Nginx中，其中它所做的唯一的事情就是定义了ngx_modules数组。ngx_modules指明Nginx运行期间有哪些模块会参与到请求的处理中，包括HTTP请求可能会使用哪些模块处理，因此，它对数组元素的顺序非常敏感，也就是说，绝大部分模块在ngx_modules数组中的顺序其实是固定的。例如，一个请求必须先执行ngx_http_gzip_filter_module模块重新修改HTTP响应中的头部后，才能使用ngx_http_header_filter模块按照headers_in结构体里的成员构造出以TCP流形式发送给客户端的HTTP响应头部。注意，我们在--add-module=参数里加入的第三方模块也在此步骤写入到ngx_modules.c文件中了. auto/modules#conf脚本用来检查Nginx在链接期间需要链接的第三方静态库、动态库或者目标文件是否存在. auto/lib/conf#处理Nginx安装后的路径case &quot;.$NGX_PREFIX&quot; in .) NGX_PREFIX=$&#123;NGX_PREFIX:-/usr/local/nginx&#125; have=NGX_PREFIX value=&quot;\\&quot;$NGX_PREFIX/\\&quot;&quot; . auto/define ;; .!) NGX_PREFIX= ;; *) have=NGX_PREFIX value=&quot;\\&quot;$NGX_PREFIX/\\&quot;&quot; . auto/define ;;esac#处理Nginx安装后conf文件的路径if [ &quot;.$NGX_CONF_PREFIX&quot; != &quot;.&quot; ]; then have=NGX_CONF_PREFIX value=&quot;\\&quot;$NGX_CONF_PREFIX/\\&quot;&quot; . auto/definefi#处理Nginx安装后，二进制文件、pid、lock等其他文件的路径可参见configure参数中路径类选项的说明have=NGX_SBIN_PATH value=&quot;\\&quot;$NGX_SBIN_PATH\\&quot;&quot; . auto/definehave=NGX_CONF_PATH value=&quot;\\&quot;$NGX_CONF_PATH\\&quot;&quot; . auto/definehave=NGX_PID_PATH value=&quot;\\&quot;$NGX_PID_PATH\\&quot;&quot; . auto/definehave=NGX_LOCK_PATH value=&quot;\\&quot;$NGX_LOCK_PATH\\&quot;&quot; . auto/definehave=NGX_ERROR_LOG_PATH value=&quot;\\&quot;$NGX_ERROR_LOG_PATH\\&quot;&quot; . auto/definehave=NGX_HTTP_LOG_PATH value=&quot;\\&quot;$NGX_HTTP_LOG_PATH\\&quot;&quot; . auto/definehave=NGX_HTTP_CLIENT_TEMP_PATH value=&quot;\\&quot;$NGX_HTTP_CLIENT_TEMP_PATH\\&quot;&quot; . auto/definehave=NGX_HTTP_PROXY_TEMP_PATH value=&quot;\\&quot;$NGX_HTTP_PROXY_TEMP_PATH\\&quot;&quot; . auto/definehave=NGX_HTTP_FASTCGI_TEMP_PATH value=&quot;\\&quot;$NGX_HTTP_FASTCGI_TEMP_PATH\\&quot;&quot; . auto/definehave=NGX_HTTP_UWSGI_TEMP_PATH value=&quot;\\&quot;$NGX_HTTP_UWSGI_TEMP_PATH\\&quot;&quot; . auto/definehave=NGX_HTTP_SCGI_TEMP_PATH value=&quot;\\&quot;$NGX_HTTP_SCGI_TEMP_PATH\\&quot;&quot; . auto/define#创建编译时使用的objs/Makefile文件. auto/make#为objs/Makefile加入需要连接的第三方静态库、动态库或者目标文件. auto/lib/make#为objs/Makefile加入install功能，当执行make install时将编译生成的必要文件复制到安装路径，建立必要的目录. auto/install# 在ngx_auto_config.h文件中加入NGX_SUPPRESS_WARN宏、NGX_SMP宏. auto/stubs#在ngx_auto_config.h文件中指定NGX_USER和NGX_GROUP宏，如果执行configure时没有参数指定，默认两者皆为nobody（也就是默认以nobody用户运行进程）have=NGX_USER value=&quot;\\&quot;$NGX_USER\\&quot;&quot; . auto/definehave=NGX_GROUP value=&quot;\\&quot;$NGX_GROUP\\&quot;&quot; . auto/define#显示configure执行的结果，如果失败，则给出原因. auto/summary （注：在configure脚本里检查某个特性是否存在时，会生成一个最简单的只包含main函数的C程序，该程序会包含相应的头文件。然后，通过检查是否可以编译通过来确认特性是否支持，并将结果记录在objs&#x2F;autoconf.err文件中。后续检查头文件、检查特性的脚本都用了类似的方法。） 1.5.3 configure生成的文件当configure执行成功时会生成objs目录，并在该目录下产生以下目录和文件： 1234567891011121314151617|---ngx_auto_headers.h|---autoconf.err|---ngx_auto_config.h|---ngx_modules.c|---src| |---core| |---event| | |---modules| |---os| | |---unix| | |---win32| |---http| | |---modules| | | |---perl| |---mail| |---misc|---Makefile 上述目录和文件介绍如下。 src目录用于存放编译时产生的目标文件。 Makefile文件用于编译Nginx工程以及在加入install参数后安装Nginx。 autoconf.err保存configure执行过程中产生的结果。 ngx_auto_headers.h和ngx_auto_config.h保存了一些宏，这两个头文件会被src&#x2F;core&#x2F;ngx_config.h及src&#x2F;os&#x2F;unix&#x2F;ngx_linux_config.h文件（可将“linux”替换为其他UNIX操作系统）引用。 ngx_modules.c是一个关键文件，我们需要看看它的内部结构。一个默认配置下生成的ngx_modules.c文件内容如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#include &lt;ngx_config.h&gt;#include &lt;ngx_core.h&gt;…ngx_module_t *ngx_modules[] = &#123;&amp;ngx_core_module,&amp;ngx_errlog_module,&amp;ngx_conf_module,&amp;ngx_events_module,&amp;ngx_event_core_module,&amp;ngx_epoll_module,&amp;ngx_http_module,&amp;ngx_http_core_module,&amp;ngx_http_log_module,&amp;ngx_http_upstream_module,&amp;ngx_http_static_module,&amp;ngx_http_autoindex_module,&amp;ngx_http_index_module,&amp;ngx_http_auth_basic_module,&amp;ngx_http_access_module,&amp;ngx_http_limit_zone_module,&amp;ngx_http_limit_req_module,&amp;ngx_http_geo_module,&amp;ngx_http_map_module,&amp;ngx_http_split_clients_module,&amp;ngx_http_referer_module,&amp;ngx_http_rewrite_module,&amp;ngx_http_proxy_module,&amp;ngx_http_fastcgi_module,&amp;ngx_http_uwsgi_module,&amp;ngx_http_scgi_module,&amp;ngx_http_memcached_module,&amp;ngx_http_empty_gif_module,&amp;ngx_http_browser_module,&amp;ngx_http_upstream_ip_hash_module,&amp;ngx_http_write_filter_module,&amp;ngx_http_header_filter_module,&amp;ngx_http_chunked_filter_module,&amp;ngx_http_range_header_filter_module,&amp;ngx_http_gzip_filter_module,&amp;ngx_http_postpone_filter_module,&amp;ngx_http_ssi_filter_module,&amp;ngx_http_charset_filter_module,&amp;ngx_http_userid_filter_module,&amp;ngx_http_headers_filter_module,&amp;ngx_http_copy_filter_module,&amp;ngx_http_range_body_filter_module,&amp;ngx_http_not_modified_filter_module,NULL&#125;; ngx_modules.c文件就是用来定义ngx_modules数组的。ngx_modules是非常关键的数组，它指明了每个模块在Nginx中的优先级，当一个请求同时符合多个模块的处理规则时，将按照它们在ngx_modules数组中的顺序选择最靠前的模块优先处理。对于HTTP过滤模块而言则是相反的，因为HTTP框架在初始化时，会在ngx_modules数组中将过滤模块按先后顺序向过滤链表中添加，但每次都是添加到链表的表头，因此，对HTTP过滤模块而言，在ngx_modules数组中越是靠后的模块反而会首先处理HTTP响应（参见第6章及第11章的11.9节）。因此，ngx_modules中模块的先后顺序非常重要，不正确的顺序会导致Nginx无法工作，这是auto&#x2F;modules脚本执行后的结果。读者可以体会一下上面的ngx_modules中同一种类型下（第8章会介绍模块类型，第10章、第11章将介绍的HTTP框架对HTTP模块的顺序是最敏感的）各个模块的顺序以及这种顺序带来的意义。可以看出，在安装过程中，configure做了大量的幕后工作，我们需要关注在这个过程中Nginx做了哪些事情。configure除了寻找依赖的软件外，还针对不同的UNIX操作系统做了许多优化工作。这是Nginx跨平台的一种具体实现，也体现了Nginx追求高性能的一贯风格。configure除了生成Makefile外，还生成了ngx_modules.c文件，它决定了运行时所有模块的优先级（在编译过程中而不是编码过程中）。对于不需要的模块，既不会加入ngx_modules数组，也不会编译进Nginx产品中，这也体现了轻量级的概念。[1] 在configure脚本里检查某个特性是否存在时，会生成一个最简单的只 包含main函数的C程序，该程序会包含相应的头文件。然后，通过检查是否可以编译通过来确认特性是否支持，并将结果记录在objs&#x2F;autoconf.err文件中。后续检查头文件、检查特性的脚本都用了类似的方法。 1.6 Nginx的命令行控制在Linux中，需要使用命令行来控制Nginx服务器的启动与停止、重载配置文件、回滚日志文件、平滑升级等行为。默认情况下，Nginx被安装在目录&#x2F;usr&#x2F;local&#x2F;nginx&#x2F;中，其二进制文件路径为&#x2F;usr&#x2F;local&#x2F;nginc&#x2F;sbin&#x2F;nginx，配置文件路径为&#x2F;usr&#x2F;local&#x2F;nginx&#x2F;conf&#x2F;nginx.conf。当然，在configure执行时是可以指定把它们安装在不同目录的。为了简单起见，本节只说明默认安装情况下的命令行的使用情况，如果读者安装的目录发生了变化，那么替换一下即可。 默认方式启动直接执行Nginx二进制程序。例如：1/usr/local/nginx/sbin/nginx 这时，会读取默认路径下的配置文件：&#x2F;usr&#x2F;local&#x2F;nginx&#x2F;conf&#x2F;nginx.conf。实际上，在没有显式指定nginx.conf配置文件路径时，将打开在configure命令执行时使用–conf-path&#x3D;PATH指定的nginx.conf文件（参见1.5.1节）。 另行指定配置文件的启动方式使用-c参数指定配置文件。例如：1/usr/local/nginx/sbin/nginx -c /tmp/nginx.conf 这时，会读取-c参数后指定的nginx.conf配置文件来启动Nginx。 另行指定安装目录的启动方式使用-p参数指定Nginx的安装目录。例如：1/usr/local/nginx/sbin/nginx -p /usr/local/nginx/ 另行指定全局配置项的启动方式可以通过-g参数临时指定一些全局配置项，以使新的配置项生效。例如：1/usr/local/nginx/sbin/nginx -g &quot;pid /var/nginx/test.pid;&quot; 上面这行命令意味着会把pid文件写到&#x2F;var&#x2F;nginx&#x2F;test.pid中。-g参数的约束条件是指定的配置项不能与默认路径下的nginx.conf中的配置项相冲突，否则无法启动。就像上例那样，类似这样的配置项：pid logs&#x2F;nginx.pid，是不能存在于默认的nginx.conf中的。另一个约束条件是，以-g方式启动的Nginx服务执行其他命令行时，需要把-g参数也带上，否则可能出现配置项不匹配的情形。例如，如果要停止Nginx服务，那么需要执行下面代码：1/usr/local/nginx/sbin/nginx -g &quot;pid /var/nginx/test.pid;&quot; -s stop 如果不带上-g”pid&#x2F;var&#x2F;nginx&#x2F;test.pid;”，那么找不到pid文件，也会出现无法停止服务的情况。 测试配置信息是否有错误在不启动Nginx的情况下，使用-t参数仅测试配置文件是否有错误。例如：1/usr/local/nginx/sbin/nginx -t 执行结果中显示配置是否正确。 在测试配置阶段不输出信息测试配置选项时，使用-q参数可以不把error级别以下的信息输出到屏幕。例如：1/usr/local/nginx/sbin/nginx -t -q 显示版本信息使用-v参数显示Nginx的版本信息。例如：1/usr/local/nginx/sbin/nginx -v 显示编译阶段的参数使用-V参数除了可以显示Nginx的版本信息外，还可以显示配置编译阶段的信息，如GCC编译器的版本、操作系统的版本、执行configure时的参数等。例如：1/usr/local/nginx/sbin/nginx -V 快速地停止服务使用-s stop可以强制停止Nginx服务。-s参数其实是告诉Nginx程序向正在运行的Nginx服务发送信号量，Nginx程序通过nginx.pid文件中得到master进程的进程ID，再向运行中的master进程发送TERM信号来快速地关闭Nginx服务。例如：1/usr/local/nginx/sbin/nginx -s stop 实际上，如果通过kill命令直接向nginx master进程发送TERM或者INT信号，效果是一样的。例如，先通过ps命令来查看nginx master的进程ID：123:ahf5wapi001:root &gt; ps -ef | grep nginxroot 10800 1 0 02:27 ? 00:00:00 nginx: master process ./nginxroot 10801 10800 0 02:27 ? 00:00:00 nginx: worker process 接下来直接通过kill命令来发送信号：1kill -s SIGTERM 10800 或者：1kill -s SIGINT 10800 上述两条命令的效果与执行&#x2F;usr&#x2F;local&#x2F;nginx&#x2F;sbin&#x2F;nginx-s stop是完全一样的。 “优雅”地停止服务如果希望Nginx服务可以正常地处理完当前所有请求再停止服务，那么可以使用-s quit参数来停止服务。例如：1/usr/local/nginx/sbin/nginx -s quit 该命令与快速停止Nginx服务是有区别的。当快速停止服务时，worker进程与master进程在收到信号后会立刻跳出循环，退出进程。而“优雅”地停止服务时，首先会关闭监听端口，停止接收新的连接，然后把当前正在处理的连接全部处理完，最后再退出进程。 与快速停止服务相似，可以直接发送QUIT信号给master进程来停止服务，其效果与执行-s quit命令是一样的。例如：1kill -s SIGQUIT &lt;nginx master pid&gt; 如果希望“优雅”地停止某个worker进程，那么可以通过向该进程发送WINCH信号来停止服务。例如：1kill -s SIGWINCH &lt;nginx worker pid&gt; 使运行中的Nginx重读配置项并生效使用-s reload参数可以使运行中的Nginx服务重新加载nginx.conf文件。例如：1/usr/local/nginx/sbin/nginx -s reload 事实上，Nginx会先检查新的配置项是否有误，如果全部正确就以“优雅”的方式关闭，再重新启动Nginx来实现这个目的。类似的，-s是发送信号，仍然可以用kill命令发送HUP信号来达到相同的效果。1kill -s SIGHUP &lt;nginx master pid&gt; 日志文件回滚使用-s reopen参数可以重新打开日志文件，这样可以先把当前日志文件改名或转移到其他目录中进行备份，再重新打开时就会生成新的日志文件。这个功能使得日志文件不至于过大。例如：1/usr/local/nginx/sbin/nginx -s reopen 当然，这与使用kill命令发送USR1信号效果相同。1kill -s SIGUSR1 &lt;nginx master pid&gt; 平滑升级Nginx当Nginx服务升级到新的版本时，必须要将旧的二进制文件Nginx替换掉，通常情况下这是需要重启服务的，但Nginx支持不重启服务来完成新版本的平滑升级。升级时包括以下步骤： 通知正在运行的旧版本Nginx准备升级。通过向master进程发送USR2信号可达到目的。例如：1kill -s SIGUSR2 &lt;nginx master pid&gt; 这时，运行中的Nginx会将pid文件重命名，如将&#x2F;usr&#x2F;local&#x2F;nginx&#x2F;logs&#x2F;nginx.pid重命名为&#x2F;usr&#x2F;local&#x2F;nginx&#x2F;logs&#x2F;nginx.pid.oldbin，这样新的Nginx才有可能启动成功。 启动新版本的Nginx，可以使用以上介绍过的任意一种启动方法。这时通过ps命令可以发现新旧版本的Nginx在同时运行。 通过kill命令向旧版本的master进程发送SIGQUIT信号，以“优雅”的方式关闭旧版本的Nginx。随后将只有新版本的Nginx服务运行，此时平滑升级完毕。 显示命令行帮助使用-h或者-?参数会显示支持的所有命令行参数。 1.7 小结本章介绍了Nginx的特点以及在什么场景下需要使用Nginx，同时介绍了如何获取Nginx以及如何配置、编译、安装运行Nginx。本章还深入介绍了最为复杂的configure过程，这部分内容是学习本书第二部分和第三部分的基础。"},{"title":"第一章 Kubernetes介绍","path":"/wiki/kubernetes-in-action/section_01/chapter_01.html","content":"本章内容涵盖 应用的开发和部署方式在近几年的发展趋势 容器如何保障应用间的隔离性，以及减少应用对部署环境的依赖性 Docker容器如何在Kubernetes系统中应用 Kubernetes如何提高开发人员和系统管理员的工作效率 在过去，多数的应用都是大型单体应用，以单个进程或几个进程的方式，运行于几台服务器之上。这些应用的发布周期长，而且迭代也不频繁。每个发布周期结束前，开发者会把应用程序打包后交付给运维团队，运维人员再处理部署、监控事宜，并且在硬件发生故障时手动迁移应用。今天，大型单体应用正被逐渐分解成小的、可独立运行的组件，我们称之为微服务。微服务彼此之间解耦，所以它们可以被独立开发、部署、升级、伸缩。这使得我们可以对每一个微服务实现快速迭代，并且迭代的速度可以和市场需求变化的速度保持一致。但是，随着部署组件的增多和数据中心的增长，配置、管理并保持系统的正常运行变得越来越困难。如果我们想要获得足够高的资源利用率并降低硬件成本，把组件部署在什么地方变得越来越难以决策。手动做所有的事情，显然不太可行。我们需要一些自动化的措施，包括自动调度、配置、监管和故障处理。这正是Kubernetes的用武之地。Kubernetes使开发者可以自主部署应用，并且控制部署的频率，完全脱离运维团队的帮助。Kubernetes同时能让运维团队监控整个系统，并且在硬件故障时重新调度应用。系统管理员的工作重心，从监管应用转移到了监管Kubernetes，以及剩余的系统资源，因为Kubernetes会帮助监管所有的应用。 注意Kubernetes是希腊语中的“领航员”或“舵手”的意思。Kubernetes有几种不同的发音方式。许多人把它读成Koo-ber-nay-tace，还有一些人读成Koo-bernetties。不管你用哪种方式，大家都知道你指的是这个单词。 Kubernetes抽象了数据中心的硬件基础设施，使得对外暴露的只是一个巨大的资源池。它让我们在部署和运行组件时，不用关注底层的服务器。使用Kubernetes部署多组件应用时，它会为每个组件都选择一个合适的服务器，部署之后它能够保证每个组件可以轻易地发现其他组件，并彼此之间实现通信。所以说应用Kubernetes可以给大多数场景下的数据中心带来增益，这不仅包括内部部署（on-premises）的数据中心，如果是类似云厂商提供的那种超大型数据中心，这种增益是更加明显的。通过Kubernetes，云厂商提供给开发者的是一个可部署且可运行任何类型应用的简易化云平台，云厂商的系统管理员可以不用关注这些海量应用到底是什么。随着越来越多的大公司把Kubernetes作为它们运行应用的最佳平台，Kubernetes帮助企业标准化了无论是云端部署还是内部部署的应用交付方式。 1.1 Kubernetes系统的需求在开始了解Kubernetes的细节之前，我们快速看一下近年来应用程序的开发部署是如何变化的。变化是由两方面导致的，一方面是大型单体应用被拆解为更多的小型微服务，另一方面是应用运行所依赖的基础架构的变化。理解这些变化，能帮助我们更好地看待使用Kubernetes和容器技术带来的好处。 1.1.1 从单体应用到微服务单体应用由很多个组件组成，这些组件紧密地耦合在一起，由于它们在同一个操作系统进程中运行，所以在开发、部署、管理的时候必须以同一个实体进行。对单体应用来说，即使是某个组件中一个小的修改，都需要重新部署整个应用。组件间缺乏严格的边界定义，相互依赖，日积月累导致系统复杂度提升，整体质量也急剧恶化。运行一个单体应用，通常需要一台能为整个应用提供足够资源的高性能服务器。为了应对不断增长的系统负荷，我们需要通过增加CPU、 内存或其他系统资源的方式来对服务器做垂直扩展，或者增加更多的跑 这些应用程序的服务器的做水平扩展。垂直扩展不需要应用程序做任何变化，但是成本很快会越来越高，并且通常会有瓶颈。如果是水平扩展，就可能需要应用程序代码做比较大的改动，有时候甚至是不可行的，比如系统的一些组件非常难于甚至不太可能去做水平扩展（像关系型数据库）。如果单体应用的任何一个部分不能扩展，整个应用就不能扩展，除非我们想办法把它拆分开。 将应用拆解为多个微服务这些问题迫使我们将复杂的大型单体应用，拆分为小的可独立部署的微服务组件。每个微服务以独立的进程（见图1.1）运行，并通过简单且定义良好的接口（API）与其他的微服务通信。服务之间可以通过类似HTTP这样的同步协议通信，或者通过像AMQP这样的异步协议通信。这些协议能够被大多数开发者所理解，并且并不局限于某种编程语言。这意味着任何一个微服务，都可以用最适合的开发语言来实现。 图1.1 单体应用中的组件与独立的微服务 因为每个微服务都是独立的进程，提供相对静态的API，所以独立开发和部署单个微服务成了可能。只要API不变或者向前兼容，改动一个微服务，并不会要求对其他微服务进行改动或者重新部署。 微服务的扩容面向单体系统，扩容针对的是整个系统，而面向微服务架构，扩容却只需要针对单个服务，这意味着你可以选择仅扩容那些需要更多资源 的服务而保持其他的服务仍然维持在原来的规模。如图1.2所示，三种组件都被复制了多个，并以多进程的方式部署在不同的服务器上，而另外的组件只能以单体进程应用运行。当单体应用因为其中一部分无法扩容而整体被限制扩容时，可以把应用拆分成多个微服务，将那些能进行扩容的组件进行水平扩展，不能进行扩容的组件进行垂直扩展。 图1.2 每个微服务能被单独扩容 部署微服务像大多数情况一样，微服务也有缺点。若你的系统仅包含少许可部署的组件，管理那些组件是简单的。决定每个组件部署在哪儿是不重要的，因为没有那么多选择。当组件数量增加时，部署相关的决定就变得越来越困难。因为不仅组件部署的组合数在增加，而且组件间依赖的组合数也在以更大的因素增加。微服务以团队形式完成工作，所以需要找到彼此进行交流。部署微服务时，部署者需要正确地配置所有服务来使其作为一个单一系统能正确工作，随着微服务的数量不断增加，配置工作变得冗杂且易错，特别是当你思考服务器宕机时运维团队需要做什么的时候。微服务还带来其他问题，比如因为跨了多个进程和机器，使得调试代码和定位异常调用变得困难。幸运的是，这些问题现在已经被诸如Zipkin这样的分布式定位系统解决。 环境需求的差异正如已经提到的，一个微服务架构中的组件不仅被独立部署，也被独立开发。因为它们的独立性，出现不同的团队开发不同的组件是很正常的事实，每个团队都有可能使用不同的库并在需求升级时替换它们。如图1.3所示，因为组件之间依赖的差异性，应用程序需要同一个库的不同版本是不可避免的。 图1.3 多个应用在同一个主机上运行可能会有依赖冲突 部署动态链接的应用需要不同版本的共享库，或者需要其他特殊环境，在生产服务器部署并管理这种应用很快会成为运维团队的噩梦。需要在同一个主机上部署的组件数量越大，满足这些组件的所有需求就越难。 1.1.2 为应用程序提供一个一致的环境不管你同时开发和部署多少个独立组件，开发和运维团队总是需要解决的一个最大的问题是程序运行环境的差异性，这种巨大差异不仅存在于开发环境与生产环境之间，甚至存在于各个生产机器之间。另外一个无法避免的事实是生产机器的环境会随着时间的推移而变化。这些差异性存在于从硬件到操作系统再到每台机器的可用库上。生产环境是由运维团队管理的，而开发者常常比较关心他们自己的开发环境。这两组人对系统管理的理解程度是不同的，这个理解偏差导致两个环境的系统有较大的差异，系统管理员更重视保持系统更新最近的安全补丁，而大多数开发者则并不太关心。生产系统可能要运行多个开发者或者开发团队的应用，而对于开发者的电脑来说就不是这个情况了。一个生产系统必须给所有它需要承载的应用提供合适的环境，尽管这些应用可能需要不同的，甚至带有冲突的版本库。为了减少仅会在生产环境才暴露的问题，最理想的做法是让应用在开发和生产阶段可以运行在完全一样的环境下，它们有完全一样的操作系统、库、系统配置、网络环境和其他所有的条件。你也不想让这个环境随着时间推移而改变。如果可能，你想要确保在一台服务器上部署新的应用时，不会影响到机器上已有的应用。 1.1.3 迈向持续交付：DevOps和无运维在最近几年中，我们看到了应用在开发流程和生产运维流程中的变化。在过去，开发团队的任务是创建应用并交付给运维团队，然后运维团队部署应用并使它运行。但是现在，公司都意识到，让同一个团队参与应用的开发、部署、运维的整个生命周期更好。这意味着开发者、QA和运维团队彼此之间的合作需要贯穿整个流程。这种实践被称为DevOps。 带来的优点让开发者更多地在生产环境中运行应用，能够使他们对用户的需求和问题，以及运维团队维护应用所面临的困难，有一个更好的理解。应用程序开发者现在更趋向于将应用尽快地发布上线，通过收集用户的反馈对应用做进一步开发。为了频繁地发布应用，就需要简化你的部署流程。理想的状态是开发人员能够自己部署应用上线，而不需要交付给运维人员操作。但是，部署应用往往需要具备对数据中心底层设备和硬件架构的理解。开发人员却通常不知道或者不想知道这些细节。 让开发者和系统管理员做他们最擅长的成功运行一个应用并服务于客户，这是开发者和系统管理员共同的目标，但他们也有着不同的个人目标和驱动因素。开发者热衷于创造新的功能和提升用户体验，他们通常不想成为确保底层操作系统已经更新所有安全补丁的那些人，他们更喜欢把那些事留给系统管理员。运维团队负责管理生产部署流程及应用所在的硬件设备。他们关心系统安全、使用率，以及其他对于开发者来说优先级不高的东西。但是，运维人员不想处理所有应用组件之间暗含的内部依赖，也不想考虑底层操作系统或者基础设施的改变会怎样影响到应用程序，但是他们却不得不关注这些事情。理想情况是，开发者是部署程序本身，不需要知道硬件基础设施的任何情况，也不需要和运维团队交涉，这被叫作NoOps。很明显，你仍然需要有一些人来关心硬件基础设施，但这些人不需要再处理应用程序的独特性。正如你所看到的，Kubernetes能让我们实现所有这些想法。通过对实际硬件做抽象，然后将自身暴露成一个平台，用于部署和运行应用程序。它允许开发者自己配置和部署应用程序，而不需要系统管理员的任何帮助，让系统管理员聚焦于保持底层基础设施运转正常的同时，不需要关注实际运行在平台上的应用程序。 1.2 介绍容器技术在1.1节中，罗列了一个不全面的开发和运维团队如今所面临的问题列表，尽管你有很多解决这些问题的方式，但本书将关注如何用Kubernetes解决。Kubernetes使用Linux容器技术来提供应用的隔离，所以在钻研Kubernetes之前，需要通过熟悉容器的基本知识来更加深入地理解Kubernetes，包括认识到存在的容器技术分支，诸如Docker或者rkt。 1.2.1 什么是容器在1.1.1节中，我们看到在同一台机器上运行的不同组件需要不同的、可能存在冲突的依赖库版本，或者是其他的不同环境需求。当一个应用程序仅由较少数量的大组件构成时，完全可以接受给每个组件分配专用的虚拟机，以及通过给每个组件提供自己的操作系统实例来隔离它们的环境。但是当这些组件开始变小且数量开始增长时，如果你不想浪费硬件资源，又想持续压低硬件成本，那就不能给每个组件配置一个虚拟机了。但是这还不仅仅是浪费硬件资源，因为每个虚拟机 都需要被单独配置和管理，所以增加虚拟机的数量也就导致了人力资源的浪费，因为这增加了系统管理员的工作负担。 用Linux容器技术隔离组件开发者不是使用虚拟机来隔离每个微服务环境（或者通常说的软件进程），而是正在转向Linux容器技术。容器允许你在同一台机器上运行多个服务，不仅提供不同的环境给每个服务，而且将它们互相隔离。容器类似虚拟机，但开销小很多。一个容器里运行的进程实际上运行在宿主机的操作系统上，就像所有其他进程一样（不像虚拟机，进程是运行在不同的操作系统上的）。但在容器里的进程仍然是和其他进程隔离的。对于容器内进程本身而 言，就好像是在机器和操作系统上运行的唯一一个进程。 比较虚拟机和容器和虚拟机比较，容器更加轻量级，它允许在相同的硬件上运行更多数量的组件。主要是因为每个虚拟机需要运行自己的一组系统进程，这就产生了除组件进程消耗以外的额外计算资源损耗。从另一方面说，一个容器仅仅是运行在宿主机上被隔离的单个进程，仅消耗应用容器消耗的资源，不会有其他进程的开销。因为虚拟机的额外开销，导致没有足够的资源给每个应用开一个专用的虚拟机，最终会将多个应用程序分组塞进每个虚拟机。当使用容器时，正如图1.4所示，能够（也应该）让每个应用有一个容器。最终结 果就是可以在同一台裸机上运行更多的应用程序。 图1.4 使用虚拟机来隔离一组应用程序与使用容器隔离单个应用程序 当你在一台主机上运行三个虚拟机的时候，你拥有了三个完全分离的操作系统，它们运行并共享一台裸机。在那些虚拟机之下是宿主机的操作系统与一个管理程序，它将物理硬件资源分成较小部分的虚拟硬件资源，从而被每个虚拟机里的操作系统使用。运行在那些虚拟机里的应用程序会执行虚拟机操作系统的系统调用，然后虚拟机内核会通过管理 程序在宿主机上的物理来CPU执行x86指令。 注意存在两种类型的管理程序。第一种类型的管理程序不会使用 宿主机OS，而第二种类型的会。 多个容器则会完全执行运行在宿主机上的同一个内核的系统调用，此内核是唯一一个在宿主机操作系统上执行x86指令的内核。CPU也不需要做任何对虚拟机能做那样的虚拟化（如图 1.5所示）。 图1.5 虚拟机和容器中的应用程序对CPU的不同使用方式 虚拟机的主要好处是它们提供完全隔离的环境，因为每个虚拟机运行在它自己的Linux内核上，而容器都是调用同一个内核，这自然会有安全隐患。如果你的硬件资源有限，那当你有少量进程需要隔离的时候，虚拟机就可以成为一个选项。为了在同一台机器上运行大量被隔离的进程，容器因它的低消耗而成为一个更好的选择。记住，每个虚拟机 运行它自己的一组系统服务，而容器则不会，因为它们都运行在同一个 操作系统上。那也就意味着运行一个容器不用像虚拟机那样要开机，它的进程可以很快被启动。 容器实现隔离机制介绍你可能会好奇，如果多个进程运行在同一个操作系统上，那容器到底是怎样隔离它们的。有两个机制可用：第一个是Linux命名空间，它使每个进程只看到它自己的系统视图（文件、进程、网络接口、主机名等）；第二个是Linux控制组（cgroups），它限制了进程能使用的资源量（CPU、内存、网络带宽等）。 用Linux命名空间隔离进程默认情况下，每个Linux系统最初仅有一个命名空间。所有系统资源（诸如文件系统、用户ID、网络接口等）属于这一个命名空间。但是你能创建额外的命名空间，以及在它们之间组织资源。对于一个进程，可以在其中一个命名空间中运行它。进程将只能看到同一个命名空间下的资源。当然，会存在多种类型的多个命名空间，所以一个进程不单单 只属于某一个命名空间，而属于每个类型的一个命名空间。存在以下类型的命名空间： Mount（mnt） Process ID（pid） Network（net） Inter-process communicaion（ipd） UTS User ID（user） 每种命名空间被用来隔离一组特定的资源。例如，UTS命名空间决定了运行在命名空间里的进程能看见哪些主机名和域名。通过分派两个不同的UTS命名空间给一对进程，能使它们看见不同的本地主机名。换句话说，这两个进程就好像正在两个不同的机器上运行一样（至少就主 机名而言是这样的）。同样地，一个进程属于什么Network命名空间决定了运行在进程里的应用程序能看见什么网络接口。每个网络接口属于一个命名空间，但是可以从一个命名空间转移到另一个。每个容器都使用它自己的网络命名空间，因此每个容器仅能看见它自己的一组网络接口。现在你应该已经了解命名空间是如何隔离容器中运行的应用的。 限制进程的可用资源另外的隔离性就是限制容器能使用的系统资源。这通过cgroups来实现。cgroups是一个Linux内核功能，它被用来限制一个进程或者一组进程的资源使用。一个进程的资源（CPU、内存、网络带宽等）使用量不能超出被分配的量。这种方式下，进程不能过分使用为其他进程保留的资源，这和进程运行在不同的机器上是类似的。 1.2.2 Docker容器平台介绍尽管容器技术已经出现很久，却是随着Docker容器平台的出现而变得广为人知。Docker是第一个使容器能在不同机器之间移植的系统。它不仅简化了打包应用的流程，也简化了打包应用的库和依赖，甚至整个操作系统的文件系统能被打包成一个简单的可移植的包，这个包可以被用来在任何其他运行Docker的机器上使用。当你用Docker运行一个被打包的应用程序时，它能看见你捆绑的文件系统的内容，不管运行在开发机器还是生产机器上，它都能看见相同的文件，即使生产机器运行的是完全不同的操作系统。应用程序不会关心它所在服务器上的任何东西，所以生产服务器上是否安装了和你开发机完全相同的一组库是不需要关心的。例如，如果你用整个红帽企业版Linux（RHEL）的文件打包了你的应用程序，不管在装有Fedora的开发机上运行它，还是在装有Debian或者其他Linux发行版的服务器上运行它，应用程序都认为它运行在RHEL中。只是内核可能不同。与在虚拟机中安装操作系统得到一个虚拟机镜像，再将应用程序打包到镜像里，通过分发整个虚拟机镜像到主机，使应用程序能够运行起来类似，Docker也能够达到相同的效果，但不是使用虚拟机来实现应用隔离，而是使用之前几节中提到的Linux容器技术来达到和虚拟机相同级别的隔离。容器也不使用庞大的单个虚拟机镜像，它使用较小的容器镜像。基于Docker容器的镜像和虚拟机镜像的一个很大的不同是容器镜像是由多层构成，它能在多个镜像之间共享和征用。如果某个已经被下载的容器镜像已经包含了后面下载镜像的某些层，那么后面下载的镜像就无须再下载这些层。 Docker的概念Docker是一个打包、分发和运行应用程序的平台。正如我们所说，它允许将你的应用程序和应用程序所依赖的整个环境打包在一起。这既可以是一些应用程序需要的库，也可以是一个被安装的操作系统所有可用的文件。Docker使得传输这个包到一个中央仓库成为可能，然后这个包就能被分发到任何运行Docker的机器上，在那儿被执行（大部分情况是这样的，但并不尽然，后面将做出解释）。三个主要概念组成了这种情形： 镜像 — Docker镜像里包含了你打包的应用程序及其所依赖的环境。它包含应用程序可用的文件系统和其他元数据，如镜像运行时的可执行文件路径。 镜像仓库 — Docker镜像仓库用于存放Docker镜像，以及促进不同人和不同电脑之间共享这些镜像。当你编译你的镜像时，要么可以在编译它的电脑上运行，要么可以先上传镜像到一个镜像仓库，然后下载到另外一台电脑上并运行它。某些仓库是公开的，允许所有人从中拉取镜像，同时也有一些是私有的，仅部分人和机器可接入。 容器 — Docker容器通常是一个Linux容器，它基于Docker镜像被创建。一个运行中的容器是一个运行在Docker主机上的进程，但它和主机，以及所有运行在主机上的其他进程都是隔离的。这个进程也是资源受限的，意味着它只能访问和使用分配给它的资源（CPU、内存等） 构建、分发和运行Dcoker镜像图1.6显示了这三个概念以及它们之间的关系。开发人员首先构建一个镜像，然后把镜像推到镜像仓库中。因此，任何可以访问镜像仓库 的人都可以使用该镜像。然后，他们可以将镜像拉取到任何运行着 Docker的机器上并运行镜像。Docker会基于镜像创建一个独立的容器， 并运行二进制可执行文件指定其作为镜像的一部分。 图1.6 Docker镜像、镜像仓库和容器 对比虚拟机与Docker容器由上文可知，Linux容器和虚拟机的确有相像之处，但容器更轻量级。现在让我们看一下Docker容器和虚拟机的具体比较（以及Docker镜像和虚拟机镜像的比较）。如图例1.7所示，相同的6个应用程序分别运 行在虚拟机上和用Docker容器运行。 图1.7 在3个虚拟机上运行6个应用及用Docker容器运行它们 你会注意到应用A和应用B无论是运行在虚拟机上还是作为两个分离容器运行时都可以访问相同的二进制和库。在虚拟机里，这是显然 的，因为两个应用都看到相同的文件系统。但是我们知道每个容器有它自己隔离的文件系统，那应用A和应用B如何共享同样的文件？ 镜像层前面已经说过Docker镜像由多层构成。不同镜像可能包含完全相同的层，因为这些Docker镜像都是基于另一个镜像之上构建的，不同的镜像都能使用相同的父镜像作为它们的基础镜像。这提升了镜像在网络上的分发效率，当传输某个镜像时，因为相同的层已被之前的镜像传输，那么这些层就不需要再被传输。层不仅使分发更高效，也有助于减少镜像的存储空间。每一层仅被存一次，当基于相同基础层的镜像被创建成两个容器时，它们就能够读相同的文件。但是如果其中一个容器写入某些文件，另外一个是无法看见文件变更的。因此，即使它们共享文件，仍然彼此隔离。这是因为容器镜像层是只读的。容器运行时，一个新的可写层在镜像层之上被创建。容器中进程写入位于底层的一个文件时，此文件的一个拷贝在顶层被创建，进程写的是此拷贝。 容器镜像可移植性的限制理论上，一个容器镜像能运行在任何一个运行Docker的机器上。但有一个小警告——一个关于运行在一台机器上的所有容器共享主机Linux内核的警告。如果一个容器化的应用需要一个特定的内核版本，那它可能不能在每台机器上都工作。如果一台机器上运行了一个不匹配的Linux内核版本，或者没有相同内核模块可用，那么此应用就不能在其上运行。虽然容器相比虚拟机轻量许多，但也给运行于其中的应用带来了一些局限性。虚拟机没有这些局限性，因为每个虚拟机都运行自己的内核。还不仅是内核的问题。一个在特定硬件架构之上编译的容器化应用，只能在有相同硬件架构的机器上运行。不能将一个x86架构编译的应用容器化后，又期望它能运行在ARM架构的机器上。你仍然需要一台虚拟机来做这件事情。 1.2.3 rkt——一个Docker的替代方案Docker是第一个使容器成为主流的容器平台。Docker本身并不提供进程隔离，实际上容器隔离是在Linux内核之上使用诸如Linux命名空间和cgroups之类的内核特性完成的，Docker仅简化了这些特性的使用。在Docker成功后，开放容器计划（OCI）就开始围绕容器格式和运行时创建了开放工业标准。Docker是计划的一部分，rkt（发音为“rock-it”）则是另外一个Linux容器引擎。和Docker一样，rkt也是一个运行容器的平台，它强调安全性、可构建性并遵从开放标准。它使用OCI容器镜像，甚至可以运行常规的Docker容器镜像。这本书只集中于使用Docker作为Kubernetes的容器，因为它是Kubernetes最初唯一支持的容器类型。最近Kubernetes也开始支持rkt及其他的容器类型。在这里提到rkt的原因是，不应该错误地认为Kubernetes是一个专为Docker容器设计的容器编排系统。实际上，在阅读这本书的过程中，你将会认识到Kubernetes的核心远不止是编排容器。容器恰好是在不同集群节点上运行应用的最佳方式。有了这些意识，终于可以深入探讨本书所讲的核心内容——Kubernetes了。 1.3 Kubernetes介绍我们已经展示了，随着系统可部署组件的数量增长，把它们都管理起来会变得越来越困难。需要一个更好的方式来部署和管理这些组件，并支持基础设施的全球性伸缩，谷歌可能是第一个意识到这一点的公司。谷歌等全球少数几个公司运行着成千上万的服务器，而且在如此海量规模下，不得不处理部署管理的问题。这推动着他们找出解决方案使成千上万组件的管理变得有效且成本低廉。 1.3.1 初衷这些年来，谷歌开发出了一个叫Borg的内部系统（后来还有一个新系统叫Omega），应用开发者和系统管理员管理那些数以千计的应用程序和服务都受益于它的帮助。除了简化开发和管理，它也帮助他们获得了更高的基础设施利用率，在你的组织如此庞大时，这很重要。当你运行成千上万台机器时，哪怕一丁点的利用率提升也意味着节约了数百万美元，所以，开发这个系统的动机是显而易见的。在保守Borg和Omega秘密数十年之后，2014年，谷歌开放了Kubernetes，一个基于Borg、Omega及其他谷歌内部系统实践的开源系统。 1.3.2 深入浅出地了解KubernetesKubernetes是一个软件系统，它允许你在其上很容易地部署和管理容器化的应用。它依赖于Linux容器的特性来运行异构应用，而无须知道这些应用的内部详情，也不需要手动将这些应用部署到每台机器。因为这些应用运行在容器里，它们不会影响运行在同一台服务器上的其他 应用，当你是为完全不同的组织机构运行应用时，这就很关键了。这对于云供应商来说是至关重要的，因为它们在追求高硬件可用率的同时也 必须保障所承载应用的完全隔离。Kubernetes使你在数以千计的电脑节点上运行软件时就像所有这些节点是单个大节点一样。它将底层基础设施抽象，这样做同时简化了应用的开发、部署，以及对开发和运维团队的管理。通过Kubernetes部署应用程序时，你的集群包含多少节点都是一样的。集群规模不会造成什么差异性，额外的集群节点只是代表一些额外的可用来部署应用的资源 Kubernetes的核心功能图1.8展示了一幅最简单的Kubernetes系统图。整个系统由一个主节点和若干个工作节点组成。开发者把一个应用列表提交到主节点，Kubernetes会将它们部署到集群的工作节点。组件被部署在哪个节点对 于开发者和系统管理员来说都不用关心。 图1.8 Kubernetes暴露整个数据中心作为单个开发平台 开发者能指定一些应用必须一起运行，Kubernetes将会在一个工作节点上部署它们。其他的将被分散部署到集群中，但是不管部署在哪儿，它们都能以相同的方式互相通信。 帮助开发者聚焦核心应用功能Kubernetes可以被当作集群的一个操作系统来看待。它降低了开发者不得不在他们的应用里实现一些和基础设施相关服务的心智负担。他们现在依赖于Kubernetes来提供这些服务，包括服务发现、扩容、负载 均衡、自恢复，甚至领导者的选举。应用程序开发者因此能集中精力实 现应用本身的功能而不用浪费时间思索怎样集成应用与基础设施。 帮助运维团队获取更高的资源利用率Kubernetes将你的容器化应用运行在集群的某个地方，并提供信息给应用组件来发现彼此并保证它们的运行。因为你的应用程序不关心它运行在哪个节点上，Kubernetes能在任何时间迁移应用并通过混合和匹 配应用来获得比手动调度高很多的资源利用率。 1.3.3 Kubernetes集群架构我们已经以上帝视角看到了Kubernetes的架构，现在让我们近距离看一下Kubernetes集群由什么组成。在硬件级别，一个Kubernetes集群由很多节点组成，这些节点被分成以下两种类型： 主节点，它承载着Kubernetes控制和管理整个集群系统的控制面板 工作节点，它们运行用户实际部署的应用 图1.9展示了运行在这两组节点上的组件，接下来进一步解释。 图1.9 组成一个Kubernetes集群的组件 控制面板控制面板用于控制集群并使它工作。它包含多个组件，组件可以运行在单个主节点上或者通过副本分别部署在多个主节点以确保高可用性。这些组件是： Kubernetes API服务器，你和其他控制面板组件都要和它通信 Scheculer，它调度你的应用（为应用的每个可部署组件分配一个工 作节点） Controller Manager，它执行集群级别的功能，如复制组件、持续跟 踪工作节点、处理节点失败等 etcd，一个可靠的分布式数据存储，它能持久化存储集群配置 控制面板的组件持有并控制集群状态，但是它们不运行你的应用程 序。这是由工作节点完成的。 工作节点工作节点是运行容器化应用的机器。运行、监控和管理应用服务的 任务是由以下组件完成的： Docker、rtk或其他的容器类型 Kubelet，它与API服务器通信，并管理它所在节点的容器 Kubernetes Service Proxy（kube-proxy），它负责组件之间的负载均 衡网络流量 我们将在第11章中详细解释所有这些组件。笔者不喜欢先解释事物是如何工作的，然后再解释它的功能并教人们如何使用它。就像学习开车，你不想知道引擎盖下是什么，你首先想要学习怎样从A点开到B点。只有在你学会了如何做到这一点后，你才会对汽车如何使这成为可能产生兴趣。毕竟，知道引擎盖下面是什么，可能在有一天它抛锚后你被困在路边时，会帮助你让车再次移动。 1.3.4 在Kubernetes中运行应用为了在Kubernetes中运行应用，首先需要将应用打包进一个或多个容器镜像，再将那些镜像推送到镜像仓库，然后将应用的描述发布到Kubernetes API服务器。该描述包括诸如容器镜像或者包含应用程序组件的容器镜像、这些组件如何相互关联，以及哪些组件需要同时运行在同一个节点上和哪些组件不需要同时运行等信息。此外，该描述还包括哪些组件为内部或外部客户提供服务且应该通过单个IP地址暴露，并使其他组件可以发现。 描述信息怎样成为一个运行的容器当API服务器处理应用的描述时，调度器调度指定组的容器到可用的工作节点上，调度是基于每组所需的计算资源，以及调度时每个节点未分配的资源。然后，那些节点上的Kubelet指示容器运行时（例如Docker）拉取所需的镜像并运行容器。仔细看图1.10以更好地理解如何在Kubernetes中部署应用程序。应用描述符列出了四个容器，并将它们分为三组（这些集合被称为pod，我们将在第3章中解释它们是什么）。前两个pod只包含一个容器，而最后一个包含两个。这意味着两个容器都需要协作运行，不应该相互隔离。在每个pod旁边，还可以看到一个数字，表示需要并行运行的每个pod的副本数量。在向Kubernetes提交描述符之后，它将把每个pod的指定副本数量调度到可用的工作节点上。节点上的Kubelets将告知Docker从镜像仓库中拉取容器镜像并运行容器。 保持容器运行一旦应用程序运行起来，Kubernetes就会不断地确认应用程序的部署状态始终与你提供的描述相匹配。例如，如果你指出你需要运行五个web服务器实例，那么Kubernetes总是保持正好运行五个实例。如果实例之一停止了正常工作，比如当进程崩溃或停止响应时，Kubernetes将自动重启它。同理，如果整个工作节点死亡或无法访问，Kubernetes将为在故障节点上运行的所有容器选择新节点，并在新选择的节点上运行它们。 图1.10 Kubernetes体系结构的基本概述和在它之上运行的应用程序 扩展副本数量当应用程序运行时，可以决定要增加或减少副本量，而Kubernetes将分别增加附加的或停止多余的副本。甚至可以把决定最佳副本数目的工作交给Kubernetes。它可以根据实时指标（如CPU负载、内存消耗、每秒查询或应用程序公开的任何其他指标）自动调整副本数。 命中移动目标我们已经说过，Kubernetes可能需要在集群中迁移你的容器。当它们运行的节点失败时，或者为了给其他容器腾出地方而从节点移除时，就会发生这种情况。如果容器向运行在集群中的其他容器或者外部客户端提供服务，那么当容器在集群内频繁调度时，它们该如何正确使用这个容器？当这些容器被复制并分布在整个集群中时，客户端如何连接到提供服务的容器呢？为了让客户能够轻松地找到提供特定服务的容器，可以告诉 Kubernetes哪些容器提供相同的服务，而Kubernetes将通过一个静态IP地址暴露所有容器，并将该地址暴露给集群中运行的所有应用程序。这是通过环境变量完成的，但是客户端也可以通过良好的DNS查找服务IP。 kube-proxy将确保到服务的连接可跨提供服务的容器实现负载均衡。服务的IP地址保持不变，因此客户端始终可以连接到它的容器，即使它们在集群中移动。 1.3.5 使用Kubernetes的好处如果在所有服务器上部署了Kubernetes，那么运维团队就不需要再部署应用程序。因为容器化的应用程序已经包含了运行所需的所有内容，系统管理员不需要安装任何东西来部署和运行应用程序。在任何部署Kubernetes的节点上，Kubernetes可以在不需要系统管理员任何帮助的情况下立即运行应用程序。 简化应用程序部署由于Kubernetes将其所有工作节点公开为一个部署平台，因此应用程序开发人员可以自己开始部署应用程序，不需要了解组成集群的服务器。实际上，现在所有节点都是一组等待应用程序使用它们的计算资源。开发人员通常不关心应用程序运行在哪个服务器上，只要服务器能够为应用程序提供足够的系统资源即可。在某些情况下，开发人员确实关心应用程序应该运行在哪种硬件上。如果节点是异构的，那么你将会发现你希望某些应用程序在具有特定功能的节点上运行，并在其他的节点上运行其他应用程序。例如，你的一个应用程序可能需要在使用ssd而不是HDDs的系统上运行，而其他应用程序在HDDs上运行良好。在这种情况下，你显然希望确保特定的应用程序总是被调度到有SSD的节点上。在不使用Kubernetes的情况下，系统管理员将选择一个具有SSD的特定节点，并在那里部署应用程序。但是当使用Kubernetes时，与其选择应用程序应该运行在某一特定节点上，不如告诉Kubernetes只在具有SSD的节点中进行选择。你将在第3章学到如何做到这一点。 更好地利用硬件通过在服务器上装配Kubernetes，并使用它运行应用程序而不是手 动运行它们，你已经将应用程序与基础设施分离开来。当你告诉 Kubernetes运行你的应用程序时，你在让它根据应用程序的资源需求描 述和每个节点上的可用资源选择最合适的节点来运行你的应用程序。通过使用容器，不再用把这个应用绑定到一个特定的集群节点，而 允许应用程序在任何时候都在集群中自由迁移，所以在集群上运行的不 同应用程序组件可以被混合和匹配来紧密打包到集群节点。这将确保节点的硬件资源得到尽可能好的利用。可以随时在集群中移动应用程序的能力，使得Kubernetes可以比人 工更好地利用基础设施。人类不擅长寻找最优的组合，尤其是当所有选 项的数量都很大的时候，比如当你有许多应用程序组件和许多服务器节 点时，所有的组件可以部署在所有的节点上。显然，计算机可以比人类 更好、更快地完成这项工作。 健康检查和自修复在服务器发生故障时，拥有一个允许在任何时候跨集群迁移应用程序的系统也很有价值。随着集群大小的增加，你将更频繁地处理出现故障的计算机组件。Kubernetes监控你的应用程序组件和它们运行的节点，并在节点出现故障时自动将它们重新调度到其他节点。这使运维团队不必手动迁移应用程序组件，并允许团队立即专注于修复节点本身，并将其修好送回到可用的硬件资源池中，而不是将重点放在重新定位应用程序上。如果你的基础设施有足够的备用资源来允许正常的系统运行，即使故障节点没有恢复，运维团队甚至不需要立即对故障做出反应，比如在凌晨3点。他们可以睡得很香，在正常的工作时间再处理失败的节点。 自动扩容使用Kubernetes来管理部署的应用程序，也意味着运维团队不需要不断地监控单个应用程序的负载，以对突发负载峰值做出反应。如前所述，可以告诉Kubernetes监视每个应用程序使用的资源，并不断调整每个应用程序的运行实例数量。如果Kubernetes运行在云基础设施上，在这些基础设施中，添加额外的节点就像通过云供应商的API请求它们一样简单，那么Kubernetes甚至可以根据部署的应用程序的需要自动地将整个集群规模放大或缩小。 简化应用部署前一节中描述的特性主要对运维团队有利。但是开发人员呢？Kubernetes是否也给他们带来什么好处？这毋庸置疑。如果你回过头来看看，应用程序开发和生产流程中都运行在同一个环境中，这对发现bug有很大的影响。我们都同意越早发现一个bug，修复它就越容易，修复它需要的工作量也就越少。由于是在开发阶段就修复bug，所以这意味着他们的工作量减少了。还有一个事实是，开发人员不需要实现他们通常会实现的特性。这包括在集群应用中发现服务和对端。这是由Kubernetes来完成的而不是应用。通常，应用程序只需要查找某些环境变量或执行DNS查询。如果这还不够，应用程序可以直接查询Kubernetes API服务器以获取该信息和其他信息。像这样查询Kubernetes API服务器，甚至可以使开发人员不必实现诸如复杂的集群leader选举机制。作为最后一个关于Kubernetes带来什么的例子，还需要考虑到开发者们的信心增加。当他们知道，新版本的应用将会被推出时Kubernetes可以自动检测一个应用的新版本是否有问题，如果是则立即停止其滚动更新，这种信心的增强通常会加速应用程序的持续交付，这对整个组织都有好处。 1.4 本章小结在这个介绍性章节中，你已经看到了近年来应用程序的变化，以及它们现在如何变得更难部署和管理。我们已经介绍了Kubernetes，并展示了它如何与Docker或其他容器平台一起帮助部署和管理应用程序及其运行的基础设施。你已经学到了： 单体应用程序更容易部署，但随着时间的推移更难维护，并且有时难以扩展。 基于微服务的应用程序体系结构使每个组件的开发更容易，但是很难配置和部署它们作为单个系统工作。 Linux容器提供的好处与虚拟机差不多，但它们轻量许多，并且允许更好地利用硬件。 通过允许更简单快捷地将容器化应用和其操作系统环境一起管理，Docker改进了现有的Linux容器技术。 Kubernetes将整个数据中心暴露为用于运行应用程序的单个计算资源。 开发人员可以通过Kubernetes部署应用程序，而无须系统管理员的帮助。 通过让Kubernetes自动地处理故障节点，系统管理员可以睡得更好。 在下一章中，你将通过构建一个应用程序并在Docker和Kubernetes中运行它，来上手实践。"},{"title":"第12章 反射","path":"/wiki/the-go-programming-language/chapter_12.html","content":"Go语言提供了一种机制，在编译时不知道类型的情况下，可更新变量、在运行时查看值、调用方法以及直接对它们的布局进行操作，这种机制称为反射（reflection）。反射也让我们可以把类型当作头等值。本章将探讨Go语言的反射能力以及它如何增强语言的表达能力，特别是他在实现两个重要API中的关键作用。这两个API分别是fmt包提供的字符串格式化功能，以及encoding&#x2F;json和encoding&#x2F;xml这种包提供的协议编码功能。反射在text&#x2F;tmplate和html&#x2F;template包提供的模版机制中也很重要。另外，反射的推导比较复杂，也不是为了随意使用而设计的，因此尽管这些包使用反射来实现，但它们并没有在自己的API中暴露反射。 12.1 为什么使用反射有时我们需要写一个函数有能力统一处理各种值类型的函数，而这些类型可能无法共享同一个接口，也可能布局未知，也可能这个类型在我们设计时还不存在，甚至这个类型会同时存在上面三种问题。一个熟悉的例子是fmt.Printf中的格式化逻辑，他可以输出任意类型的任意值，甚至是用户自定义的一个类型。让我们先尝试用我们已学到的知识来实现一个类似的函数。为了简化起见，该函数只接受一个参数，并且与fmt.Sprint一样返回一个字符串，所以我们称这个函数为Sprint。我们先用类型分支来判断这个参数是否定义了一个String方法，如果已定义则直接调用它。然后添加一些switch分支来判断参数的动态类型是否是基本类型（如string、int、bool等），再对每种类型采用不同的格式化操作。 1func Sprint"},{"title":"前言","path":"/wiki/the-go-programming-language/index.html","content":"“Go是一种开源的程序设计语言，它意在使得人们能够方便地构建简单、可靠、高效的软件”"}]